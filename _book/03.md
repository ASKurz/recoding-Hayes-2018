# (PART) MEDIATION ANALYSIS {-}

# The Simple Mediation Model



## Estimation of the direce, indirect, and total effects of $X$

## Example with dichotomous $X$: The influence of presumed media influence

Here we load a couple necessary packages, load the data, and take a peek at them.


```r
library(tidyverse)

pmi <- read_csv("data/pmi/pmi.csv")

glimpse(pmi)
```

```
## Observations: 123
## Variables: 6
## $ cond     <int> 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0...
## $ pmi      <dbl> 7.0, 6.0, 5.5, 6.5, 6.0, 5.5, 3.5, 6.0, 4.5, 7.0, 1.0, 6.0, 5.0, 7.0, 7.0, 7.0, 4.5, 3.5...
## $ import   <int> 6, 1, 6, 6, 5, 1, 1, 6, 6, 6, 3, 3, 4, 7, 1, 6, 3, 3, 2, 4, 4, 6, 7, 4, 5, 4, 6, 5, 5, 7...
## $ reaction <dbl> 5.25, 1.25, 5.00, 2.75, 2.50, 1.25, 1.50, 4.75, 4.25, 6.25, 1.25, 2.75, 3.75, 5.00, 4.00...
## $ gender   <int> 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1...
## $ age      <dbl> 51.0, 40.0, 26.0, 21.0, 27.0, 25.0, 23.0, 25.0, 22.0, 24.0, 22.0, 21.0, 23.0, 21.0, 22.0...
```

You can get the male/female split like so:


```r
pmi %>% 
  group_by(gender) %>% 
  count()
```

```
## # A tibble: 2 x 2
## # Groups:   gender [2]
##   gender     n
##    <int> <int>
## 1      0    80
## 2      1    43
```

Here is the split by `condition`:


```r
pmi %>% 
  group_by(cond) %>% 
  count()
```

```
## # A tibble: 2 x 2
## # Groups:   cond [2]
##    cond     n
##   <int> <int>
## 1     0    65
## 2     1    58
```

Here is how to get the ungrouped mean and $SD$ values for `reaction` and `pmi`, as presented in Table 3.1,


```r
pmi %>% 
  select(reaction, pmi) %>% 
  gather() %>% 
  group_by(key) %>% 
  summarise(mean = mean(value),
            sd = sd(value)) %>% 
  mutate_if(is.double, round, digits = 3)
```

```
## # A tibble: 2 x 3
##   key       mean    sd
##   <chr>    <dbl> <dbl>
## 1 pmi       5.60  1.32
## 2 reaction  3.48  1.55
```

You might get the mean and $SD$ values for `reaction` and `pmi` grouped by `cond` like this:


```r
pmi %>% 
  select(reaction, pmi, cond) %>% 
  gather(key, value, -cond) %>% 
  group_by(cond, key) %>% 
  summarise(mean = mean(value),
            sd = sd(value)) %>% 
  mutate_if(is.double, round, digits = 3)
```

```
## # A tibble: 4 x 4
## # Groups:   cond [2]
##    cond key       mean    sd
##   <int> <chr>    <dbl> <dbl>
## 1     0 pmi       5.38  1.34
## 2     0 reaction  3.25  1.61
## 3     1 pmi       5.85  1.27
## 4     1 reaction  3.75  1.45
```

Let's load our primary statistical package.


```r
library(brms)
```

Before we begin, I should acknowledge that I greatly benefited by [this great blog post on path analysis in brms](http://www.imachordata.com/bayesian-sem-with-brms/) by Jarrett Byrnes. In brms, we handle mediation models using the [multivariate syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html). There are a few ways to do this. Let's start simple. 

If you look at the path model in Figure 3.3, you'll note that `reaction` is predicted by `pmi` and `cond`. `pmi`, in turn, is predicted solely by `cond`. So we have two regression models, which is just the kind of thing the brms multivariate syntax is for. So first let's specify both models, which we'll nest in `bf()` functions and save as objects.


```r
y_model <- bf(reaction ~ 1 + pmi + cond)
m_model <- bf(pmi ~ 1 + cond)
```

Now we have our `bf()` objects in hand, we'll combine them with the `+` operator within the `brm()` function. We'll also specify `set_rescor(FALSE)`--we're not interested in adding a residual correlation between `reaction` and `pmi`.


```r
model1 <-
  brm(data = pmi, family = gaussian,
      y_model + m_model + set_rescor(FALSE),
      chains = 4, cores = 4)
```

Here are our results.


```r
print(model1)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: reaction ~ 1 + pmi + cond 
##          pmi ~ 1 + cond 
##    Data: pmi (Number of observations: 123) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                    Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## reaction_Intercept     0.53      0.55    -0.56     1.65       4000 1.00
## pmi_Intercept          5.38      0.17     5.06     5.70       4000 1.00
## reaction_pmi           0.50      0.10     0.31     0.70       4000 1.00
## reaction_cond          0.26      0.25    -0.24     0.75       4000 1.00
## pmi_cond               0.47      0.24    -0.01     0.94       4000 1.00
## 
## Family Specific Parameters: 
##                Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma_reaction     1.41      0.09     1.24     1.60       4000 1.00
## sigma_pmi          1.32      0.09     1.16     1.50       4000 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

If you compare our model summary with the coefficients in the path model in Figure 3.3, you'll see our coefficients are the same. The brms summary also includes intercepts and residual variances, which are typically omitted in path diagrams even though they're still part of the model.

If you're getting lost in all the model output, try this.


```r
fixef(model1)[3:5, ] %>% round(digits = 3)
```

```
##               Estimate Est.Error   Q2.5 Q97.5
## reaction_pmi     0.504     0.098  0.309 0.697
## reaction_cond    0.257     0.252 -0.243 0.748
## pmi_cond         0.474     0.240 -0.005 0.936
```

Also note that Hayes tends to refer to the intercepts as constants. 

In his Table 3.2, Hayes included the $R^2$ values. Here are ours.


```r
bayes_R2(model1) %>% round(digits = 3)
```

```
##             Estimate Est.Error  Q2.5 Q97.5
## R2_reaction    0.208     0.056 0.099 0.315
## R2_pmi         0.039     0.031 0.000 0.112
```

It's worth it to actually plot the $R^2$ distributions.


```r
# we'll get our color palette from ggthemes
library(ggthemes)

bayes_R2(model1, summary = F) %>% 
  as_tibble() %>% 
  gather() %>% 
  
  ggplot(aes(x = value, fill = key)) +
  geom_density(color = "transparent", alpha = 2/3) +
  scale_fill_colorblind() +  # we got this color palette from the ggthemes package
  coord_cartesian(xlim = 0:1) +
  labs(title = expression(paste("The ", italic("R")^{2}, " distributions for fit0")),
       x = NULL) +
  theme_classic()
```

<img src="03_files/figure-html/unnamed-chunk-11-1.png" width="576" />

We went through the trouble of plotting the $R^2$ distributions because it’s useful to understand that they won’t often be symmetric when they’re near their logical boundaries (i.e., 0 and 1). This is where asymmetric Bayesian credible intervals can really shine.

Let's get down to business and examine the indirect effect, the $ab$ pathway. In our model:

* $a$ = `pmi_cond`
* $b$ = `reaction_pmi`

You can isolate them with `fixef()[i]`.


```r
fixef(model1)[5 , ]
```

```
##     Estimate    Est.Error         Q2.5        Q97.5 
##  0.474456456  0.240391424 -0.005044517  0.936437859
```

```r
fixef(model1)[3 , ]
```

```
##   Estimate  Est.Error       Q2.5      Q97.5 
## 0.50449451 0.09774672 0.30939585 0.69683689
```

So the naive approach would be to just multiply them.


```r
(fixef(model1)[5 , ] * fixef(model1)[3 , ]) %>% round(digits = 3)
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
##     0.239     0.023    -0.002     0.653
```

Now, this does get us the correct 'Estimate' (i.e., posterior mean). However, the posterior $SD$ and 95% intervals are off. If you want to do this properly, you need to work with the poster samples themselves. Here they are:


```r
post <- posterior_samples(model1)

glimpse(post)
```

```
## Observations: 4,000
## Variables: 8
## $ b_reaction_Intercept <dbl> 1.950176626, -0.642275060, 1.756071145, -0.528590500, 0.965722944, -0.381105...
## $ b_pmi_Intercept      <dbl> 5.463846, 5.387659, 5.209249, 5.235493, 5.224044, 5.177939, 5.354357, 5.3499...
## $ b_reaction_pmi       <dbl> 0.2303314, 0.7141880, 0.2955409, 0.5939846, 0.4863486, 0.6712376, 0.5585035,...
## $ b_reaction_cond      <dbl> 0.568063017, 0.144055063, 0.335846731, 0.938714819, -0.041544801, 0.00300684...
## $ b_pmi_cond           <dbl> 0.410513415, 0.341716877, 0.771462188, 0.557731830, 0.479940454, 0.862242164...
## $ sigma_reaction       <dbl> 1.518972, 1.323994, 1.485331, 1.345070, 1.484862, 1.509208, 1.210915, 1.6082...
## $ sigma_pmi            <dbl> 1.414180, 1.229791, 1.421680, 1.252101, 1.506832, 1.322447, 1.276929, 1.3284...
## $ lp__                 <dbl> -436.9322, -434.8615, -435.8168, -438.9417, -436.6032, -435.2722, -435.1702,...
```

Here we compute the indirect effect, `ab`.


```r
post <-
  post %>% 
  mutate(ab = b_pmi_cond*b_reaction_pmi)
```

Now we have `ab` as a properly computed vector, we can summarize it with the `quantile()` function.


```r
quantile(post$ab, probs = c(.5, .025, .975)) %>% 
  round(digits = 3)
```

```
##    50%   2.5%  97.5% 
##  0.232 -0.003  0.522
```

And we can even visualize it as a density.


```r
post %>% 
  
  ggplot(aes(x = ab)) +
  geom_density(color = "transparent", 
               fill = colorblind_pal()(3)[3]) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(paste("Our indirect effect, the ", italic("ab"), " pathway")),
       x = NULL) +
  theme_classic()
```

<img src="03_files/figure-html/unnamed-chunk-17-1.png" width="384" />

It's also worth pointing out that as the indirect effect isn't perfectly symmetric, its mean and median aren't quite the same.


```r
post %>% 
  summarize(mean = mean(ab),
            median = median(ab)) %>% 
  round(digits = 3)
```

```
##   mean median
## 1 0.24  0.232
```

Their magnitudes are similar, but this asymmetry will be a source of contrast to our estimates and those in the text. This is also something to consider when reporting on central tendency. When the indirect effect--or any other parameter, for that matter--is quite asymmetric, you might prefer reporting the median rather than the mean.

On page 90, Hayes computed the *adjusted means* for $Y$. For both `cond == 1` and `cond == 0`, he computed the expected values for `reaction` when `pmi` was at its mean. A natural way to do that in brms is with `fitted()`. First we'll put our input values for `cond` and `pmi` in a tibble, which we'll call `nd`. Then we'll feed `nd` into the `newdata` argument within the `fitted()` function.


```r
nd <-
  tibble(cond = 1:0,
         pmi = mean(pmi$pmi))

fitted(model1, newdata = nd)
```

```
## , , reaction
## 
##      Estimate Est.Error     Q2.5    Q97.5
## [1,] 3.615537 0.1873641 3.247219 3.985118
## [2,] 3.358847 0.1752851 3.009573 3.697929
## 
## , , pmi
## 
##      Estimate Est.Error     Q2.5    Q97.5
## [1,] 5.852410 0.1736779 5.506716 6.195187
## [2,] 5.377954 0.1681786 5.056328 5.695418
```

Because `model1` is a multivariate model, `fitted()` returns the model-implied summaries for both `reaction` and `pmi`. If you just want the adjusted means for `reaction`, you can use the `resp` argument within `fitted()`.


```r
fitted(model1, newdata = nd, resp = "reaction") %>% round(digits = 3)
```

```
##      Estimate Est.Error  Q2.5 Q97.5
## [1,]    3.616     0.187 3.247 3.985
## [2,]    3.359     0.175 3.010 3.698
```

Note how this is where the two values in the $Y$ adjusted column in Table 3.1 came from. 

However, if we want to reproduce how Hayes computed the total effect (i.e., $c'$ + $ab$), we'll need to work with the posterior itself, `post`. Recall, we've already saved the indirect effect as a vector, `ab`. The direct effect, $c'$, is labeled `b_reaction_cond` within `post`. in order to get the total effect, $c$, all we need to is add those vectors together.


```r
post <-
  post %>% 
  mutate(total_effect = b_reaction_cond + ab)
```

Here's the posterior mean with its 95% intervals


```r
post %>% 
  summarize(mean = mean(total_effect),
            ll = quantile(total_effect, prob = .025),
            ul = quantile(total_effect, prob = .975))
```

```
##        mean          ll       ul
## 1 0.4967976 -0.03600994 1.024548
```

### ~~Estimation of the model in PROCESS for SPSS and SAS.~~

Nothing new for us, here.

## Statistical inference

### Inference about the total effect of $X$ on $Y$.

### Inference about the direct effect of $X$ on $Y$.

In this section, Hayes provides a $t$-test and corresponding $p$-value for the direct effect (i.e., $c'$, `b_reaction_cond`). Instead of the $t$-test, we can just look at the posterior distribution.


```r
post %>% 
  
  ggplot(aes(x = b_reaction_cond)) +
  geom_density(color = "transparent", 
               fill = colorblind_pal()(4)[4]) +
  geom_vline(xintercept = 0, color = "white", linetype = 2) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(paste("Yep, 0 is a credible value for ", italic("c"), ".")),
       x = NULL) +
  theme_classic()
```

<img src="03_files/figure-html/unnamed-chunk-23-1.png" width="384" />

If you wanted to quantify what proportion of the density was less than 0, you could do:


```r
post %>% 
  summarize(proportion_below_zero = filter(., b_reaction_cond < 0) %>% nrow()/nrow(.))
```

```
##   proportion_below_zero
## 1                0.1505
```

This is something like a Bayesian $p$-value. But of course, you could always just look at the posterior intervals.


```r
posterior_interval(model1)["b_reaction_cond", ]
```

```
##       2.5%      97.5% 
## -0.2432898  0.7476449
```

### Inference about the indirect of $X$ on $Y$ through $M$.

#### The normal theory approach.

This is not our approach.

#### Bootstrap confidence interval.

This is not our approach.

However, Markov chain Monte Carlo (MCMC) methods are iterative and share some characteristics with boostrapping. On page 98, Hayes outlined 6 steps for constructing the $ab$ bootstrap confidence interval. Here are our responses to those steps w/r/t Bayes with MCMC--or in our case HMC (i.e., Hamiltonian Monte Carlo).

If HMC or MCMC, in general, are new to you, you might check out [this lecture](https://www.youtube.com/watch?v=BWEtS3HuU5A&t=7s) or the [Stan reference manual](http://mc-stan.org/users/documentation/) if you're more technically oriented.

Anyway, Hayes's 6 steps:

##### Step 1.

With HMC we do not take random samples of the data themselves. Rather, we take random draws from the posterior distribution. The posterior distribution is the joint probability distribution of our model. 

##### Step 2.

After we fit our model with the `brm()` function and save our posterior draws in a data frame (i.e., `post <- posterior_samples(my_model_fit)`), we then make a new column (a.k.a. vector, variable) that is the product of our coefficients for the $a$ and $b$ pathways. In the example above, this looked like `post %>% mutate(ab = b_pmi_cond*b_reaction_pmi)`. Let's take a look at those columns.


```r
post %>% 
  select(b_pmi_cond, b_reaction_pmi, ab) %>% 
  slice(1:10)
```

```
##    b_pmi_cond b_reaction_pmi         ab
## 1   0.4105134      0.2303314 0.09455412
## 2   0.3417169      0.7141880 0.24405010
## 3   0.7714622      0.2955409 0.22799862
## 4   0.5577318      0.5939846 0.33128411
## 5   0.4799405      0.4863486 0.23341837
## 6   0.8622422      0.6712376 0.57876937
## 7   0.3388167      0.5585035 0.18923030
## 8   0.6320108      0.5958088 0.37655762
## 9   0.2710269      0.6620422 0.17943122
## 10  0.2677347      0.6401424 0.17138832
```

Our data frame, `post`, has 4000 rows. Why 4000? By default, brms runs 4 HMC chains. Each chain has 2000 iterations, 1000 of which are warmups, which we always discard. As such, there are 1000 good iterations left in each chain and $1000\times4 = 4000$. We can change these defaults as needed. At this point in the project, the default settings have been fine.

Each row in `post` contains the parameter values based on one of those draws. And again, these are draws from the posterior distribution. They are not draws from the data.

##### Step 3.

We don't refit the model $k$ times based on the samples from the data. We take a number of draws from the posterior distribution. Hayes likes to take 5000 samples when he bootstraps. Happily, that number is quite similar to our 4000 HMC draws. Whether 5000, 4000 or 10,000, these are all large enough numbers that the distributions become fairly stable. With HMC, however, you might want to increase the number of iterations if the effective sample size, 'Eff.Sample' in the `print()` output, is substantially smaller than the number of iterations.

##### Step 4.

When we use the `quantile()` function to compute our Bayesian credible intervals, we've sorted. Conceptually, we've done this:


```r
post %>% 
  select(ab) %>% 
  arrange(ab) %>% 
  slice(1:10)
```

```
##             ab
## 1  -0.19874481
## 2  -0.17442439
## 3  -0.15657977
## 4  -0.15650372
## 5  -0.13878656
## 6  -0.12153300
## 7  -0.10987695
## 8  -0.10746942
## 9  -0.10168737
## 10 -0.09758838
```

##### Step 5. 

Yes, this is what we do, too.


```r
ci <- 95

.5*(100 - ci)
```

```
## [1] 2.5
```

##### Step 6.

This is also what we do.


```r
ci <- 95

(100 - .5*(100 - ci))
```

```
## [1] 97.5
```

Also, notice the headers in the rightmost two columns in our `posterior_summary()` output:


```r
posterior_summary(model1)
```

```
##                          Estimate  Est.Error          Q2.5        Q97.5
## b_reaction_Intercept    0.5328574 0.55479853 -5.576345e-01    1.6452479
## b_pmi_Intercept         5.3779538 0.16817860  5.056328e+00    5.6954178
## b_reaction_pmi          0.5044945 0.09774672  3.093958e-01    0.6968369
## b_reaction_cond         0.2566901 0.25232357 -2.432898e-01    0.7476449
## b_pmi_cond              0.4744565 0.24039142 -5.044517e-03    0.9364379
## sigma_reaction          1.4093572 0.09208966  1.241056e+00    1.6010297
## sigma_pmi               1.3195622 0.08718468  1.164042e+00    1.5044898
## lp__                 -434.8809615 1.97212737 -4.397098e+02 -432.1698836
```

Those .025 and .975 quantiles from above are just what brms is giving us in our 95% Bayesian credible intervals.

Here's our version of Figure 3.5


```r
# these will come in handy in the subtitle
ll <- quantile(post$ab, probs = .025) %>% round(digits = 3)
ul <- quantile(post$ab, probs = .975) %>% round(digits = 3)

post %>% 
  
  ggplot(aes(x = ab)) +
  geom_histogram(color = "white", size = .25, 
               fill = colorblind_pal()(5)[5],
               binwidth = .025, boundary = 0) +
  geom_vline(xintercept = quantile(post$ab, probs = c(.025, .975)),
             linetype = 3, color = colorblind_pal()(6)[6]) +
  labs(x = expression(paste("Indirect effect (", italic("ab"), ")")),
       y = "Frequency in 4,000 HMC posterior draws",
       subtitle = paste("95% of the posterior draws are between", ll, "and", ul)) +
  theme_classic()
```

<img src="03_files/figure-html/unnamed-chunk-31-1.png" width="408" />

Again, as Hayes discussed how to specify different types of intervals in PROCESS on page 102, you can ask for different kinds of intervals in your `print()` or `summary()` output with the `probs` argument, just as you can with `quantile()` when working directly with the posterior draws. 

Hayes discussed setting the seed in PROCESS on page 104. You can do this with the `seed` argument in the `brm()` function, too. 

#### Alternative "asymmetric" confidence interval approaches.

This section does not quite refer to us. I'm a little surprised Hayes didn't at least dedicate a paragraph or two mentioning Bayesian estimation. Sure, he mentioned Monte Carlo, but not within the context of Bayes. So it goes...

## An example with continuous $X$: Economic stress among small-business owners

Here's the `estress` data.


```r
estress <- read_csv("data/estress/estress.csv")

glimpse(estress)
```

```
## Observations: 262
## Variables: 7
## $ tenure   <dbl> 1.67, 0.58, 0.58, 2.00, 5.00, 9.00, 0.00, 2.50, 0.50, 0.58, 9.00, 1.92, 2.00, 1.42, 0.92...
## $ estress  <dbl> 6.0, 5.0, 5.5, 3.0, 4.5, 6.0, 5.5, 3.0, 5.5, 6.0, 5.5, 4.0, 3.0, 2.5, 3.5, 6.0, 4.0, 6.0...
## $ affect   <dbl> 2.60, 1.00, 2.40, 1.16, 1.00, 1.50, 1.00, 1.16, 1.33, 3.00, 3.00, 2.00, 1.83, 1.16, 1.16...
## $ withdraw <dbl> 3.00, 1.00, 3.66, 4.66, 4.33, 3.00, 1.00, 1.00, 2.00, 4.00, 4.33, 1.00, 5.00, 1.66, 4.00...
## $ sex      <int> 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1...
## $ age      <int> 51, 45, 42, 50, 48, 48, 51, 47, 40, 43, 57, 36, 33, 29, 33, 48, 40, 45, 37, 42, 54, 57, ...
## $ ese      <dbl> 5.33, 6.05, 5.26, 4.35, 4.86, 5.05, 3.66, 6.13, 5.26, 4.00, 2.53, 6.60, 5.20, 5.66, 5.66...
```

The model set up is just like before. There are no complications switching from a binary $X$ variable to a continuous one.


```r
y_model <- bf(withdraw ~ 1 + estress + affect)
m_model <- bf(affect ~ 1 + estress)
```

With our `y_model` and `m_model` defined, we're ready to fit.


```r
model2 <-
  brm(data = estress, family = gaussian,
      y_model + m_model + set_rescor(FALSE),
      chains = 4, cores = 4)
```

Let's take a look.


```r
print(model2, digits = 3)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: withdraw ~ 1 + estress + affect 
##          affect ~ 1 + estress 
##    Data: estress (Number of observations: 262) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                    Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## withdraw_Intercept    1.450     0.257    0.939    1.953       4000 1.000
## affect_Intercept      0.800     0.145    0.523    1.080       4000 0.999
## withdraw_estress     -0.077     0.053   -0.181    0.025       4000 1.000
## withdraw_affect       0.766     0.104    0.561    0.969       4000 1.000
## affect_estress        0.173     0.030    0.113    0.230       4000 0.999
## 
## Family Specific Parameters: 
##                Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma_withdraw    1.139     0.048    1.048    1.232       4000 0.999
## sigma_affect      0.686     0.031    0.628    0.750       4000 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

The 'Eff.Sample' and 'Rhat' values look great. Happily, the values in our summary cohere well with those Hayes reported in Table 3.5. Here are our $R^2$ values.


```r
bayes_R2(model2)
```

```
##              Estimate  Est.Error      Q2.5     Q97.5
## R2_withdraw 0.1815384 0.03849542 0.1044912 0.2572579
## R2_affect   0.1169312 0.03505706 0.0524473 0.1861113
```

These are also quite similar to those in the text. Here's our indirect effect.


```r
# putting the posterior draws into a data frame
post <- posterior_samples(model2)

# computing the ab coefficient with multiplication
post <-
  post %>% 
  mutate(ab = b_affect_estress*b_withdraw_affect)

# getting the posterior median and 95% intervals with `quantile()`
quantile(post$ab, probs = c(.5, .025, .975)) %>% round(digits = 3)
```

```
##   50%  2.5% 97.5% 
## 0.131 0.079 0.195
```

We can visualize its shape, median, and 95% intervals in a density plot.


```r
post %>% 
  ggplot(aes(x = ab)) +
  geom_density(color = "transparent", 
               fill = colorblind_pal()(7)[7]) +
  geom_vline(xintercept = quantile(post$ab, probs = c(.025, .5, .975)), 
             color = "white", linetype = c(2, 1, 2), size = c(.5, .8, .5)) +
  scale_x_continuous(breaks = quantile(post$ab, probs = c(.025, .5, .975)),
                     labels = quantile(post$ab, probs = c(.025, .5, .975)) %>% round(2) %>% as.character()) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = expression(paste("Behold our ", italic("ab"), "!")),
       x = NULL) +
  theme_classic()
```

<img src="03_files/figure-html/unnamed-chunk-37-1.png" width="384" />

Here's $c'$, the direct effect of `esterss` predicting `withdraw`.


```r
posterior_summary(model2)["b_withdraw_estress", ]
```

```
##    Estimate   Est.Error        Q2.5       Q97.5 
## -0.07663292  0.05256371 -0.18050458  0.02503829
```

It has wide flapping intervals which do straddle zero. A little addition will give us the direct effect, $c$.


```r
post <-
  post %>% 
  mutate(c = b_withdraw_estress + ab)

quantile(post$c, probs = c(.5, .025, .975)) %>% round(digits = 3)
```

```
##    50%   2.5%  97.5% 
##  0.056 -0.053  0.165
```

## Reference {-}

[Hayes, A. F. (2018). *Introduction to mediation, moderation, and conditional process analysis: A regression-based approach.* (2nd ed.). New York, NY, US: The Guilford Press.](http://afhayes.com/introduction-to-mediation-moderation-and-conditional-process-analysis.html)

## Session info {-}


```r
sessionInfo()
```

```
## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ggthemes_3.5.0  brms_2.3.1      Rcpp_0.12.17    bindrcpp_0.2.2  forcats_0.3.0   stringr_1.3.1  
##  [7] dplyr_0.7.6     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1     tibble_1.4.2    ggplot2_3.0.0  
## [13] tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##  [1] nlme_3.1-137         matrixStats_0.53.1   xts_0.10-2           lubridate_1.7.4      threejs_0.3.1       
##  [6] httr_1.3.1           rprojroot_1.3-2      rstan_2.17.3         tools_3.5.1          backports_1.1.2     
## [11] utf8_1.1.4           R6_2.2.2             DT_0.4               lazyeval_0.2.1       colorspace_1.3-2    
## [16] withr_2.1.2          tidyselect_0.2.4     gridExtra_2.3        mnormt_1.5-5         Brobdingnag_1.2-5   
## [21] compiler_3.5.1       cli_1.0.0            rvest_0.3.2          shinyjs_1.0          xml2_1.2.0          
## [26] labeling_0.3         colourpicker_1.0     bookdown_0.7         scales_0.5.0         dygraphs_1.1.1.5    
## [31] mvtnorm_1.0-8        psych_1.8.4          ggridges_0.5.0       digest_0.6.15        StanHeaders_2.17.2  
## [36] foreign_0.8-70       rmarkdown_1.10       base64enc_0.1-3      pkgconfig_2.0.1      htmltools_0.3.6     
## [41] htmlwidgets_1.2      rlang_0.2.1          readxl_1.1.0         rstudioapi_0.7       shiny_1.1.0         
## [46] bindr_0.1.1          zoo_1.8-2            jsonlite_1.5         gtools_3.8.1         crosstalk_1.0.0     
## [51] inline_0.3.15        magrittr_1.5         loo_2.0.0            bayesplot_1.5.0      Matrix_1.2-14       
## [56] munsell_0.5.0        abind_1.4-5          stringi_1.2.3        yaml_2.1.19          plyr_1.8.4          
## [61] grid_3.5.1           parallel_3.5.1       promises_1.0.1       crayon_1.3.4         miniUI_0.1.1.1      
## [66] lattice_0.20-35      haven_1.1.2          hms_0.4.2            knitr_1.20           pillar_1.2.3        
## [71] igraph_1.2.1         markdown_0.8         shinystan_2.5.0      reshape2_1.4.3       stats4_3.5.1        
## [76] rstantools_1.5.0     glue_1.2.0           evaluate_0.10.1      modelr_0.1.2         httpuv_1.4.4.2      
## [81] cellranger_1.1.0     gtable_0.2.0         assertthat_0.2.0     xfun_0.3             mime_0.5            
## [86] xtable_1.8-2         broom_0.4.5          coda_0.19-1          later_0.7.3          rsconnect_0.8.8     
## [91] shinythemes_1.1.1    bridgesampling_0.4-0
```

