# (PART) CONDITIONAL PROCESS ANALYSIS {-}

# Fundamentals of Conditional Process Analysis

```{r, echo = FALSE, cachse = FALSE}
options(width = 110)
```

> Thus far in this book, mediation and moderation have been treated as distinct, separate, and independent concepts with different analytical procedures and interpretations. Yet processes modeled with mediation analysis likely are contingent and hence moderated, in that they operate differently for different people or in different contexts or circumstances. A more complete analysis, therefore, should attempt to model the mechanisms at work linking $X$ to $Y$ while simultaneously allowing those effects to be contingent on context, circumstance, or individual differences. (p. 395)

## Examples of conditional process models in the literature

You can look up the various examples in the literature on your own. The main point is

> moderation can be combined with mediation in a number of different ways. But these examples [we skipped for the sake of brevity] only scratch the surface of what is possible. Think about the number of possibilities when you increase the number of mediators, distinguish between moderation of paths in a parallel versus serial multiple mediator model, or allow for multiple moderators of different paths or the same path, and so forth. The possibilities are nearly endless. But regardless of the configuration of moderated paths or complexity of the model, conditional process analysis involves the estimation and interpretation of direct and indirect effects, just as in a simple mediation analysis. However, when causal effects in a mediation model are moderated, they will be conditional on those moderators. Thus, an understanding of the concepts of the *conditional direct effect* and the *conditional indirect effect* is required before one should attempt to under- take a conditional process analysis. (p. 401, *emphasis* in the original)

## Conditional direct and indirect effects

> When a direct or indirect effect is conditional, analysis and interpretation of the results of the modeling process should be based on a formal estimate of and inference about conditional direct and/or conditional in- direct effects. In this section, [Hayes illustrated] the computation of conditional direct and indirect effects for example models that combine moderation and mediation. (p. 403)

## Example: Hiding your feelings from your work team

Here we load a couple necessary packages, load the data, and take a `glimpse()`.

```{r, warning = F, message = F}
library(tidyverse)

teams <- read_csv("data/teams/teams.csv")

glimpse(teams)
```

Load the **brms** package.

```{r, message = F, warning = F}
library(brms)
```

Recall that we fit mediation models with **brms** using multivariate syntax. In previous attempts, we've defined and saved the model components outside of the `brm()` function and then plugged then into `brm()` using their identifier. Just to shake things up a bit, we'll just do all the steps right in `brm()`, this time.

```{r model1, cache = T, message = F, warning = F}
model1 <-
  brm(data = teams, family = gaussian,
      bf(negtone ~ 1 + dysfunc) +
        bf(perform ~ 1 + dysfunc + negtone + negexp + negtone:negexp) + 
        set_rescor(FALSE),
      chains = 4, cores = 4)
```

```{r}
print(model1, digits = 3)
```

Our model summary coheres nicely with Table 11.1 and the formulas on page 409. Here are the $R^2$ distribution summaries.

```{r}
bayes_R2(model1) %>% round(digits = 3)
```

On page 410 Hayes reported two sample means. Compute them like so.

```{r}
mean(teams$negexp) %>% round(digits = 3)   # w_bar
mean(teams$perform) %>% round(digits = 3)  # m_bar
```

For our Figure 11.4 and other similar figures in this chapter, we'll use spaghetti plots. Recall that with a spaghetti plots for linear models, we only need two values for the variable on the x-axis, rather than the typical 30+.

```{r}
nd <-
  crossing(negtone = c(-.8, .8),
           negexp  = quantile(teams$negexp, probs = c(.16, .50, .84))) %>% 
  mutate(dysfunc   = mean(teams$dysfunc))
```

Here's our Figure 11.4, which uses only the first 40 HMC iterations for the spaghetti-plot lines.

```{r, fig.width = 10, fig.height = 3.25, warning = F, message = F}
# `fitted()`
fitted(model1, 
       newdata = nd,
       resp = "perform", 
       summary = F) %>% 
  # wrangle
  as_tibble() %>% 
  gather() %>% 
  bind_cols(
    nd %>% 
      expand(nesting(negtone, negexp),
             iter = 1:4000)
  ) %>% 
  mutate(negexp = factor(str_c("expresivity = ", negexp), 
                         levels = c("expresivity = -0.49", "expresivity = -0.06", "expresivity = 0.6"))) %>% 
  filter(iter < 41) %>% 
  
  # plot
  ggplot(aes(x = negtone, y = value, group = iter)) +
  geom_line(color = "skyblue3",
            size = 1/4) +
  coord_cartesian(xlim = c(-.5, .5),
                  ylim = c(-.6, .6)) +
  labs(x = expression(paste("Negative Tone of the Work Climate (", italic(M), ")")),
       y = "Team Performance") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        strip.background = element_rect(color = "transparent", fill = "transparent")) +
  facet_wrap(~negexp)
```

Also, the plot theme in this chapter is a nod to the style [John Kruschke](http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/) frequently uses in his papers and texts.

Using Hayes' notation from the top of page 412, we can express $M$'s conditional effect on $Y$ as

$$\theta_{M \rightarrow Y} = b_1 + b_3 W,$$

where $M$ is `negtone`, $Y$ is `perform`, and $W$ is `negexp`. We can extract our posterior summaries for $b_1$ and $b_3$ like so.

```{r}
fixef(model1)[c("perform_negtone", "perform_negtone:negexp"), ]
```

## ~~Estimation of a conditional process model using PROCESS~~ 

We just fit the model in the last section. No need to repeat.

## Quantifying and visualizing (conditional) indirect and direct effects.

> The analysis presented thus far has been piecemeal, in that [Hayes] addressed how to estimate the regression coefficients for each equation in this conditional process model and how to interpret them using standard principles of regression analysis, moderation analysis, and so forth. But a complete analysis goes further by integrating the estimates of each of the effects in the model (i.e., $X \rightarrow M, \theta_{M \rightarrow Y}$) to yield the direct and indirect effects of $X$ on $Y$. That is, the individual effects as quantified with the regression coefficients (conditional or otherwise) in equations 11.10 and 11.11 are not necessarily of immediate interest or relevance. Estimating them is a means to an end. What matters is the estimation of the direct and indirect effects, for they convey information about how $X$ influences $Y$ directly or through a mediator and how those effects are contingent on a moderator. (pp. 417--418)

#### The conditional indirect effect of $X$.

One way to make a version of Table 11.2 is to work with the `posterior_samples()`, simply summarizing the distributions with means.

```{r}
post <-
  posterior_samples(model1)

post %>% 
  mutate(a  = b_negtone_dysfunc,
         b1 = b_perform_negtone,
         b3 = `b_perform_negtone:negexp`) %>% 
  expand(nesting(a, b1, b3),
         w = c(-0.531, -0.006, 0.600)) %>% 
  mutate(conditional_effect          =      b1 + b3 * w,
         conditional_indirect_effect = a * (b1 + b3 * w)) %>% 
  select(-(b1:b3)) %>% 
  pivot_longer(-w) %>% 
  group_by(w, name) %>% 
  summarise(mean = mean(value) %>% round(digits = 3)) %>% 
  pivot_wider(names_from = name, values_from = mean)
```

That kind of summary isn't the most Bayesian of us. 

```{r, fig.width = 6, fig.height = 4, warning = F, message = F}
post %>% 
  mutate(a  = b_negtone_dysfunc,
         b1 = b_perform_negtone,
         b3 = `b_perform_negtone:negexp`) %>% 
  expand(nesting(a, b1, b3),
         w = c(-0.531, -0.006, 0.600)) %>% 
  mutate(conditional_effect          =      b1 + b3 * w,
         conditional_indirect_effect = a * (b1 + b3 * w)) %>% 
  select(-(b1:b3)) %>% 
  pivot_longer(-w) %>% 
  mutate(label = str_c("W = ", w),
         w = fct_reorder(label,
                         w)) %>% 
  
  ggplot(aes(x = value)) +
  geom_vline(xintercept = 0, color = "grey50", linetype = 2) +
  geom_histogram(color = "white", fill = "skyblue3") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("posterior") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        strip.background = element_rect(color = "transparent", fill = "transparent")) +
  facet_grid(w~name)
```

Here the posterior distribution for each is on full display.

#### The direct effect.

The direct effect of $X$ on $Y$ (i.e., `dysfunc` on `perform`) for this model is `b_perform_dysfunc` in **brms**. Here's how to get its summary values from `posterior_summary()`.

```{r}
posterior_summary(model1)["b_perform_dysfunc", ] %>% round(digits = 3)
```

### Visualizing the direct and indirect effects.

For Figure 11.7 we'll use the first 400 HMC iterations.

```{r, fig.width = 6, fig.height = 4.5}
post <-
  post %>% 
  mutate(`-0.7` = b_negtone_dysfunc * (b_perform_negtone + `b_perform_negtone:negexp` * -0.7),
         `0.7`  = b_negtone_dysfunc * (b_perform_negtone + `b_perform_negtone:negexp` * 0.7))
  
post %>% 
  select(b_perform_dysfunc, `-0.7`:`0.7`) %>% 
  gather(key, value, -b_perform_dysfunc) %>% 
  mutate(negexp = key %>% as.double(),
         iter = rep(1:4000, times = 2)) %>% 
  filter(iter < 401) %>% 
  
  ggplot(aes(x = negexp, group = iter)) +
  geom_hline(aes(yintercept = b_perform_dysfunc),
             color = "skyblue3",
             size = .3, alpha = .3) +
  geom_line(aes(y = value),
            color = "skyblue3",
            size = .3, alpha = .3) +
  coord_cartesian(xlim = c(-.5, .6),
                  ylim = c(-1.25, .75)) +
  labs(x = expression(paste("Nonverbal Negative Expressivity (", italic(W), ")")),
       y = "Effect of Dysfunctional Behavior on Team Performance") +
  theme_bw() +
  theme(panel.grid = element_blank())
```

Since the `b_perform_dysfunc` values are constant across $W$, the individual HMC iterations end up perfectly parallel in the spaghetti plot. This is an example of a visualization I'd avoid making with a spaghetti plot for a professional presentation. But hopefully it has some pedagogical value, here.

## Statistical inference

### Inference about the direct effect.

We've already been expressing undertainty in terms of percentile-based 95% intervals and histograms. Here's a plot of the direct effect, `b_perform_dysfunc`.

```{r, fig.width = 3.5, fig.height = 2.75, warning = F, message = F}
library(tidybayes)

post %>% 
  ggplot(aes(x = b_perform_dysfunc)) +
  geom_histogram(binwidth = .025, boundary = 0, 
                 color = "white", fill = "skyblue3", size = 1/4) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = .95) +
  scale_x_continuous(breaks = mode_hdi(post$b_perform_dysfunc, .width = .95)[1, 1:3],
                     labels = mode_hdi(post$b_perform_dysfunc, .width = .95)[1, 1:3] %>% round(3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("The direct effect (i.e., b_perform_dysfunc)") +
  theme_bw() +
  theme(panel.grid   = element_blank(),
        panel.border = element_blank(),
        axis.line.x  = element_line(size = 1/4))
```

Since we're plotting in a style similar to Kruschke, we switched from emphasizing the posterior mean or median to marking off the posterior mode, which is Kruschkes' preferred measure of central tendency. We also ditched our typical percentile-based 95% intervals for highest posterior density intervals. The `stat_pointintervalh()` function from the Matthew Kay's [**tidybayes** package](https://github.com/mjskay/tidybayes) made it easy to compute those values with the `point_interval = mode_hdi` argument. Note how we also used `tidybayes::mode_hdi()` to compute those values and plug them into `scale_x_continuous()`.

### Inference about the indirect effect.

Much like above, we can make a plot of the conditional indirect effect $ab_3$.

```{r, fig.width = 3.5, fig.height = 2.75}
post <-
  post %>% 
  mutate(ab_3 = b_negtone_dysfunc * `b_perform_negtone:negexp`)

post %>% 
  ggplot(aes(x = ab_3)) +
  geom_histogram(binwidth = .025, boundary = 0, 
                 color = "white", fill = "skyblue3", size = 1/4) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = .95) +
  scale_x_continuous(expression(paste("The indirect effect, ", italic(ab)[3])),
                     breaks = mode_hdi(post$ab_3, .width = .95)[1, 1:3],
                     labels = mode_hdi(post$ab_3, .width = .95)[1, 1:3] %>% round(3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_bw() +
  theme(panel.grid   = element_blank(),
        panel.border = element_blank(),
        axis.line.x  = element_line(size = 1/4))
```

### Probing moderation of mediation.

> One of the contributions of [Preacher et al. (2007)](https://kuscholarworks.ku.edu/bitstream/handle/1808/1658/preacher_rucker_hayes_2007.pdf?sequence=1&isAllowed=y) to the literature on moderated mediation analysis was their discussion of inference for conditional indirect effects. They suggested two approaches, one a normal theory-based approach that is an analogue of the Sobel test in unmoderated mediation analysis, and another based on bootstrapping. (p. 426)

One of the contributions of this project is moving away from NHST in favor of Bayesian modeling. Since we’ve already been referencing him with our plot themes, you might check out Kruschke's [textbook](https://sites.google.com/site/doingbayesiandataanalysis/) for more discussion on Bayes versus NHST.

#### Normal theory approach.

As we're square within the Bayesian modeling paradigm, we have no need to appeal to normal theory for the posterior $SD$s or 95% intervals. 

#### ~~Bootstrap confidence intervals~~ Two types of Bayesian credible intervals.

We produced the posterior means corresponding to those in Table 11.3 some time ago. Here they are, again, with percentile-based 95% intervals via `tidybayes::mean_qi()`.

```{r, warning = F, message = F}
post %>% 
  mutate(a  = b_negtone_dysfunc,
         b1 = b_perform_negtone,
         b3 = `b_perform_negtone:negexp`) %>% 
  expand(nesting(a, b1, b3),
         w = c(-0.531, -0.006, 0.600)) %>% 
  mutate(`a(b1 + b3w)` = a * (b1 + b3 * w)) %>% 
  group_by(w) %>% 
  mean_qi(`a(b1 + b3w)`) %>% 
  select(w:.upper) %>% 
  mutate_if(is.double, round, digits = 3)
```

If we wanted to summarize those same effects with posterior modes and 95% highest posterior density intervals, instead, we'd replace our `mean_qi()` lnie with `mode_hdi()`.

```{r, warning = F, message = F}
post %>% 
  mutate(a  = b_negtone_dysfunc,
         b1 = b_perform_negtone,
         b3 = `b_perform_negtone:negexp`) %>% 
  expand(nesting(a, b1, b3),
         w = c(-0.531, -0.006, 0.600)) %>% 
  mutate(`a(b1 + b3w)` = a * (b1 + b3 * w)) %>% 
  group_by(w) %>% 
  mode_hdi(`a(b1 + b3w)`) %>% 
  select(w:.upper) %>% 
  mutate_if(is.double, round, digits = 3)
```

And we might plot these with something like this.

```{r, fig.width = 6, fig.height = 3}
post %>% 
  mutate(a  = b_negtone_dysfunc,
         b1 = b_perform_negtone,
         b3 = `b_perform_negtone:negexp`) %>% 
  expand(nesting(a, b1, b3),
         w = c(-0.531, -0.006, 0.600)) %>% 
  mutate(`a(b1 + b3w)` = a * (b1 + b3 * w)) %>% 
  select(w:`a(b1 + b3w)`) %>% 
  mutate(label = str_c("W = ", w),
         w = fct_reorder(label,
                         w)) %>% 

  ggplot(aes(x = `a(b1 + b3w)`)) +
  geom_vline(xintercept = 0, color = "grey50", linetype = 2) +
  geom_histogram(binwidth = .05, boundary = 0, 
                 color = "white", fill = "skyblue3", size = 1/4) +
  stat_pointintervalh(aes(y = 0),
                      point_interval = mode_hdi, .width = .95) +
  scale_x_continuous("The conditional indirect effect", limits = c(-1.25, .75)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_flip() +
  theme_bw() +
  theme(panel.grid   = element_blank(),
        panel.border = element_blank(),
        axis.line.y  = element_line(size = 1/4),
        strip.text   = element_text(hjust = 0),
        strip.background = element_rect(color = "transparent", fill = "transparent")) +
  facet_wrap(~w, nrow = 1)
```

This, of course, leads us right into the next section.

#### A Johnson-Neyman approach.

On page 429, Hayes discussed how Preacher et al. (2007)'s attempt to apply the JN technique in this context presumed

> the sampling distribution of the conditional indirect effect is normal. Given that the sampling distribution of the conditional indirect effect is not normal, the approach they describe yields, at best, an approximate solution. To [Hayes's] knowledge, no one has ever proposed a bootstrapping-based analogue of the Johnson-Neyman method for probing the moderation of an indirect effect.

However, our Bayesian HMC approach makes no such assumption. All we need to do is manipulate the posterior as usual. Here it is, this time using all 4000 iterations.

```{r, fig.width = 6, fig.height = 4.5}
post %>% 
  transmute(iter   = 1:n(),
            `-0.8` = b_perform_negtone + `b_perform_negtone:negexp` * -0.8,
            `0.8`  = b_perform_negtone + `b_perform_negtone:negexp` * 0.8) %>% 
  pivot_longer(-iter) %>% 
  mutate(key = name %>% as.double()) %>% 
  
  ggplot(aes(x = key, y = value, group = iter)) +
  geom_line(color = "skyblue3",
            size = 1/6, alpha = 1/15) +
  coord_cartesian(xlim = c(-.5, .6),
                  ylim = c(-1.25, .75)) +
  labs(x = expression(italic(W)),
       y = "The conditional indirect effect") +
  theme_bw() +
  theme(panel.grid = element_blank())
```

Glorious.

## References {-}

[Hayes, A. F. (2018). *Introduction to mediation, moderation, and conditional process analysis: A regression-based approach.* (2nd ed.). New York, NY, US: The Guilford Press.](http://afhayes.com/introduction-to-mediation-moderation-and-conditional-process-analysis.html)

## Session info {-}

```{r}
sessionInfo()
```
