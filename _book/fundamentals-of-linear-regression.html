<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Fundamentals of Linear Regression | Recoding Introduction to Mediation, Moderation, and Conditional Process Analysis</title>
  <meta name="description" content="This ebook is an effort to connect Hayes’s conditional process analysis work with the Bayesian paradigm. Herein I refit his models with my favorite R package for Bayesian regression, Bürkner’s brms, and use the tidyverse for data manipulation and plotting." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Fundamentals of Linear Regression | Recoding Introduction to Mediation, Moderation, and Conditional Process Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This ebook is an effort to connect Hayes’s conditional process analysis work with the Bayesian paradigm. Herein I refit his models with my favorite R package for Bayesian regression, Bürkner’s brms, and use the tidyverse for data manipulation and plotting." />
  <meta name="github-repo" content="ASKurz/recoding-Hayes-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Fundamentals of Linear Regression | Recoding Introduction to Mediation, Moderation, and Conditional Process Analysis" />
  <meta name="twitter:site" content="@SolomonKurz" />
  <meta name="twitter:description" content="This ebook is an effort to connect Hayes’s conditional process analysis work with the Bayesian paradigm. Herein I refit his models with my favorite R package for Bayesian regression, Bürkner’s brms, and use the tidyverse for data manipulation and plotting." />
  

<meta name="author" content="A Solomon Kurz" />


<meta name="date" content="2023-01-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="the-simple-mediation-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this"><i class="fa fa-check"></i>Why this?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#my-assumptions-about-you"><i class="fa fa-check"></i>My assumptions about you</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-and-understand-this-ebook"><i class="fa fa-check"></i>How to use and understand this ebook</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#r-setup"><i class="fa fa-check"></i><strong>R</strong> setup</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#times-change-and-so-do-we"><i class="fa fa-check"></i>Times change and so do we</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#section"><i class="fa fa-check"></i>1.0.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#section-1"><i class="fa fa-check"></i>1.1.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#section-2"><i class="fa fa-check"></i>1.2.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#section-3"><i class="fa fa-check"></i>1.3.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-next"><i class="fa fa-check"></i>What’s next?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#thank-yous-are-in-order"><i class="fa fa-check"></i>Thank-you’s are in order</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license-and-citation"><i class="fa fa-check"></i>License and citation</a></li>
</ul></li>
<li class="part"><span><b>I FUNDAMENTALS</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#session-info"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Fundamentals of Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#correlation-and-prediction"><i class="fa fa-check"></i><b>2.1</b> Correlation and prediction</a></li>
<li class="chapter" data-level="2.2" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#the-simple-linear-regression-model"><i class="fa fa-check"></i><b>2.2</b> The simple linear regression model</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#interpretation-of-the-constant-and-regression-coefficient."><i class="fa fa-check"></i><b>2.2.1</b> Interpretation of the constant and regression coefficient.</a></li>
<li class="chapter" data-level="2.2.2" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#the-standardized-regression-model."><i class="fa fa-check"></i><b>2.2.2</b> The standardized regression model.</a></li>
<li class="chapter" data-level="2.2.3" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#simple-linear-regression-with-a-dichotomous-antecedent-variable."><i class="fa fa-check"></i><b>2.2.3</b> Simple linear regression with a dichotomous antecedent variable.</a></li>
<li class="chapter" data-level="2.2.4" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#a-note-on-symbolic-representations."><i class="fa fa-check"></i><b>2.2.4</b> A note on symbolic representations.</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#alternative-explanations-for-association"><i class="fa fa-check"></i><b>2.3</b> Alternative explanations for association</a></li>
<li class="chapter" data-level="2.4" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>2.4</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#the-standardized-regression-model.-1"><i class="fa fa-check"></i><b>2.4.1</b> The standardized regression model.</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#measures-of-model-fit"><i class="fa fa-check"></i><b>2.5</b> Measures of model fit</a></li>
<li class="chapter" data-level="2.6" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#statistical-inference"><i class="fa fa-check"></i><b>2.6</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#testing-a-null-hypothesis."><i class="fa fa-check"></i><b>2.6.1</b> Testing a null hypothesis.</a></li>
<li class="chapter" data-level="2.6.2" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#interval-estimation."><i class="fa fa-check"></i><b>2.6.2</b> Interval estimation.</a></li>
<li class="chapter" data-level="2.6.3" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#testing-a-hypothesis-about-a-set-of-antecedent-variables."><i class="fa fa-check"></i><b>2.6.3</b> Testing a hypothesis about a set of antecedent variables.</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#multicategorical-antecedent-variables"><i class="fa fa-check"></i><b>2.7</b> Multicategorical antecedent variables</a></li>
<li class="chapter" data-level="2.8" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#assumptions-for-interpretation-and-statistical-inference"><i class="fa fa-check"></i><b>2.8</b> Assumptions for interpretation and statistical inference</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#linearity."><i class="fa fa-check"></i><b>2.8.1</b> Linearity.</a></li>
<li class="chapter" data-level="2.8.2" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#normality."><i class="fa fa-check"></i><b>2.8.2</b> Normality.</a></li>
<li class="chapter" data-level="2.8.3" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#homoscedasticity."><i class="fa fa-check"></i><b>2.8.3</b> Homoscedasticity.</a></li>
<li class="chapter" data-level="2.8.4" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#independence."><i class="fa fa-check"></i><b>2.8.4</b> Independence.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="fundamentals-of-linear-regression.html"><a href="fundamentals-of-linear-regression.html#session-info-1"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="part"><span><b>II MEDIATION ANALYSIS</b></span></li>
<li class="chapter" data-level="3" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html"><i class="fa fa-check"></i><b>3</b> The Simple Mediation Model</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#the-simple-mediation-model-1"><i class="fa fa-check"></i><b>3.1</b> The simple mediation model</a></li>
<li class="chapter" data-level="3.2" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#estimation-of-the-direct-indirect-and-total-effects-of-x"><i class="fa fa-check"></i><b>3.2</b> Estimation of the direct, indirect, and total effects of <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#example-with-dichotomous-x-the-influence-of-presumed-media-influence"><i class="fa fa-check"></i><b>3.3</b> Example with dichotomous <span class="math inline">\(X\)</span>: The influence of presumed media influence</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#estimation-of-the-model-in-process-for-spss-and-sas."><i class="fa fa-check"></i><b>3.3.1</b> <del>Estimation of the model in PROCESS for SPSS and SAS.</del></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#statistical-inference-1"><i class="fa fa-check"></i><b>3.4</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#inference-about-the-total-effect-of-x-on-y."><i class="fa fa-check"></i><b>3.4.1</b> Inference about the total effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>.</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#inference-about-the-direct-effect-of-x-on-y."><i class="fa fa-check"></i><b>3.4.2</b> Inference about the direct effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>.</a></li>
<li class="chapter" data-level="3.4.3" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#inference-about-the-indirect-of-x-on-y-through-m."><i class="fa fa-check"></i><b>3.4.3</b> Inference about the indirect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> through <span class="math inline">\(M\)</span>.</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#an-example-with-continuous-x-economic-stress-among-small-business-owners"><i class="fa fa-check"></i><b>3.5</b> An example with continuous <span class="math inline">\(X\)</span>: Economic stress among small-business owners</a></li>
<li class="chapter" data-level="" data-path="the-simple-mediation-model.html"><a href="the-simple-mediation-model.html#session-info-2"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html"><i class="fa fa-check"></i><b>4</b> Causal Steps, Confounding, and Causal Order</a>
<ul>
<li class="chapter" data-level="4.1" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#what-about-barron-and-kenny"><i class="fa fa-check"></i><b>4.1</b> What about Barron and Kenny?</a></li>
<li class="chapter" data-level="4.2" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#confounding-and-causal-order"><i class="fa fa-check"></i><b>4.2</b> Confounding and causal order</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#accounting-for-confounding-and-epiphenomenal-association."><i class="fa fa-check"></i><b>4.2.1</b> Accounting for confounding and epiphenomenal association.</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#effect-size"><i class="fa fa-check"></i><b>4.3</b> Effect size</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#the-partially-standardized-effect."><i class="fa fa-check"></i><b>4.3.1</b> The partially standardized effect.</a></li>
<li class="chapter" data-level="4.3.2" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#the-completely-standardized-effect."><i class="fa fa-check"></i><b>4.3.2</b> The completely standardized effect.</a></li>
<li class="chapter" data-level="4.3.3" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#some-problematic-measures-only-for-indirect-effects."><i class="fa fa-check"></i><b>4.3.3</b> Some (problematic) measures only for indirect effects.</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#statistical-power"><i class="fa fa-check"></i><b>4.4</b> Statistical power</a></li>
<li class="chapter" data-level="4.5" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#multiple-xs-or-ys-analyze-separately-or-simultaneously"><i class="fa fa-check"></i><b>4.5</b> Multiple <span class="math inline">\(X\)</span>s or <span class="math inline">\(Y\)</span>s: Analyze separately or simultaneously?</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#multiple-x-variables."><i class="fa fa-check"></i><b>4.5.1</b> Multiple <span class="math inline">\(X\)</span> variables.</a></li>
<li class="chapter" data-level="4.5.2" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#estimation-of-a-model-with-multiple-x-variables-in-process-brms."><i class="fa fa-check"></i><b>4.5.2</b> Estimation of a model with multiple <span class="math inline">\(X\)</span> variables in <del>PROCESS</del> brms.</a></li>
<li class="chapter" data-level="4.5.3" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#multiple-y-variables."><i class="fa fa-check"></i><b>4.5.3</b> Multiple <span class="math inline">\(Y\)</span> variables.</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#chapter-summary"><i class="fa fa-check"></i><b>4.6</b> Chapter summary</a></li>
<li class="chapter" data-level="" data-path="causal-steps-confounding-and-causal-order.html"><a href="causal-steps-confounding-and-causal-order.html#session-info-3"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html"><i class="fa fa-check"></i><b>5</b> More Than One Mediator</a>
<ul>
<li class="chapter" data-level="5.1" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#the-parallel-multiple-mediator-model"><i class="fa fa-check"></i><b>5.1</b> The parallel multiple mediator model</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#direct-and-indirect-effects-in-a-parallel-multiple-mediator-model."><i class="fa fa-check"></i><b>5.1.1</b> Direct and indirect effects in a parallel multiple mediator model.</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#example-using-the-presumed-media-influence-study"><i class="fa fa-check"></i><b>5.2</b> Example using the presumed media influence study</a></li>
<li class="chapter" data-level="5.3" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#statistical-inference-2"><i class="fa fa-check"></i><b>5.3</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#inference-about-the-direct-and-total-effects."><i class="fa fa-check"></i><b>5.3.1</b> Inference about the direct and total effects.</a></li>
<li class="chapter" data-level="5.3.2" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#inference-about-specific-indirect-effects."><i class="fa fa-check"></i><b>5.3.2</b> Inference about specific indirect effects.</a></li>
<li class="chapter" data-level="5.3.3" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#pairwise-comparisons-between-specific-indirect-effects."><i class="fa fa-check"></i><b>5.3.3</b> Pairwise comparisons between specific indirect effects.</a></li>
<li class="chapter" data-level="5.3.4" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#inference-about-the-total-indirect-effect."><i class="fa fa-check"></i><b>5.3.4</b> Inference about the total indirect effect.</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#the-serial-multiple-mediator-model"><i class="fa fa-check"></i><b>5.4</b> The serial multiple mediator model</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#direct-and-indirect-effects-in-a-serial-multiple-mediator-model."><i class="fa fa-check"></i><b>5.4.1</b> Direct and indirect effects in a serial multiple mediator model.</a></li>
<li class="chapter" data-level="5.4.2" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#statistical-inference."><i class="fa fa-check"></i><b>5.4.2</b> Statistical inference.</a></li>
<li class="chapter" data-level="5.4.3" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#example-from-the-presumed-media-influence-study."><i class="fa fa-check"></i><b>5.4.3</b> Example from the presumed media influence study.</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#models-with-parallel-and-serial-mediation-properties"><i class="fa fa-check"></i><b>5.5</b> Models with parallel and serial mediation properties</a></li>
<li class="chapter" data-level="5.6" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#complementarity-and-competition-among-mediators"><i class="fa fa-check"></i><b>5.6</b> Complementarity and competition among mediators</a></li>
<li class="chapter" data-level="" data-path="more-than-one-mediator.html"><a href="more-than-one-mediator.html#session-info-4"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html"><i class="fa fa-check"></i><b>6</b> Mediation Analysis with a Multicategorical Antecedent</a>
<ul>
<li class="chapter" data-level="6.1" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#relative-total-direct-and-indirect-effects"><i class="fa fa-check"></i><b>6.1</b> Relative total, direct, and indirect effects</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#relative-indirect-effects."><i class="fa fa-check"></i><b>6.1.1</b> Relative indirect effects.</a></li>
<li class="chapter" data-level="6.1.2" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#relative-direct-effects."><i class="fa fa-check"></i><b>6.1.2</b> Relative direct effects.</a></li>
<li class="chapter" data-level="6.1.3" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#relative-total-effects."><i class="fa fa-check"></i><b>6.1.3</b> Relative total effects.</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#an-example-sex-discrimination-in-the-workplace"><i class="fa fa-check"></i><b>6.2</b> An example: Sex discrimination in the workplace</a></li>
<li class="chapter" data-level="6.3" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#using-a-different-group-coding-system"><i class="fa fa-check"></i><b>6.3</b> Using a different group coding system</a></li>
<li class="chapter" data-level="6.4" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#some-miscellaneous-issues-unrelated-to-those-hayes-covered-in-the-text"><i class="fa fa-check"></i><b>6.4</b> Some miscellaneous issues [unrelated to those Hayes covered in the text]</a></li>
<li class="chapter" data-level="" data-path="mediation-analysis-with-a-multicategorical-antecedent.html"><a href="mediation-analysis-with-a-multicategorical-antecedent.html#session-info-5"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="part"><span><b>III MODERATION ANALYSIS</b></span></li>
<li class="chapter" data-level="7" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html"><i class="fa fa-check"></i><b>7</b> Fundamentals of Moderation Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#conditional-and-unconditional-effects"><i class="fa fa-check"></i><b>7.1</b> Conditional and unconditional effects</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#eliminating-the-constraint-of-unconditionality."><i class="fa fa-check"></i><b>7.1.1</b> Eliminating the constraint of unconditionality.</a></li>
<li class="chapter" data-level="7.1.2" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#symmetry-in-moderation."><i class="fa fa-check"></i><b>7.1.2</b> Symmetry in moderation.</a></li>
<li class="chapter" data-level="7.1.3" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#interpretation-of-the-regression-coefficients."><i class="fa fa-check"></i><b>7.1.3</b> Interpretation of the regression coefficients.</a></li>
<li class="chapter" data-level="7.1.4" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#the-importance-of-b_3-wen-asking-about-mediation."><i class="fa fa-check"></i><b>7.1.4</b> The importance of <span class="math inline">\(b_3\)</span> wen asking about mediation.</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#an-example-climate-change-disasters-and-humanitarianism"><i class="fa fa-check"></i><b>7.2</b> An example: Climate change disasters and humanitarianism</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#estimation-using-process-brms."><i class="fa fa-check"></i><b>7.2.1</b> Estimation using <del>PROCESS</del> brms.</a></li>
<li class="chapter" data-level="7.2.2" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#interpreting-the-regression-coefficients."><i class="fa fa-check"></i><b>7.2.2</b> Interpreting the regression coefficients.</a></li>
<li class="chapter" data-level="7.2.3" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#variable-scaling-and-the-interpretation-of-b_1-and-b_3."><i class="fa fa-check"></i><b>7.2.3</b> Variable scaling and the interpretation of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_3\)</span>.</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#visualizing-moderation"><i class="fa fa-check"></i><b>7.3</b> Visualizing moderation</a></li>
<li class="chapter" data-level="7.4" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#probing-an-interaction"><i class="fa fa-check"></i><b>7.4</b> Probing an interaction</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#the-pick-a-point-approach."><i class="fa fa-check"></i><b>7.4.1</b> The pick-a-point approach.</a></li>
<li class="chapter" data-level="7.4.2" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#the-johnson-neyman-technique."><i class="fa fa-check"></i><b>7.4.2</b> The Johnson-Neyman technique.</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#the-difference-between-testing-for-moderation-and-probing-it"><i class="fa fa-check"></i><b>7.5</b> The difference between testing for moderation and probing it</a></li>
<li class="chapter" data-level="7.6" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#artificial-categorization-and-subgroups"><i class="fa fa-check"></i><b>7.6</b> Artificial categorization and subgroups</a></li>
<li class="chapter" data-level="" data-path="fundamentals-of-moderation-analysis.html"><a href="fundamentals-of-moderation-analysis.html#session-info-6"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html"><i class="fa fa-check"></i><b>8</b> Extending the Fundamental Principles of Moderation Analysis</a>
<ul>
<li class="chapter" data-level="8.1" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#moderation-with-a-dichotomous-moderator"><i class="fa fa-check"></i><b>8.1</b> Moderation with a dichotomous moderator</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#visualizing-and-probing-the-interaction."><i class="fa fa-check"></i><b>8.1.1</b> Visualizing and probing the interaction.</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#interaction-between-two-quantitative-variables"><i class="fa fa-check"></i><b>8.2</b> Interaction between two quantitative variables</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#visualizing-and-probing-the-interaction.-1"><i class="fa fa-check"></i><b>8.2.1</b> Visualizing and probing the interaction.</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#hierarchical-versus-simultaneous-entry"><i class="fa fa-check"></i><b>8.3</b> Hierarchical versus simultaneous entry</a></li>
<li class="chapter" data-level="8.4" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#the-equivalence-between-moderated-regression-analysis-and-a-2-x-2-factorial-analysis-of-variance"><i class="fa fa-check"></i><b>8.4</b> The equivalence between moderated regression analysis and a 2 X 2 factorial analysis of variance</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#simple-effects-parameterization."><i class="fa fa-check"></i><b>8.4.1</b> Simple effects parameterization.</a></li>
<li class="chapter" data-level="8.4.2" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#main-effects-parameterization."><i class="fa fa-check"></i><b>8.4.2</b> Main effects parameterization.</a></li>
<li class="chapter" data-level="8.4.3" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#conducting-a-2-times-2-between-participants-factorial-anova-using-process-another-regression-model-with-brms."><i class="fa fa-check"></i><b>8.4.3</b> Conducting <del>a <span class="math inline">\(2 \times 2\)</span> between-participants factorial ANOVA using PROCESS</del> another regression model with brms.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="extending-the-fundamental-principles-of-moderation-analysis.html"><a href="extending-the-fundamental-principles-of-moderation-analysis.html#session-info-7"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html"><i class="fa fa-check"></i><b>9</b> Some Myths and Additional Extensions of Moderation Analysis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#truths-and-myths-about-mean-centering"><i class="fa fa-check"></i><b>9.1</b> Truths and myths about mean-centering</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#the-effect-of-mean-centering-on-multicollinearity-and-the-standard-error-of-b_3."><i class="fa fa-check"></i><b>9.1.1</b> The effect of mean-centering on multicollinearity and the standard error of <span class="math inline">\(b_3\)</span>.</a></li>
<li class="chapter" data-level="9.1.2" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#the-effect-of-mean-centering-on-b_1-b_2-and-their-standard-errors-posterior-textitsds."><i class="fa fa-check"></i><b>9.1.2</b> The effect of mean-centering on <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span>, and their <del>standard errors</del> posterior <span class="math inline">\(\textit{SD}\)</span>’s.</a></li>
<li class="chapter" data-level="9.1.3" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#the-centering-option-in-process."><i class="fa fa-check"></i><b>9.1.3</b> The <del>centering option in PROCESS</del>.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#the-estimation-and-interpretation-of-standardized-regression-coefficients-in-a-moderation-analysis"><i class="fa fa-check"></i><b>9.2</b> The estimation and interpretation of standardized regression coefficients in a moderation analysis</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#variant-1."><i class="fa fa-check"></i><b>9.2.1</b> Variant 1.</a></li>
<li class="chapter" data-level="9.2.2" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#variant-2."><i class="fa fa-check"></i><b>9.2.2</b> Variant 2.</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#a-caution-on-manual-centering-and-standardization-because-of-missing-data"><i class="fa fa-check"></i><b>9.3</b> A caution on manual centering and standardization [because of missing data]</a></li>
<li class="chapter" data-level="9.4" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#more-than-one-moderator"><i class="fa fa-check"></i><b>9.4</b> More than one moderator</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#additive-multiple-moderation."><i class="fa fa-check"></i><b>9.4.1</b> Additive multiple moderation.</a></li>
<li class="chapter" data-level="9.4.2" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#moderated-moderation."><i class="fa fa-check"></i><b>9.4.2</b> Moderated moderation.</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#comparing-conditional-effects"><i class="fa fa-check"></i><b>9.5</b> Comparing conditional effects</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#comparing-conditional-effects-in-the-additive-multiple-moderation-model."><i class="fa fa-check"></i><b>9.5.1</b> Comparing conditional effects in the additive multiple moderation model.</a></li>
<li class="chapter" data-level="9.5.2" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#comparing-conditional-effects-in-the-moderated-moderation-model."><i class="fa fa-check"></i><b>9.5.2</b> Comparing conditional effects in the moderated moderation model.</a></li>
<li class="chapter" data-level="9.5.3" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#implementation-in-process-brms.-1"><i class="fa fa-check"></i><b>9.5.3</b> Implementation in <del>PROCESS</del> brms.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="some-myths-and-additional-extensions-of-moderation-analysis.html"><a href="some-myths-and-additional-extensions-of-moderation-analysis.html#session-info-8"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html"><i class="fa fa-check"></i><b>10</b> Multicategorical Focal Antecedents and Moderators</a>
<ul>
<li class="chapter" data-level="10.1" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#moderation-of-the-effect-of-a-multicategorical-antecedent-variable"><i class="fa fa-check"></i><b>10.1</b> Moderation of the effect of a multicategorical antecedent variable</a></li>
<li class="chapter" data-level="10.2" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#an-example-from-the-sex-disrimination-in-the-workplace-study"><i class="fa fa-check"></i><b>10.2</b> An example from the sex disrimination in the workplace study</a></li>
<li class="chapter" data-level="10.3" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#visualizing-the-model"><i class="fa fa-check"></i><b>10.3</b> Visualizing the model</a></li>
<li class="chapter" data-level="10.4" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#probing-the-interaction"><i class="fa fa-check"></i><b>10.4</b> Probing the interaction</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#the-pick-a-point-approach.-1"><i class="fa fa-check"></i><b>10.4.1</b> The pick-a-point approach.</a></li>
<li class="chapter" data-level="10.4.2" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#the-johnson-neyman-technique.-1"><i class="fa fa-check"></i><b>10.4.2</b> The Johnson-Neyman technique.</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#when-the-moderator-is-multicategorical"><i class="fa fa-check"></i><b>10.5</b> When the moderator is multicategorical</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#an-example."><i class="fa fa-check"></i><b>10.5.1</b> An example.</a></li>
<li class="chapter" data-level="10.5.2" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#probing-the-interaction-and-interpreting-the-regression-coefficients."><i class="fa fa-check"></i><b>10.5.2</b> Probing the interaction and interpreting the regression coefficients.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multicategorical-focal-antecedents-and-moderators.html"><a href="multicategorical-focal-antecedents-and-moderators.html#session-info-9"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="part"><span><b>IV CONDITIONAL PROCESS ANALYSIS</b></span></li>
<li class="chapter" data-level="11" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html"><i class="fa fa-check"></i><b>11</b> Fundamentals of Conditional Process Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#examples-of-conditional-process-models-in-the-literature"><i class="fa fa-check"></i><b>11.1</b> Examples of conditional process models in the literature</a></li>
<li class="chapter" data-level="11.2" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#conditional-direct-and-indirect-effects"><i class="fa fa-check"></i><b>11.2</b> Conditional direct and indirect effects</a></li>
<li class="chapter" data-level="11.3" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#example-hiding-your-feelings-from-your-work-team"><i class="fa fa-check"></i><b>11.3</b> Example: Hiding your feelings from your work team</a></li>
<li class="chapter" data-level="11.4" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#estimation-of-a-conditional-process-model-using-process"><i class="fa fa-check"></i><b>11.4</b> <del>Estimation of a conditional process model using PROCESS</del></a></li>
<li class="chapter" data-level="11.5" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#quantifying-and-visualizing-conditional-indirect-and-direct-effects."><i class="fa fa-check"></i><b>11.5</b> Quantifying and visualizing (conditional) indirect and direct effects.</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#visualizing-the-direct-and-indirect-effects."><i class="fa fa-check"></i><b>11.5.1</b> Visualizing the direct and indirect effects.</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#statistical-inference-3"><i class="fa fa-check"></i><b>11.6</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#inference-about-the-direct-effect."><i class="fa fa-check"></i><b>11.6.1</b> Inference about the direct effect.</a></li>
<li class="chapter" data-level="11.6.2" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#inference-about-the-indirect-effect."><i class="fa fa-check"></i><b>11.6.2</b> Inference about the indirect effect.</a></li>
<li class="chapter" data-level="11.6.3" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#probing-moderation-of-mediation."><i class="fa fa-check"></i><b>11.6.3</b> Probing moderation of mediation.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="fundamentals-of-conditional-process-analysis.html"><a href="fundamentals-of-conditional-process-analysis.html#session-info-10"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html"><i class="fa fa-check"></i><b>12</b> Further Examples of Conditional Process Analysis</a>
<ul>
<li class="chapter" data-level="12.1" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#revisiting-the-disaster-framing-study"><i class="fa fa-check"></i><b>12.1</b> Revisiting the disaster framing study</a></li>
<li class="chapter" data-level="12.2" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#moderation-of-the-direct-and-indirect-effects-in-a-conditional-process-model"><i class="fa fa-check"></i><b>12.2</b> Moderation of the direct and indirect effects in a conditional process model</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#estimation-using-process."><i class="fa fa-check"></i><b>12.2.1</b> <del>Estimation using PROCESS</del>.</a></li>
<li class="chapter" data-level="12.2.2" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#quantifying-direct-and-indirect-effects."><i class="fa fa-check"></i><b>12.2.2</b> Quantifying direct and indirect effects.</a></li>
<li class="chapter" data-level="12.2.3" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#visualizing-the-direct-and-indirect-effects.-1"><i class="fa fa-check"></i><b>12.2.3</b> Visualizing the direct and indirect effects.</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#statistical-inference-4"><i class="fa fa-check"></i><b>12.3</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#inference-about-the-direct-effect.-1"><i class="fa fa-check"></i><b>12.3.1</b> Inference about the direct effect.</a></li>
<li class="chapter" data-level="12.3.2" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#inference-about-the-indirect-effect.-1"><i class="fa fa-check"></i><b>12.3.2</b> Inference about the indirect effect.</a></li>
<li class="chapter" data-level="12.3.3" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#pruning-the-model."><i class="fa fa-check"></i><b>12.3.3</b> Pruning the model.</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#mediated-moderation"><i class="fa fa-check"></i><b>12.4</b> Mediated moderation</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#mediated-moderation-as-the-indirect-effect-of-a-product."><i class="fa fa-check"></i><b>12.4.1</b> Mediated moderation as the indirect effect of a product.</a></li>
<li class="chapter" data-level="12.4.2" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#why-mediated-moderation-is-neither-interesting-nor-meaningful."><i class="fa fa-check"></i><b>12.4.2</b> Why mediated moderation is neither interesting nor meaningful.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="further-examples-of-conditional-process-analysis.html"><a href="further-examples-of-conditional-process-analysis.html#session-info-11"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html"><i class="fa fa-check"></i><b>13</b> Conditional Process Analysis with a Multicategorical Antecedent</a>
<ul>
<li class="chapter" data-level="13.1" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#revisiting-sexual-discrimination-in-the-workplace"><i class="fa fa-check"></i><b>13.1</b> Revisiting sexual discrimination in the workplace</a></li>
<li class="chapter" data-level="13.2" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#looking-at-the-components-of-the-indirect-effect-of-x"><i class="fa fa-check"></i><b>13.2</b> Looking at the components of the indirect effect of <span class="math inline">\(X\)</span></a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#examining-the-first-stage-of-the-mediation-process."><i class="fa fa-check"></i><b>13.2.1</b> Examining the first stage of the mediation process.</a></li>
<li class="chapter" data-level="13.2.2" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#estimating-the-second-stage-of-the-mediation-process."><i class="fa fa-check"></i><b>13.2.2</b> Estimating the second stage of the mediation process.</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#relative-conditional-indirect-effects"><i class="fa fa-check"></i><b>13.3</b> Relative conditional indirect effects</a></li>
<li class="chapter" data-level="13.4" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#testing-and-probing-moderation-of-mediation"><i class="fa fa-check"></i><b>13.4</b> Testing and probing moderation of mediation</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#a-test-of-moderation-of-the-relative-indirect-effect."><i class="fa fa-check"></i><b>13.4.1</b> A test of moderation of the relative indirect effect.</a></li>
<li class="chapter" data-level="13.4.2" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#probing-moderation-of-mediation.-2"><i class="fa fa-check"></i><b>13.4.2</b> Probing moderation of mediation.</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#relative-conditional-direct-effects"><i class="fa fa-check"></i><b>13.5</b> Relative conditional direct effects</a></li>
<li class="chapter" data-level="" data-path="conditional-process-analysis-with-a-multicategorical-antecedent.html"><a href="conditional-process-analysis-with-a-multicategorical-antecedent.html#session-info-12"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="part"><span><b>V MISCELLANEA</b></span></li>
<li class="chapter" data-level="14" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html"><i class="fa fa-check"></i><b>14</b> Miscellaneous Topics and Some Frequently Asked Questions</a>
<ul>
<li class="chapter" data-level="14.1" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#a-strategy-for-approaching-a-conditional-process-analysis"><i class="fa fa-check"></i><b>14.1</b> A strategy for approaching a conditional process analysis</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-1-construct-your-conceptual-diagram-of-the-process."><i class="fa fa-check"></i><b>14.1.1</b> Step 1: Construct your conceptual diagram of the process.</a></li>
<li class="chapter" data-level="14.1.2" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-2-translate-the-conceptual-model-into-a-statistical-model."><i class="fa fa-check"></i><b>14.1.2</b> Step 2: Translate the conceptual model into a statistical model.</a></li>
<li class="chapter" data-level="14.1.3" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-3-estimate-the-statistical-model."><i class="fa fa-check"></i><b>14.1.3</b> Step 3: Estimate the statistical model.</a></li>
<li class="chapter" data-level="14.1.4" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-4-determine-whether-expected-moderation-exists."><i class="fa fa-check"></i><b>14.1.4</b> Step 4: Determine whether expected moderation exists.</a></li>
<li class="chapter" data-level="14.1.5" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-5-probe-and-interpret-interactions-involving-components-of-the-indirect-effect."><i class="fa fa-check"></i><b>14.1.5</b> Step 5: Probe and interpret interactions involving components of the indirect effect.</a></li>
<li class="chapter" data-level="14.1.6" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-6-quantify-and-test-conditional-indirect-effects-if-relevant."><i class="fa fa-check"></i><b>14.1.6</b> Step 6: Quantify and test conditional indirect effects (if relevant).</a></li>
<li class="chapter" data-level="14.1.7" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-7-quantify-and-test-conditional-direct-effects-if-relevant."><i class="fa fa-check"></i><b>14.1.7</b> Step 7: Quantify and test conditional direct effects (if relevant).</a></li>
<li class="chapter" data-level="14.1.8" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#step-8-tell-your-story."><i class="fa fa-check"></i><b>14.1.8</b> Step 8: Tell your story.</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#how-do-i-write-about-this"><i class="fa fa-check"></i><b>14.2</b> How do I write about this?</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#reporting-a-mediation-analysis."><i class="fa fa-check"></i><b>14.2.1</b> Reporting a mediation analysis.</a></li>
<li class="chapter" data-level="14.2.2" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#reporting-a-moderation-analysis."><i class="fa fa-check"></i><b>14.2.2</b> Reporting a moderation analysis.</a></li>
<li class="chapter" data-level="14.2.3" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#reporting-a-conditional-process-analysis."><i class="fa fa-check"></i><b>14.2.3</b> Reporting a conditional process analysis.</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#should-i-use-structural-equation-modeling-instead-of-regression-analysis"><i class="fa fa-check"></i><b>14.3</b> Should I use structural equation modeling instead of regression analysis?</a></li>
<li class="chapter" data-level="14.4" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#the-pitfalls-of-subgroups-analysis"><i class="fa fa-check"></i><b>14.4</b> The pitfalls of subgroups analysis</a></li>
<li class="chapter" data-level="14.5" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#can-a-variable-simultaneously-mediate-and-moderate-another-variables-effect"><i class="fa fa-check"></i><b>14.5</b> Can a variable simultaneously mediate and moderate another variable’s effect?</a></li>
<li class="chapter" data-level="14.6" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#interaction-between-x-and-m-in-mediation-analysis"><i class="fa fa-check"></i><b>14.6</b> Interaction between <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span> in mediation analysis?</a></li>
<li class="chapter" data-level="14.7" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#repeated-measures-designs"><i class="fa fa-check"></i><b>14.7</b> Repeated measures designs</a></li>
<li class="chapter" data-level="14.8" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#dichotomous-ordinal-count-and-survival-outcomes"><i class="fa fa-check"></i><b>14.8</b> Dichotomous, ordinal, count, and survival outcomes</a></li>
<li class="chapter" data-level="" data-path="miscellaneous-topics-and-some-frequently-asked-questions.html"><a href="miscellaneous-topics-and-some-frequently-asked-questions.html#session-info-13"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Recoding <em>Introduction to Mediation, Moderation, and Conditional Process Analysis</em></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="fundamentals-of-linear-regression" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Fundamentals of Linear Regression<a href="fundamentals-of-linear-regression.html#fundamentals-of-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<blockquote>
<p>Linear regression is the foundation of most of the methods [Hayes described in his] book, so a solid understanding of the fundamentals of linear regression is essential. I assume that most readers have been exposed to linear regression in some form before discovering this book, and so some of the material will be review. Even so, I encourage everyone to read this chapter. <span class="citation">(<a href="#ref-hayesIntroductionMediationModeration2018" role="doc-biblioref">Andrew F. Hayes, 2018, p. 30</a>)</span></p>
</blockquote>
<p>Since we’re adding Bayes and the <strong>tidyverse</strong> into the mix, walking through this chapter will be double important, for us.</p>
<div id="correlation-and-prediction" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Correlation and prediction<a href="fundamentals-of-linear-regression.html#correlation-and-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here we load a couple necessary packages, load the data, and take a peek.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="fundamentals-of-linear-regression.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb8-2"><a href="fundamentals-of-linear-regression.html#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="fundamentals-of-linear-regression.html#cb8-3" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/glbwarm/glbwarm.csv&quot;</span>)</span>
<span id="cb8-4"><a href="fundamentals-of-linear-regression.html#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="fundamentals-of-linear-regression.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(glbwarm)</span></code></pre></div>
<pre><code>## Rows: 815
## Columns: 7
## $ govact   &lt;dbl&gt; 3.6, 5.0, 6.6, 1.0, 4.0, 7.0, 6.8, 5.6, 6.0, 2.6, 1.4, 5.6, 7.0, 3.8, 3.4, 4.2, 1.0, 2.6, 5…
## $ posemot  &lt;dbl&gt; 3.67, 2.00, 2.33, 5.00, 2.33, 1.00, 2.33, 4.00, 5.00, 5.00, 1.00, 4.00, 1.00, 5.67, 3.00, 1…
## $ negemot  &lt;dbl&gt; 4.67, 2.33, 3.67, 5.00, 1.67, 6.00, 4.00, 5.33, 6.00, 2.00, 1.00, 4.00, 5.00, 4.67, 2.00, 1…
## $ ideology &lt;dbl&gt; 6, 2, 1, 1, 4, 3, 4, 5, 4, 7, 6, 4, 2, 4, 5, 2, 6, 4, 2, 4, 4, 2, 6, 4, 4, 3, 4, 5, 4, 5, 4…
## $ age      &lt;dbl&gt; 61, 55, 85, 59, 22, 34, 47, 65, 50, 60, 71, 60, 71, 59, 32, 36, 69, 70, 41, 48, 38, 63, 71,…
## $ sex      &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0…
## $ partyid  &lt;dbl&gt; 2, 1, 1, 1, 1, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1…</code></pre>
<p>If you are new to <strong>tidyverse</strong>-style syntax, possibly the oddest component is the pipe (i.e., <code>%&gt;%</code>). I’m not going to explain the <code>%&gt;%</code> in this project, but you might learn more about in <a href="https://www.youtube.com/watch?v=9yjhxvu-pDg">this brief clip</a>, starting around <a href="https://www.youtube.com/watch?v=K-ss_ag2k9E&amp;t=1285s">minute 21:25 in this talk by Wickham</a>, or in <a href="https://r4ds.had.co.nz/transform.html#combining-multiple-operations-with-the-pipe">Section 5.6.1</a> from <span class="citation">Grolemund &amp; Wickham (<a href="#ref-grolemundDataScience2017" role="doc-biblioref">2017</a>)</span>. Really, all of <a href="https://r4ds.had.co.nz/transform.html">Chapter 5</a> of <em>R4DS</em> is great for new <strong>R</strong> and new <strong>tidyverse</strong> users, and <a href="https://r4ds.had.co.nz/data-visualisation.html">Chapter 3</a> is a nice introduction to plotting with <strong>ggplot2</strong>.</p>
<p>Here is our version of Figure 2.1.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="fundamentals-of-linear-regression.html#cb10-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb10-2"><a href="fundamentals-of-linear-regression.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(negemot, govact) <span class="sc">%&gt;%</span> </span>
<span id="cb10-3"><a href="fundamentals-of-linear-regression.html#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-4"><a href="fundamentals-of-linear-regression.html#cb10-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-5"><a href="fundamentals-of-linear-regression.html#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> negemot, <span class="at">y =</span> govact, <span class="at">size =</span> n)) <span class="sc">+</span></span>
<span id="cb10-6"><a href="fundamentals-of-linear-regression.html#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">show.legend =</span> F) <span class="sc">+</span></span>
<span id="cb10-7"><a href="fundamentals-of-linear-regression.html#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;NEGEMOT: Negative emotions about climate change (&quot;</span>, <span class="fu">italic</span>(<span class="st">&quot;X&quot;</span>), <span class="st">&quot;)&quot;</span>)),</span>
<span id="cb10-8"><a href="fundamentals-of-linear-regression.html#cb10-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GOVACT: Support for governmentaction (&quot;</span>, <span class="fu">italic</span>(<span class="st">&quot;Y&quot;</span>), <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb10-9"><a href="fundamentals-of-linear-regression.html#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-3-1.png" width="480" /></p>
<p>There are other ways to handle the <a href="https://ggplot2.tidyverse.org/reference/position_jitter.html">overplotting issue, such as jittering</a>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="fundamentals-of-linear-regression.html#cb11-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb11-2"><a href="fundamentals-of-linear-regression.html#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> negemot, <span class="at">y =</span> govact)) <span class="sc">+</span></span>
<span id="cb11-3"><a href="fundamentals-of-linear-regression.html#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">height =</span> .<span class="dv">05</span>, <span class="at">width =</span> .<span class="dv">05</span>, </span>
<span id="cb11-4"><a href="fundamentals-of-linear-regression.html#cb11-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">alpha =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">size =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb11-5"><a href="fundamentals-of-linear-regression.html#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;NEGEMOT: Negative emotions about climate change (&quot;</span>, <span class="fu">italic</span>(<span class="st">&quot;X&quot;</span>), <span class="st">&quot;)&quot;</span>)),</span>
<span id="cb11-6"><a href="fundamentals-of-linear-regression.html#cb11-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GOVACT: Support for governmentaction (&quot;</span>, <span class="fu">italic</span>(<span class="st">&quot;Y&quot;</span>), <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb11-7"><a href="fundamentals-of-linear-regression.html#cb11-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-4-1.png" width="480" /></p>
<p>The <code>cor()</code> function is a simple way to compute a Pearson’s correlation coefficient.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="fundamentals-of-linear-regression.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(glbwarm<span class="sc">$</span>negemot, glbwarm<span class="sc">$</span>govact)</span></code></pre></div>
<pre><code>## [1] 0.5777458</code></pre>
<p>If you want more plentiful output, the <code>cor.test()</code> function returns a <span class="math inline">\(t\)</span>-value, the degrees of freedom, the corresponding <span class="math inline">\(p\)</span>-value and the 95% confidence intervals, in addition to the Pearson’s correlation coefficient.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="fundamentals-of-linear-regression.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>(glbwarm<span class="sc">$</span>negemot, glbwarm<span class="sc">$</span>govact)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  glbwarm$negemot and glbwarm$govact
## t = 20.183, df = 813, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.5301050 0.6217505
## sample estimates:
##       cor 
## 0.5777458</code></pre>
<p>To get the Bayesian version, we’ll open our focal statistical package, Bürkner’s <a href="https://github.com/paul-buerkner/brms"><strong>brms</strong></a>. I should briefly note that you could also do many of these analyses with other packages, such as <a href="https://cran.r-project.org/web/packages/blavaan/index.html"><strong>blavaan</strong></a> <span class="citation">(<a href="#ref-blavaan2021" role="doc-biblioref">Merkle et al., 2021</a>, <a href="#ref-R-blavaan" role="doc-biblioref">2022</a>; <a href="#ref-Merkle2018blavaan" role="doc-biblioref">Merkle &amp; Rosseel, 2018</a>)</span> or <a href="https://github.com/stan-dev/rstanarm"><strong>rstanarm</strong></a> <span class="citation">(<a href="#ref-rstanarm2018" role="doc-biblioref">Brilleman et al., 2018</a>; <a href="#ref-R-rstanarm" role="doc-biblioref">Gabry &amp; Goodrich, 2022</a>)</span>. I just prefer <strong>brms</strong>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="fundamentals-of-linear-regression.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(brms)</span></code></pre></div>
<p>We’ll start simple and just use the default priors and settings, but with the addition of parallel sampling via <code>cores = 4</code> and telling <strong>brms</strong> to save our output as an external file with the <code>file</code> argument.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="fundamentals-of-linear-regression.html#cb17-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb17-2"><a href="fundamentals-of-linear-regression.html#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb17-3"><a href="fundamentals-of-linear-regression.html#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb17-4"><a href="fundamentals-of-linear-regression.html#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bf</span>(<span class="fu">mvbind</span>(negemot, govact) <span class="sc">~</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb17-5"><a href="fundamentals-of-linear-regression.html#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_rescor</span>(<span class="cn">TRUE</span>),</span>
<span id="cb17-6"><a href="fundamentals-of-linear-regression.html#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb17-7"><a href="fundamentals-of-linear-regression.html#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.01&quot;</span>)</span></code></pre></div>
<p>We can examine a summary of the output with the <code>print()</code> function.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="fundamentals-of-linear-regression.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model2<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: negemot ~ 1 
##          govact ~ 1 
##    Data: glbwarm (Number of observations: 815) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## negemot_Intercept     3.56      0.05     3.45     3.66 1.00     3384     2769
## govact_Intercept      4.59      0.05     4.49     4.68 1.00     3339     2436
## 
## Family Specific Parameters: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_negemot     1.53      0.04     1.46     1.60 1.00     3377     3162
## sigma_govact      1.36      0.03     1.30     1.43 1.00     2925     3102
## 
## Residual Correlations: 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## rescor(negemot,govact)     0.58      0.02     0.53     0.62 1.00     3056     2514
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>In regression models, we typically use predictor variables to explain variation in our criterion variables. It’s pretty much never the case that our predictors explain all the variation. The variation that’s left over is often called residual variation, residual variance, residuals, error, or even <span class="math inline">\(\epsilon\)</span>. Throughout the text, Hayes typically referred to it as <span class="math inline">\(e\)</span>.</p>
<p>More formally, when we use likelihood-based estimators, such as maximum likelihood (popular with multilevel modeling and structural equation modeling) or Bayesian estimators, we express the models for our criterion variables in terms of likelihood functions. Probably the most common likelihood function, and the one consistent with the models in Hayes’s text, is the Gaussian likelihood. With that likelihood we say our criterion variable <span class="math inline">\(y_i\)</span> is normally distributed with a mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. We might express that formally as</p>
<p><span class="math display">\[
y_i \sim \operatorname{Normal} (\mu, \sigma),
\]</span></p>
<p>where <span class="math inline">\(\sim\)</span> stands for “is distributed as” and <span class="math inline">\(i\)</span> indexes the <span class="math inline">\(i\)</span>th row in the data. When we add predictors to the model, they are typically used to model the mean <span class="math inline">\(\mu\)</span>. Thus, in the case there we have a sole predictor <span class="math inline">\(x_i\)</span> which varies across participants, we’d expand our model formula to</p>
<p><span class="math display">\[\begin{align*}
y_i &amp; \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 x_i,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the intercept and <span class="math inline">\(\beta_1\)</span> is the slope for predictor <span class="math inline">\(x\)</span>, which varies across cases. In this formulation, <span class="math inline">\(\sigma\)</span> is the standard deviation after accounting for the systemic variation explained by <span class="math inline">\(x_i\)</span>. That is, it’s the residual variance (i.e., <span class="math inline">\(\epsilon\)</span>), but in a standard-deviation metric. <em>Why in a standard deviation metric?</em>, you ask. There are technical reasons why <strong>brms</strong> expresses it as a standard deviation which I’m not going to go into, here. Just beware that whereas many frequentist software packages express the residual variation in a variance metric, it’s expressed in a standard-deviation metric in <strong>brms</strong>. Just go with it and move on.</p>
<p>Within the <strong>brms</strong> framework, <span class="math inline">\(\sigma\)</span> of the Gaussian likelihood is considered a “family-specific” parameter. As it turns out, there are many other fine likelihood functions in addition to the Gaussian and not all of them have a <span class="math inline">\(\sigma\)</span> parameter. For example, there is no <span class="math inline">\(\sigma\)</span> for the Poisson <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> distribution, which is popular likelihood function for count variables. Because Bürkner made the <strong>brms</strong> package capable of a variety of likelihood functions, it behooved him to give this section of the output a generic name.</p>
<p>When you have a regression model with multiple Gaussian criterion variables, those variables will typically have some degree of covariation. It’s often termed <em>residual covariance</em>, particularly within the structural equation modeling paradigm <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. But when you have an intercept-only regression model with multiple variables, that residual covariance is just a covariance. And when you express those variation parameters in terms of standard deviations <span class="math inline">\(\sigma\)</span>, their covariance is expressed as a correlation <span class="math inline">\(\rho\)</span>. Since our multivariate model is of two variables, we have one <span class="math inline">\(\rho\)</span> parameter for the <span class="math inline">\(\sigma\)</span>’s, <code>rescor(negemot,govact)</code>, which is our Bayesian analogue to the Pearson’s correlation.</p>
<p>To learn more about the multivariate syntax in <strong>brms</strong>, check out Bürkner’s <span class="citation">(<a href="#ref-Bürkner2022Multivariate" role="doc-biblioref">2022b</a>)</span> vignette, <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html"><em>Estimating multivariate models with brms</em></a>, or execute <code>vignette("brms_multivariate")</code>. Or just hold your horses until we get into mediation. All of our mediation models will use one version of the multivariate syntax or another.</p>
<p>But to clarify the output, above:</p>
<ul>
<li>‘Estimate’ = the posterior mean, analogous to the frequentist point estimate,</li>
<li>‘Est.Error’ = the posterior <span class="math inline">\(\textit{SD}\)</span>, analogous to the frequentist standard error,</li>
<li>‘l-95% CI’ = the lower-level of the percentile-based 95% Bayesian credible interval, and</li>
<li>‘u-95% CI’ = the upper-level of the same.</li>
</ul>
</div>
<div id="the-simple-linear-regression-model" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> The simple linear regression model<a href="fundamentals-of-linear-regression.html#the-simple-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here is how one might get the simple OLS coefficients in base <strong>R</strong> with the <code>lm()</code> function.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="fundamentals-of-linear-regression.html#cb20-1" aria-hidden="true" tabindex="-1"></a>(model2<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">data =</span> glbwarm, govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> negemot))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = govact ~ 1 + negemot, data = glbwarm)
## 
## Coefficients:
## (Intercept)      negemot  
##      2.7573       0.5142</code></pre>
<p>For more detailed output, put the model object <code>model2.2</code> into the <code>summary()</code> function.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="fundamentals-of-linear-regression.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = govact ~ 1 + negemot, data = glbwarm)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3285 -0.6731  0.1018  0.7554  3.2142 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.75732    0.09866   27.95   &lt;2e-16 ***
## negemot      0.51424    0.02548   20.18   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.111 on 813 degrees of freedom
## Multiple R-squared:  0.3338, Adjusted R-squared:  0.333 
## F-statistic: 407.3 on 1 and 813 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here’s the Bayesian model in <strong>brms</strong>.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="fundamentals-of-linear-regression.html#cb24-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb24-2"><a href="fundamentals-of-linear-regression.html#cb24-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb24-3"><a href="fundamentals-of-linear-regression.html#cb24-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb24-4"><a href="fundamentals-of-linear-regression.html#cb24-4" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> negemot,</span>
<span id="cb24-5"><a href="fundamentals-of-linear-regression.html#cb24-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb24-6"><a href="fundamentals-of-linear-regression.html#cb24-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.03&quot;</span>)</span></code></pre></div>
<p>There are several ways to get a <strong>brms</strong> model summary. A go-to is with the <code>summary()</code> function, just like we did with our OLS version of the model.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="fundamentals-of-linear-regression.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2<span class="fl">.3</span>) </span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ 1 + negemot 
##    Data: glbwarm (Number of observations: 815) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.76      0.10     2.57     2.94 1.00     4183     2804
## negemot       0.51      0.02     0.47     0.56 1.00     4243     3058
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.11      0.03     1.06     1.17 1.00     3410     2874
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>The <code>print()</code> function works very much the same way. To get a more focused look, you can use the <code>posterior_summary()</code> function:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="fundamentals-of-linear-regression.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span>(model2<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>##                  Estimate   Est.Error          Q2.5         Q97.5
## b_Intercept     2.7589620 0.096779459     2.5709679     2.9429637
## b_negemot       0.5140993 0.024492016     0.4678335     0.5624802
## sigma           1.1127269 0.027565258     1.0620343     1.1679305
## lprior         -3.2741231 0.006381825    -3.2872712    -3.2626796
## lp__        -1245.9427589 1.226504608 -1249.1391499 -1244.5787873</code></pre>
<p>That also yields the log posterior, <code>lp__</code>, which you can learn more about in <a href="https://CRAN.R-project.org/package=rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient">this section</a> of the <span class="citation">(<a href="#ref-standevelopmentteamRStanInterfaceStan2023" role="doc-biblioref">2023</a>)</span> vignette by the Stan Development Team, <a href="https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html"><em>RStan: the R interface to Stan</em></a> or <a href="https://www.jax.org/news-and-insights/jax-blog/2015/october/lp-in-stan-output">this brief blog post</a> by Xulong Wang. We won’t focus on the <code>lp__</code> directly in this project. But its influence will be lurking in the shadows.</p>
<p>The output also contains a row for <code>lprior</code>. This is the log of the joint prior and is a new addition to the output based on <strong>brms</strong> version 2.17.0. It is related to functionality from the up-and-coming <a href="https://github.com/n-kall/priorsense">priorsense package</a> <span class="citation">(<a href="#ref-R-priorsense" role="doc-biblioref">Kallioinen et al., 2022</a>)</span>, which is based on new work, such as <span class="citation">Kallioinen et al. (<a href="#ref-kallioinen2021DetectingAndDiagnosing" role="doc-biblioref">2021</a>)</span>. Though you’ll see it pop up in our output from time to time, the <code>lprior</code> will also be outside of our focus in this project.</p>
<p>But anyways, the <code>Q2.5</code> and <code>Q97.5</code>, are the lower- and upper-levels of the 95% credible intervals. The <code>Q</code> prefix stands for quantile (<a href="https://github.com/paul-buerkner/brms/issues/425">see this thread</a>). In this case, these are a renamed version of the <code>l-95% CI</code> and <code>u-95% CI</code> columns from our <code>summary()</code> output.</p>
<p>To make a quick plot of the regression line, one can use the convenient <code>brms::conditional_effects()</code> function.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="fundamentals-of-linear-regression.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">conditional_effects</span>(model2<span class="fl">.3</span>)</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-13-1.png" width="432" /></p>
<p>If you want to customize that output, you might nest it in <code>plot()</code>.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="fundamentals-of-linear-regression.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">conditional_effects</span>(model2<span class="fl">.3</span>),</span>
<span id="cb30-2"><a href="fundamentals-of-linear-regression.html#cb30-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">points =</span> T,</span>
<span id="cb30-3"><a href="fundamentals-of-linear-regression.html#cb30-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">point_args =</span> <span class="fu">c</span>(<span class="at">height =</span> .<span class="dv">05</span>, <span class="at">width =</span> .<span class="dv">05</span>, </span>
<span id="cb30-4"><a href="fundamentals-of-linear-regression.html#cb30-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">alpha =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">size =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>))</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-14-1.png" width="432" /></p>
<p>It’s also useful to be able to work with the output of a <strong>brms</strong> model directly. For our first step, we’ll put our posterior draws into a data frame with the <code>as_draws_df()</code> function.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="fundamentals-of-linear-regression.html#cb31-1" aria-hidden="true" tabindex="-1"></a>draws <span class="ot">&lt;-</span> <span class="fu">as_draws_df</span>(model2<span class="fl">.3</span>)</span>
<span id="cb31-2"><a href="fundamentals-of-linear-regression.html#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="fundamentals-of-linear-regression.html#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(draws)</span></code></pre></div>
<pre><code>## # A draws_df: 6 iterations, 1 chains, and 5 variables
##   b_Intercept b_negemot sigma lprior  lp__
## 1         2.8      0.51   1.1   -3.3 -1245
## 2         2.7      0.56   1.1   -3.3 -1248
## 3         2.9      0.47   1.1   -3.3 -1246
## 4         2.8      0.49   1.1   -3.3 -1246
## 5         2.8      0.50   1.1   -3.3 -1245
## 6         2.7      0.54   1.1   -3.3 -1246
## # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;}</code></pre>
<p>Next, we’ll use the <code>fitted()</code> function to compute model-implied summaries for the expected <code>govact</code> value, given particular predictor values. Our first model only has <code>negemot</code> as a predictor, and we’ll ask for the expected <code>govact</code> values for <code>negemot</code> ranging from 0 to 7.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="fundamentals-of-linear-regression.html#cb33-1" aria-hidden="true" tabindex="-1"></a>nd <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">negemot =</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">30</span>))</span>
<span id="cb33-2"><a href="fundamentals-of-linear-regression.html#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="fundamentals-of-linear-regression.html#cb33-3" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span></span>
<span id="cb33-4"><a href="fundamentals-of-linear-regression.html#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fitted</span>(model2<span class="fl">.3</span>,</span>
<span id="cb33-5"><a href="fundamentals-of-linear-regression.html#cb33-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">newdata =</span> nd) <span class="sc">%&gt;%</span> </span>
<span id="cb33-6"><a href="fundamentals-of-linear-regression.html#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb33-7"><a href="fundamentals-of-linear-regression.html#cb33-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(nd)</span>
<span id="cb33-8"><a href="fundamentals-of-linear-regression.html#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="fundamentals-of-linear-regression.html#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(f)</span></code></pre></div>
<pre><code>##   Estimate  Est.Error     Q2.5    Q97.5   negemot
## 1 2.758962 0.09677946 2.570968 2.942964 0.0000000
## 2 2.883055 0.09142674 2.705143 3.059423 0.2413793
## 3 3.007148 0.08614717 2.838623 3.174034 0.4827586
## 4 3.131241 0.08095506 2.972113 3.287673 0.7241379
## 5 3.255334 0.07586836 3.106983 3.402155 0.9655172
## 6 3.379427 0.07090976 3.240109 3.516169 1.2068966</code></pre>
<p>The first two columns should look familiar to the output from <code>summary(model2.3)</code>, above. The next two columns, <code>Q2.5</code> and <code>Q97.5</code>, are the lower- and upper-levels of the 95% credible intervals, like we got from <code>posterior_summary()</code>. The final column is the result of the <code>bind_cols(nd)</code> code.</p>
<p>Here’s our bespoke version of Figure 2.4.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="fundamentals-of-linear-regression.html#cb35-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb35-2"><a href="fundamentals-of-linear-regression.html#cb35-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(negemot, govact) <span class="sc">%&gt;%</span> </span>
<span id="cb35-3"><a href="fundamentals-of-linear-regression.html#cb35-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb35-4"><a href="fundamentals-of-linear-regression.html#cb35-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb35-5"><a href="fundamentals-of-linear-regression.html#cb35-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> negemot)) <span class="sc">+</span></span>
<span id="cb35-6"><a href="fundamentals-of-linear-regression.html#cb35-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> govact, <span class="at">size =</span> n),</span>
<span id="cb35-7"><a href="fundamentals-of-linear-regression.html#cb35-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">show.legend =</span> F) <span class="sc">+</span></span>
<span id="cb35-8"><a href="fundamentals-of-linear-regression.html#cb35-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="at">data =</span> f,</span>
<span id="cb35-9"><a href="fundamentals-of-linear-regression.html#cb35-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">ymin =</span> Q2<span class="fl">.5</span>, <span class="at">ymax =</span> Q97<span class="fl">.5</span>),</span>
<span id="cb35-10"><a href="fundamentals-of-linear-regression.html#cb35-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">fill =</span> <span class="st">&quot;grey75&quot;</span>, <span class="at">alpha =</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb35-11"><a href="fundamentals-of-linear-regression.html#cb35-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> f,</span>
<span id="cb35-12"><a href="fundamentals-of-linear-regression.html#cb35-12" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">y =</span> Estimate)) <span class="sc">+</span></span>
<span id="cb35-13"><a href="fundamentals-of-linear-regression.html#cb35-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="fl">2.2</span>, <span class="at">y =</span> <span class="fl">7.5</span>, <span class="at">label =</span> <span class="st">&quot;Cases with positive residuals&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red3&quot;</span>) <span class="sc">+</span></span>
<span id="cb35-14"><a href="fundamentals-of-linear-regression.html#cb35-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="fl">4.75</span>, <span class="at">y =</span> .<span class="dv">8</span>, <span class="at">label =</span> <span class="st">&quot;Cases with negative residuals&quot;</span>, <span class="at">color =</span> <span class="st">&quot;blue3&quot;</span>) <span class="sc">+</span></span>
<span id="cb35-15"><a href="fundamentals-of-linear-regression.html#cb35-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="st">&quot;NEGEMOT: Negative emotions about climate change (&quot;</span><span class="sc">*</span><span class="fu">italic</span>(<span class="st">&quot;X&quot;</span>)<span class="sc">*</span><span class="st">&quot;)&quot;</span>),</span>
<span id="cb35-16"><a href="fundamentals-of-linear-regression.html#cb35-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="st">&quot;GOVACT: Support for governmentaction (&quot;</span><span class="sc">*</span><span class="fu">italic</span>(<span class="st">&quot;Y&quot;</span>)<span class="sc">*</span><span class="st">&quot;)&quot;</span>)) <span class="sc">+</span></span>
<span id="cb35-17"><a href="fundamentals-of-linear-regression.html#cb35-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">range</span>(glbwarm<span class="sc">$</span>negemot)) <span class="sc">+</span></span>
<span id="cb35-18"><a href="fundamentals-of-linear-regression.html#cb35-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-17-1.png" width="480" /></p>
<p>Note how that figure is a combination of the original <code>glbwarm</code> data and our <code>f</code> output.</p>
<div id="interpretation-of-the-constant-and-regression-coefficient." class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Interpretation of the constant and regression coefficient.<a href="fundamentals-of-linear-regression.html#interpretation-of-the-constant-and-regression-coefficient." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>“The regression constant is conceptually equivalent to the <span class="math inline">\(Y\)</span>-intercept in the equation for a line. It quantifies the estimated value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span>” (p. 39).</p>
</div>
<div id="the-standardized-regression-model." class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> The standardized regression model.<a href="fundamentals-of-linear-regression.html#the-standardized-regression-model." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Thus far, the interpretation of the regression coefficients in a regression model has been couched in <em>unstandardized</em> or <em>raw metric</em> form. Many regression routines will also produce a version of the model in <em>standardized</em> form. The standardized regression model is what results when all variables are first standardized prior to estimation of the model by expressing each measurement in units of standard deviations from the sample mean. (p. 40, <em>emphasis</em> in the original)</p>
</blockquote>
<p><strong>brms</strong> will not produce standardized solutions on the fly. To get them, you will have to manually standardize the variables before entering them into the model.</p>
</div>
<div id="simple-linear-regression-with-a-dichotomous-antecedent-variable." class="section level3 hasAnchor" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Simple linear regression with a dichotomous antecedent variable.<a href="fundamentals-of-linear-regression.html#simple-linear-regression-with-a-dichotomous-antecedent-variable." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <code>glbwarm</code> data, <code>sex</code> is a dichotomous variable.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="fundamentals-of-linear-regression.html#cb36-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb36-2"><a href="fundamentals-of-linear-regression.html#cb36-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> sex)) <span class="sc">+</span></span>
<span id="cb36-3"><a href="fundamentals-of-linear-regression.html#cb36-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>() <span class="sc">+</span></span>
<span id="cb36-4"><a href="fundamentals-of-linear-regression.html#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-18-1.png" width="384" /></p>
<p>In these data, <code>sex</code> is coded females = 0, males = 1. Here we add <code>sex</code> to the model.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="fundamentals-of-linear-regression.html#cb37-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb37-2"><a href="fundamentals-of-linear-regression.html#cb37-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb37-3"><a href="fundamentals-of-linear-regression.html#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb37-4"><a href="fundamentals-of-linear-regression.html#cb37-4" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> sex,</span>
<span id="cb37-5"><a href="fundamentals-of-linear-regression.html#cb37-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb37-6"><a href="fundamentals-of-linear-regression.html#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.04&quot;</span>)</span></code></pre></div>
<p>Check the summary.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="fundamentals-of-linear-regression.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model2<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ 1 + sex 
##    Data: glbwarm (Number of observations: 815) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.72      0.07     4.58     4.85 1.00     3967     2964
## sex          -0.27      0.10    -0.46    -0.08 1.00     3834     2662
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.36      0.03     1.29     1.43 1.00     4223     2883
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Our model summary is very close to that in the text. If you just wanted the coefficients, you might use the <code>fixef()</code> function.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="fundamentals-of-linear-regression.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.4</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##           Estimate Est.Error   Q2.5  Q97.5
## Intercept    4.716     0.067  4.585  4.847
## sex         -0.267     0.097 -0.456 -0.081</code></pre>
<p>Though not necessary, we used the <code>round()</code> function to reduce the number of significant digits in the output. You can get a little more information with the <code>posterior_summary()</code> function.</p>
<p>But since Bayesian estimation yields an entire posterior distribution, you can visualize that distribution in any number of ways. Because we’ll be using <strong>ggplot2</strong>, we’ll need to put the posterior draws into a data frame before plotting.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="fundamentals-of-linear-regression.html#cb42-1" aria-hidden="true" tabindex="-1"></a>draws <span class="ot">&lt;-</span> <span class="fu">as_draws_df</span>(model2<span class="fl">.4</span>)</span></code></pre></div>
<p>We could summarize the posterior with box plots</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="fundamentals-of-linear-regression.html#cb43-1" aria-hidden="true" tabindex="-1"></a>draws <span class="sc">%&gt;%</span> </span>
<span id="cb43-2"><a href="fundamentals-of-linear-regression.html#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">female =</span> b_Intercept) <span class="sc">%&gt;%</span> </span>
<span id="cb43-3"><a href="fundamentals-of-linear-regression.html#cb43-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">male =</span> female <span class="sc">+</span> b_sex) <span class="sc">%&gt;%</span> </span>
<span id="cb43-4"><a href="fundamentals-of-linear-regression.html#cb43-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(male, female)) <span class="sc">%&gt;%</span> </span>
<span id="cb43-5"><a href="fundamentals-of-linear-regression.html#cb43-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb43-6"><a href="fundamentals-of-linear-regression.html#cb43-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> name, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb43-7"><a href="fundamentals-of-linear-regression.html#cb43-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb43-8"><a href="fundamentals-of-linear-regression.html#cb43-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb43-9"><a href="fundamentals-of-linear-regression.html#cb43-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-22-1.png" width="288" /></p>
<p>or with overlapping density plots</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="fundamentals-of-linear-regression.html#cb44-1" aria-hidden="true" tabindex="-1"></a>draws <span class="sc">%&gt;%</span> </span>
<span id="cb44-2"><a href="fundamentals-of-linear-regression.html#cb44-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">female =</span> b_Intercept) <span class="sc">%&gt;%</span> </span>
<span id="cb44-3"><a href="fundamentals-of-linear-regression.html#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">male =</span> female <span class="sc">+</span> b_sex) <span class="sc">%&gt;%</span> </span>
<span id="cb44-4"><a href="fundamentals-of-linear-regression.html#cb44-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(male, female)) <span class="sc">%&gt;%</span></span>
<span id="cb44-5"><a href="fundamentals-of-linear-regression.html#cb44-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb44-6"><a href="fundamentals-of-linear-regression.html#cb44-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">group =</span> name, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb44-7"><a href="fundamentals-of-linear-regression.html#cb44-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>, <span class="at">alpha =</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb44-8"><a href="fundamentals-of-linear-regression.html#cb44-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-23-1.png" width="384" /></p>
<p>or even with violin plots with superimposed posterior medians and 95% intervals.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="fundamentals-of-linear-regression.html#cb45-1" aria-hidden="true" tabindex="-1"></a>draws <span class="sc">%&gt;%</span> </span>
<span id="cb45-2"><a href="fundamentals-of-linear-regression.html#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">female =</span> b_Intercept) <span class="sc">%&gt;%</span> </span>
<span id="cb45-3"><a href="fundamentals-of-linear-regression.html#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">male =</span> female <span class="sc">+</span> b_sex) <span class="sc">%&gt;%</span> </span>
<span id="cb45-4"><a href="fundamentals-of-linear-regression.html#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(male, female)) <span class="sc">%&gt;%</span></span>
<span id="cb45-5"><a href="fundamentals-of-linear-regression.html#cb45-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb45-6"><a href="fundamentals-of-linear-regression.html#cb45-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> name, <span class="at">y =</span> value)) <span class="sc">+</span></span>
<span id="cb45-7"><a href="fundamentals-of-linear-regression.html#cb45-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_violin</span>(<span class="fu">aes</span>(<span class="at">fill =</span> name), <span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>, <span class="at">alpha =</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb45-8"><a href="fundamentals-of-linear-regression.html#cb45-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_summary</span>(<span class="at">fun =</span> median,</span>
<span id="cb45-9"><a href="fundamentals-of-linear-regression.html#cb45-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">fun.min =</span> <span class="cf">function</span>(i){<span class="fu">quantile</span>(i, <span class="at">probs =</span> .<span class="dv">025</span>)},</span>
<span id="cb45-10"><a href="fundamentals-of-linear-regression.html#cb45-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">fun.max =</span> <span class="cf">function</span>(i){<span class="fu">quantile</span>(i, <span class="at">probs =</span> .<span class="dv">975</span>)}) <span class="sc">+</span></span>
<span id="cb45-11"><a href="fundamentals-of-linear-regression.html#cb45-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb45-12"><a href="fundamentals-of-linear-regression.html#cb45-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-24-1.png" width="288" /></p>
<p>For even more ideas, see <a href="https://twitter.com/mjskay">Matthew Kay</a>’s <a href="https://mjskay.github.io/tidybayes/articles/tidybayes.html"><strong>tidybayes</strong></a> and <a href="https://mjskay.github.io/ggdist/"><strong>ggdist</strong></a> packages.</p>
<p>As Hayes discussed on page 42, you can also get a sense of the model estimates for women and men with a little addition. Here we continue to use the <code>round()</code> function to simplify the output.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="fundamentals-of-linear-regression.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for women</span></span>
<span id="cb46-2"><a href="fundamentals-of-linear-regression.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">fixef</span>(model2<span class="fl">.4</span>)[<span class="dv">1</span>, ], <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##  Estimate Est.Error      Q2.5     Q97.5 
##      4.72      0.07      4.58      4.85</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="fundamentals-of-linear-regression.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for men (just the posterior mean)</span></span>
<span id="cb48-2"><a href="fundamentals-of-linear-regression.html#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">fixef</span>(model2<span class="fl">.4</span>)[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> <span class="fu">fixef</span>(model2<span class="fl">.4</span>)[<span class="dv">2</span>, <span class="dv">1</span>], <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 4.45</code></pre>
<p>Hayes then considered that</p>
<blockquote>
<p>although the model will always generate the group means, the regression coefficient and regression constant will depend on how the two groups are coded. For instance, suppose females were coded <span class="math inline">\(X = −1\)</span> and males were coded <span class="math inline">\(X = 1\)</span>. (p. 42)</p>
</blockquote>
<p>To follow along, we’ll first recode <code>sex</code>, saving it as <code>sex_recode</code>.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="fundamentals-of-linear-regression.html#cb50-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="ot">&lt;-</span> glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb50-2"><a href="fundamentals-of-linear-regression.html#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sex_recode =</span> <span class="fu">if_else</span>(sex <span class="sc">==</span> <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span></code></pre></div>
<p>Now fit the new model.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="fundamentals-of-linear-regression.html#cb51-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb51-2"><a href="fundamentals-of-linear-regression.html#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb51-3"><a href="fundamentals-of-linear-regression.html#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb51-4"><a href="fundamentals-of-linear-regression.html#cb51-4" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> sex_recode,</span>
<span id="cb51-5"><a href="fundamentals-of-linear-regression.html#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb51-6"><a href="fundamentals-of-linear-regression.html#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.05&quot;</span>)</span></code></pre></div>
<p>Check the primary coefficients.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="fundamentals-of-linear-regression.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.5</span>)</span></code></pre></div>
<pre><code>##              Estimate  Est.Error       Q2.5       Q97.5
## Intercept   4.5842056 0.04757457  4.4901617  4.67730937
## sex_recode -0.1349992 0.04797450 -0.2303585 -0.03947171</code></pre>
<p>They match up well with the results in the middle of page 42. Now the <code>Intercept</code> in the output, what Hayes called <span class="math inline">\(i_Y\)</span> is the unweighted mean of the means, <span class="math inline">\(\big (\overline Y_\text{male} + \overline Y_\text{female} \big ) \big / 2\)</span>, and the coefficient <code>sex_recode</code> (i.e., what Hayes called <span class="math inline">\(b\)</span>) is one half the difference between those means. Here’s how to work with the posterior draws from this model to reproduce the group mean estimates.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="fundamentals-of-linear-regression.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as_draws_df</span>(model2<span class="fl">.5</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb54-2"><a href="fundamentals-of-linear-regression.html#cb54-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">transmute</span>(<span class="at">male   =</span> b_Intercept <span class="sc">+</span> b_sex_recode <span class="sc">*</span> <span class="dv">1</span>,</span>
<span id="cb54-3"><a href="fundamentals-of-linear-regression.html#cb54-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">female =</span> b_Intercept <span class="sc">+</span> b_sex_recode <span class="sc">*</span> <span class="sc">-</span><span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb54-4"><a href="fundamentals-of-linear-regression.html#cb54-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb54-5"><a href="fundamentals-of-linear-regression.html#cb54-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-6"><a href="fundamentals-of-linear-regression.html#cb54-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">group =</span> name, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb54-7"><a href="fundamentals-of-linear-regression.html#cb54-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>, <span class="at">alpha =</span> <span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb54-8"><a href="fundamentals-of-linear-regression.html#cb54-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-28-1.png" width="384" /></p>
<p>You’ll see it looks just like the plot from above.</p>
<div id="a-caution-about-the-standardized-regression-coefficient." class="section level4 hasAnchor" number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> A caution about the standardized regression coefficient.<a href="fundamentals-of-linear-regression.html#a-caution-about-the-standardized-regression-coefficient." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<blockquote>
<p>The standardized regression coefficient is a function of both the mean difference and the distribution of the cases across the groups. This is an undesirable property of <span class="math inline">\(\tilde b\)</span> when <span class="math inline">\(X\)</span> is dichotomous. <em>I recommend that the standardized regression coefficient for a dichotomous antecedent variable not be interpreted or reported</em>.</p>
<p>If you desire an index of a mean difference in standard deviation units, I recommend standardizing <span class="math inline">\(Y\)</span> but not the dichotomous <span class="math inline">\(X\)</span> and then interpreting the <em>unstandardized</em> regression coefficient in a model estimating <span class="math inline">\(Z_Y\)</span> from <span class="math inline">\(X\)</span>. In such a model, <span class="math inline">\(b\)</span> is a <em>partially</em> standardized regression coefficient. (pp. 43–44, <em>emphasis</em> in the original)</p>
</blockquote>
<p>Here’s how to fit the partially-standardized model, first with <code>lm()</code>.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="fundamentals-of-linear-regression.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># standardize the Y</span></span>
<span id="cb55-2"><a href="fundamentals-of-linear-regression.html#cb55-2" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="ot">&lt;-</span> glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb55-3"><a href="fundamentals-of-linear-regression.html#cb55-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">govact_z =</span> (govact <span class="sc">-</span> <span class="fu">mean</span>(govact)) <span class="sc">/</span> <span class="fu">sd</span>(govact))</span>
<span id="cb55-4"><a href="fundamentals-of-linear-regression.html#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="fundamentals-of-linear-regression.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb55-6"><a href="fundamentals-of-linear-regression.html#cb55-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(<span class="at">data =</span> glbwarm, govact_z <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> sex) <span class="sc">%&gt;%</span> </span>
<span id="cb55-7"><a href="fundamentals-of-linear-regression.html#cb55-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># summarize</span></span>
<span id="cb55-8"><a href="fundamentals-of-linear-regression.html#cb55-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = govact_z ~ 1 + sex, data = glbwarm)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.7329 -0.5278  0.1104  0.6985  1.8746 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  0.09629    0.04876   1.975  0.04865 * 
## sex         -0.19717    0.06978  -2.826  0.00483 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9957 on 813 degrees of freedom
## Multiple R-squared:  0.009726,   Adjusted R-squared:  0.008508 
## F-statistic: 7.985 on 1 and 813 DF,  p-value: 0.004833</code></pre>
<p>Now we’ll fit the model as Bayesians with <code>brms::brm()</code>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="fundamentals-of-linear-regression.html#cb57-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb57-2"><a href="fundamentals-of-linear-regression.html#cb57-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb57-3"><a href="fundamentals-of-linear-regression.html#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb57-4"><a href="fundamentals-of-linear-regression.html#cb57-4" aria-hidden="true" tabindex="-1"></a>  govact_z <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> sex,</span>
<span id="cb57-5"><a href="fundamentals-of-linear-regression.html#cb57-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb57-6"><a href="fundamentals-of-linear-regression.html#cb57-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.06&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="fundamentals-of-linear-regression.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.6</span>)</span></code></pre></div>
<pre><code>##              Estimate  Est.Error          Q2.5       Q97.5
## Intercept  0.09780898 0.04818554  0.0002537162  0.19502714
## sex       -0.19805783 0.06898876 -0.3341843382 -0.06063056</code></pre>
<blockquote>
<p>The constant <span class="math inline">\(i_Y\)</span> is the mean standardized <span class="math inline">\(Y\)</span> for females, and <span class="math inline">\(b\)</span> is the mean difference between males and females in standard deviations of <span class="math inline">\(Y\)</span>. So men are estimated to differ from women by 0.197 standard deviations in their support for government action. The negative sign for <span class="math inline">\(b\)</span> means males are lower than females, on average, in their support. (p. 44)</p>
</blockquote>
<p>The parameter summaries from our Bayesian model was the same as the OLS summaries up to two decimal places. This will often be the case. One is not more correct. They are the results of using different procedures.</p>
</div>
</div>
<div id="a-note-on-symbolic-representations." class="section level3 hasAnchor" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> A note on symbolic representations.<a href="fundamentals-of-linear-regression.html#a-note-on-symbolic-representations." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section is worthy of repeating in full.</p>
<blockquote>
<p>A brief digression is in order at this point. It is important when reporting the results of an analysis to define the symbols you use unless there is a strong convention, for a failure to do so can invite confusion. Different uses of <span class="math inline">\(b\)</span> and <span class="math inline">\(\beta\)</span> in regression analysis are an important case in point. There is much inconsistency in the substantive and methodology literature as to how regression coefficients are symbolized in unstandardized versus standardized form. Some use <span class="math inline">\(b\)</span> or <span class="math inline">\(B\)</span> to refer to the unstandardized regression coefficient and <span class="math inline">\(\beta\)</span> to refer to the standardized regression coefficient. Others, rather than using <span class="math inline">\(\beta\)</span>, spell it out by referencing “beta weights” or just talk about the “betas.” Some use <span class="math inline">\(\beta\)</span> to refer to a population regression coefficient, to distinguish it from a sample estimate, others use <span class="math inline">\(\beta\)</span> as the unstandardized regression weight, and there are still others who use <span class="math inline">\(\hat \beta\)</span> to refer to a sample unstandardized regression coefficient and leave the hat off for its corresponding population or “true” value. In this book, I use <span class="math inline">\(\tilde \beta\)</span> for the standardized regression weight.</p>
<p>Ultimately, the symbols we use are for the most part arbitrary. We can use any symbols we want. My point is that you should not assume others will know what symbols you use mean, for your familiar symbols to represent certain concepts may not be understood as representing those concepts by all. The same applies to terms such as “beta coefficient” or other verbalizations of symbols. Best to define your symbols in advance, or otherwise let your reader know what your symbols mean when used in text and tables. This will help others better understand and interpret your work. (p. 44)</p>
</blockquote>
</div>
</div>
<div id="alternative-explanations-for-association" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Alternative explanations for association<a href="fundamentals-of-linear-regression.html#alternative-explanations-for-association" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>That “correlation does not imply causation” is etched into the brains of all scientists. If variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are correlated, that doesn’t mean that <span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span> or that <span class="math inline">\(Y\)</span> causes <span class="math inline">\(X\)</span>. The ability to infer cause–effect is not even a statistical matter in the end. Rather, it is the design of one’s study, the data collection procedures one employs, and theoretical plausibility that most directly influence whether a cause–effect claim can be made and with what degree of confidence, not the size or sign of a statistical index of association. (p. 45)</p>
</blockquote>
<p>On page 46, Hayes reported a couple correlations. Here’s how to get them from base <strong>R</strong>.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="fundamentals-of-linear-regression.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(glbwarm<span class="sc">$</span>sex, glbwarm<span class="sc">$</span>negemot)</span></code></pre></div>
<pre><code>## [1] -0.1173564</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="fundamentals-of-linear-regression.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(glbwarm<span class="sc">$</span>sex, glbwarm<span class="sc">$</span>govact)</span></code></pre></div>
<pre><code>## [1] -0.09861854</code></pre>
<p>Again, if we wanted to get full Bayesian estimates, we’d fit an intercept-only multivariate model.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="fundamentals-of-linear-regression.html#cb64-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.7</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb64-2"><a href="fundamentals-of-linear-regression.html#cb64-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb64-3"><a href="fundamentals-of-linear-regression.html#cb64-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb64-4"><a href="fundamentals-of-linear-regression.html#cb64-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bf</span>(<span class="fu">mvbind</span>(negemot, govact, sex) <span class="sc">~</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb64-5"><a href="fundamentals-of-linear-regression.html#cb64-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_rescor</span>(<span class="cn">TRUE</span>),</span>
<span id="cb64-6"><a href="fundamentals-of-linear-regression.html#cb64-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb64-7"><a href="fundamentals-of-linear-regression.html#cb64-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.07&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="fundamentals-of-linear-regression.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model2<span class="fl">.7</span>, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: negemot ~ 1 
##          govact ~ 1 
##          sex ~ 1 
##    Data: glbwarm (Number of observations: 815) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS
## negemot_Intercept    3.558     0.052    3.456    3.659 1.000     5210     3607
## govact_Intercept     4.587     0.046    4.495    4.680 1.001     5030     3475
## sex_Intercept        0.488     0.018    0.453    0.524 1.002     6910     2899
## 
## Family Specific Parameters: 
##               Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS
## sigma_negemot    1.532     0.039    1.457    1.610 1.001     6173     3221
## sigma_govact     1.364     0.034    1.299    1.431 1.000     5658     3362
## sigma_sex        0.501     0.013    0.477    0.527 1.002     7446     2803
## 
## Residual Correlations: 
##                        Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS
## rescor(negemot,govact)    0.576     0.024    0.528    0.621 1.000     5450     3211
## rescor(negemot,sex)      -0.117     0.034   -0.181   -0.050 1.001     7425     2856
## rescor(govact,sex)       -0.098     0.034   -0.164   -0.031 1.000     7115     2976
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>For our purposes, the action is in the ‘rescor(<span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>)’ portions of the ‘Family Specific Parameters’ section.</p>
<p>Anyway, if you wanted to get all the Pearson’s correlations among the <code>glbwarm</code> variables, rather than piecewise <code>cor()</code> approach, you could use the <code>lowerCor()</code> function from the <a href="https://cran.r-project.org/web/packages/psych/index.html"><strong>psych</strong> package</a> <span class="citation">(<a href="#ref-R-psych" role="doc-biblioref">Revelle, 2022</a>)</span>.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="fundamentals-of-linear-regression.html#cb67-1" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">lowerCor</span>(glbwarm[, <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>], <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##          govct  posmt  negmt  idlgy  age    sex    prtyd 
## govact    1.000                                          
## posemot   0.043  1.000                                   
## negemot   0.578  0.128  1.000                            
## ideology -0.418 -0.029 -0.349  1.000                     
## age      -0.097  0.042 -0.057  0.212  1.000              
## sex      -0.099  0.074 -0.117  0.133  0.166  1.000       
## partyid  -0.360 -0.036 -0.324  0.619  0.154  0.109  1.000</code></pre>
</div>
<div id="multiple-linear-regression" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Multiple linear regression<a href="fundamentals-of-linear-regression.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>The simple linear regression model is easily extended to the estimation of a consequent variable using more than one antecedent variable. Including more than one antecedent in a regression model allows you to simultaneously investigate the role of multiple influences on a consequent variable. An additional and important benefit of the multiple regression model is that it provides various measures of <em>partial association</em> that quantify the component of the association between an antecedent and a consequent that is unique to that antecedent relative to other antecedent variables in the model. (p. 48, <em>emphasis</em> in the original)</p>
</blockquote>
<p>Using Hayes’ notation, the linear model we’re about to fit follows the basic equation</p>
<p><span class="math display">\[\hat Y = i_Y + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4 + b_4 X_5.\]</span></p>
<p>For us, there’s technically more involved because our Bayesian paradigm includes priors, which we’re not focusing on at the moment. Anyway, there’s nothing particularly special about jumping from univariable to multivariable models with <strong>brms</strong>. You just keep tacking on predictors with the <code>+</code> operator.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="fundamentals-of-linear-regression.html#cb69-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.8</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb69-2"><a href="fundamentals-of-linear-regression.html#cb69-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb69-3"><a href="fundamentals-of-linear-regression.html#cb69-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb69-4"><a href="fundamentals-of-linear-regression.html#cb69-4" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> negemot <span class="sc">+</span> posemot <span class="sc">+</span> ideology <span class="sc">+</span> sex <span class="sc">+</span> age,</span>
<span id="cb69-5"><a href="fundamentals-of-linear-regression.html#cb69-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb69-6"><a href="fundamentals-of-linear-regression.html#cb69-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.08&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="fundamentals-of-linear-regression.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model2<span class="fl">.8</span>)</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ 1 + negemot + posemot + ideology + sex + age 
##    Data: glbwarm (Number of observations: 815) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.06      0.21     3.66     4.46 1.00     5255     3316
## negemot       0.44      0.03     0.39     0.49 1.00     5468     3047
## posemot      -0.03      0.03    -0.08     0.03 1.00     5267     3212
## ideology     -0.22      0.03    -0.27    -0.17 1.00     5270     3107
## sex          -0.01      0.08    -0.16     0.14 1.00     6013     3219
## age          -0.00      0.00    -0.01     0.00 1.00     5776     3240
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.07      0.03     1.02     1.12 1.00     6157     3074
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Following Hayes on pages 50 and 51, here is the posterior mean (i.e., what you might call the Bayesian point estimate) for someone with</p>
<ul>
<li>negative emotions = 3,</li>
<li>positive emotions = 4,</li>
<li><code>ideology</code> = 2,</li>
<li>is male (i.e., <code>sex</code> = 1), and</li>
<li>is 30 years of <code>age</code>.</li>
</ul>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="fundamentals-of-linear-regression.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">1</span>] <span class="sc">+</span> </span>
<span id="cb72-2"><a href="fundamentals-of-linear-regression.html#cb72-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">2</span>] <span class="sc">*</span> <span class="dv">3</span> <span class="sc">+</span> </span>
<span id="cb72-3"><a href="fundamentals-of-linear-regression.html#cb72-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">3</span>] <span class="sc">*</span> <span class="dv">4</span> <span class="sc">+</span> </span>
<span id="cb72-4"><a href="fundamentals-of-linear-regression.html#cb72-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">4</span>] <span class="sc">*</span> <span class="dv">2</span> <span class="sc">+</span> </span>
<span id="cb72-5"><a href="fundamentals-of-linear-regression.html#cb72-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">5</span>] <span class="sc">*</span> <span class="dv">1</span> <span class="sc">+</span> </span>
<span id="cb72-6"><a href="fundamentals-of-linear-regression.html#cb72-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">6</span>] <span class="sc">*</span> <span class="dv">30</span></span></code></pre></div>
<pre><code>## [1] 4.792134</code></pre>
<p>Here’s the same deal for a man of the same profile, but with one point higher on <code>negemot</code>.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="fundamentals-of-linear-regression.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">1</span>] <span class="sc">+</span> </span>
<span id="cb74-2"><a href="fundamentals-of-linear-regression.html#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">2</span>] <span class="sc">*</span> <span class="dv">4</span> <span class="sc">+</span> </span>
<span id="cb74-3"><a href="fundamentals-of-linear-regression.html#cb74-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">3</span>] <span class="sc">*</span> <span class="dv">4</span> <span class="sc">+</span> </span>
<span id="cb74-4"><a href="fundamentals-of-linear-regression.html#cb74-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">4</span>] <span class="sc">*</span> <span class="dv">2</span> <span class="sc">+</span> </span>
<span id="cb74-5"><a href="fundamentals-of-linear-regression.html#cb74-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">5</span>] <span class="sc">*</span> <span class="dv">1</span> <span class="sc">+</span> </span>
<span id="cb74-6"><a href="fundamentals-of-linear-regression.html#cb74-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="dv">6</span>] <span class="sc">*</span> <span class="dv">30</span></span></code></pre></div>
<pre><code>## [1] 5.233116</code></pre>
<p>If you want a full expression of the model uncertainty in terms of the shape of the posterior distribution and the 95% intervals, you’ll might use <code>as_draws_df()</code> and do a little data processing.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="fundamentals-of-linear-regression.html#cb76-1" aria-hidden="true" tabindex="-1"></a>draws <span class="ot">&lt;-</span> <span class="fu">as_draws_df</span>(model2<span class="fl">.8</span>)</span>
<span id="cb76-2"><a href="fundamentals-of-linear-regression.html#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="fundamentals-of-linear-regression.html#cb76-3" aria-hidden="true" tabindex="-1"></a>draws <span class="ot">&lt;-</span> draws <span class="sc">%&gt;%</span></span>
<span id="cb76-4"><a href="fundamentals-of-linear-regression.html#cb76-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">our_posterior =</span> b_Intercept <span class="sc">+</span> b_negemot <span class="sc">*</span> <span class="dv">4</span> <span class="sc">+</span> b_posemot <span class="sc">*</span> <span class="dv">4</span> <span class="sc">+</span> b_ideology <span class="sc">*</span> <span class="dv">2</span> <span class="sc">+</span> b_sex <span class="sc">*</span> <span class="dv">1</span> <span class="sc">+</span> b_age <span class="sc">*</span> <span class="dv">30</span>)</span>
<span id="cb76-5"><a href="fundamentals-of-linear-regression.html#cb76-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-6"><a href="fundamentals-of-linear-regression.html#cb76-6" aria-hidden="true" tabindex="-1"></a><span class="co"># this intermediary step will make it easier to specify the break points and their labels for the x-axis  </span></span>
<span id="cb76-7"><a href="fundamentals-of-linear-regression.html#cb76-7" aria-hidden="true" tabindex="-1"></a>post_summary <span class="ot">&lt;-</span></span>
<span id="cb76-8"><a href="fundamentals-of-linear-regression.html#cb76-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">quantile</span>(draws<span class="sc">$</span>our_posterior, <span class="at">probs =</span> <span class="fu">c</span>(.<span class="dv">025</span>, .<span class="dv">5</span>, .<span class="dv">975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb76-9"><a href="fundamentals-of-linear-regression.html#cb76-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb76-10"><a href="fundamentals-of-linear-regression.html#cb76-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">labels =</span> value <span class="sc">%&gt;%</span> </span>
<span id="cb76-11"><a href="fundamentals-of-linear-regression.html#cb76-11" aria-hidden="true" tabindex="-1"></a>           <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb76-12"><a href="fundamentals-of-linear-regression.html#cb76-12" aria-hidden="true" tabindex="-1"></a>           <span class="fu">as.character</span>())</span>
<span id="cb76-13"><a href="fundamentals-of-linear-regression.html#cb76-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-14"><a href="fundamentals-of-linear-regression.html#cb76-14" aria-hidden="true" tabindex="-1"></a><span class="co"># plot!</span></span>
<span id="cb76-15"><a href="fundamentals-of-linear-regression.html#cb76-15" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> draws,</span>
<span id="cb76-16"><a href="fundamentals-of-linear-regression.html#cb76-16" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x =</span> our_posterior)) <span class="sc">+</span></span>
<span id="cb76-17"><a href="fundamentals-of-linear-regression.html#cb76-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb76-18"><a href="fundamentals-of-linear-regression.html#cb76-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> post_summary<span class="sc">$</span>value,</span>
<span id="cb76-19"><a href="fundamentals-of-linear-regression.html#cb76-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="fu">c</span>(.<span class="dv">5</span>, .<span class="dv">75</span>, .<span class="dv">5</span>), <span class="at">linetype =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb76-20"><a href="fundamentals-of-linear-regression.html#cb76-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="cn">NULL</span>,</span>
<span id="cb76-21"><a href="fundamentals-of-linear-regression.html#cb76-21" aria-hidden="true" tabindex="-1"></a>                     <span class="at">breaks =</span> post_summary<span class="sc">$</span>value,</span>
<span id="cb76-22"><a href="fundamentals-of-linear-regression.html#cb76-22" aria-hidden="true" tabindex="-1"></a>                     <span class="at">labels =</span> post_summary<span class="sc">$</span>labels) <span class="sc">+</span></span>
<span id="cb76-23"><a href="fundamentals-of-linear-regression.html#cb76-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb76-24"><a href="fundamentals-of-linear-regression.html#cb76-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;The expected govact score for a 30-year-old man for</span><span class="sc">\n</span><span class="st">whom negemot and posemot both equal 4 and ideology</span><span class="sc">\n</span><span class="st">equals 2. The solid and dashed white vertical lines are the</span><span class="sc">\n</span><span class="st">posterior median and 95% intervals, respectively.&quot;</span>) <span class="sc">+</span></span>
<span id="cb76-25"><a href="fundamentals-of-linear-regression.html#cb76-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-37-1.png" width="384" /></p>
<p>In the text, Hayes showed that individuals based on these two profiles would be expected to differ by 0.441 (i.e., <span class="math inline">\(5.244 - 4.803 = 0.441\)</span>). That’s fine if you’re only working with OLS point estimates. But a proper Bayesian approach would express the difference in terms of an entire poster distribution, or at least a point estimate accompanied by some sort of intervals. Here we’ll just work with the posterior to create a difference distribution. You could do that with a little deft <code>as_draws_df()</code> wrangling. Here we’ll employ <code>fitted()</code>.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="fundamentals-of-linear-regression.html#cb77-1" aria-hidden="true" tabindex="-1"></a>nd <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb77-2"><a href="fundamentals-of-linear-regression.html#cb77-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">negemot  =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">4</span>),</span>
<span id="cb77-3"><a href="fundamentals-of-linear-regression.html#cb77-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">posemot  =</span> <span class="dv">4</span>,</span>
<span id="cb77-4"><a href="fundamentals-of-linear-regression.html#cb77-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">ideology =</span> <span class="dv">2</span>,</span>
<span id="cb77-5"><a href="fundamentals-of-linear-regression.html#cb77-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">sex      =</span> <span class="dv">1</span>,</span>
<span id="cb77-6"><a href="fundamentals-of-linear-regression.html#cb77-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">age      =</span> <span class="dv">30</span>)</span>
<span id="cb77-7"><a href="fundamentals-of-linear-regression.html#cb77-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-8"><a href="fundamentals-of-linear-regression.html#cb77-8" aria-hidden="true" tabindex="-1"></a><span class="fu">fitted</span>(model2<span class="fl">.8</span>, </span>
<span id="cb77-9"><a href="fundamentals-of-linear-regression.html#cb77-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">newdata =</span> nd,</span>
<span id="cb77-10"><a href="fundamentals-of-linear-regression.html#cb77-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">summary =</span> F) <span class="sc">%&gt;%</span> </span>
<span id="cb77-11"><a href="fundamentals-of-linear-regression.html#cb77-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb77-12"><a href="fundamentals-of-linear-regression.html#cb77-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="fu">str_c</span>(<span class="st">&quot;condition_&quot;</span>, letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])) <span class="sc">%&gt;%</span> </span>
<span id="cb77-13"><a href="fundamentals-of-linear-regression.html#cb77-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">difference =</span> condition_b <span class="sc">-</span> condition_a) <span class="sc">%&gt;%</span> </span>
<span id="cb77-14"><a href="fundamentals-of-linear-regression.html#cb77-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb77-15"><a href="fundamentals-of-linear-regression.html#cb77-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> difference)) <span class="sc">+</span></span>
<span id="cb77-16"><a href="fundamentals-of-linear-regression.html#cb77-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;black&quot;</span>, <span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>) <span class="sc">+</span></span>
<span id="cb77-17"><a href="fundamentals-of-linear-regression.html#cb77-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb77-18"><a href="fundamentals-of-linear-regression.html#cb77-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;The posterior density for the difference between</span><span class="sc">\n</span><span class="st">the two conditions.&quot;</span>) <span class="sc">+</span></span>
<span id="cb77-19"><a href="fundamentals-of-linear-regression.html#cb77-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-38-1.png" width="384" /></p>
<div id="the-standardized-regression-model.-1" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> The standardized regression model.<a href="fundamentals-of-linear-regression.html#the-standardized-regression-model.-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As <strong>brms</strong> doesn’t automatically give us the standardized coefficients the way OLS output often does, we’ll have to be proactive. One solution is just to standardized the data themselves and then re-fit the model with those standardized variables. That leads us to the issue of how one standardized variables to begin with. Recall that standardizing entails subtracting the mean of a variable from that variable and then dividing that value by the standard deviation. We don’t want to do that by hand. So one handy way is to make a custom function to do that work for us.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="fundamentals-of-linear-regression.html#cb78-1" aria-hidden="true" tabindex="-1"></a>standardize <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb78-2"><a href="fundamentals-of-linear-regression.html#cb78-2" aria-hidden="true" tabindex="-1"></a>  (x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">/</span> <span class="fu">sd</span>(x)</span>
<span id="cb78-3"><a href="fundamentals-of-linear-regression.html#cb78-3" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>To learn more about making custom functions in <strong>R</strong>, check our <a href="https://r4ds.had.co.nz/functions.html">Chapter 19</a> of <em>R4DS</em>.</p>
<p>Here we’ll employ our custom <code>standardize()</code> function to make standardized versions of our variables.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="fundamentals-of-linear-regression.html#cb79-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="ot">&lt;-</span> glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb79-2"><a href="fundamentals-of-linear-regression.html#cb79-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">posemot_z  =</span> <span class="fu">standardize</span>(posemot), </span>
<span id="cb79-3"><a href="fundamentals-of-linear-regression.html#cb79-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">negemot_z  =</span> <span class="fu">standardize</span>(negemot), </span>
<span id="cb79-4"><a href="fundamentals-of-linear-regression.html#cb79-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">ideology_z =</span> <span class="fu">standardize</span>(ideology),</span>
<span id="cb79-5"><a href="fundamentals-of-linear-regression.html#cb79-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">sex_z      =</span> <span class="fu">standardize</span>(sex),</span>
<span id="cb79-6"><a href="fundamentals-of-linear-regression.html#cb79-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">age_z      =</span> <span class="fu">standardize</span>(age))</span></code></pre></div>
<p>Now we’ve got us our standardized variables, let’s fit a standardized model.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="fundamentals-of-linear-regression.html#cb80-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.9</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb80-2"><a href="fundamentals-of-linear-regression.html#cb80-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb80-3"><a href="fundamentals-of-linear-regression.html#cb80-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb80-4"><a href="fundamentals-of-linear-regression.html#cb80-4" aria-hidden="true" tabindex="-1"></a>  govact_z <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> negemot_z <span class="sc">+</span> posemot_z <span class="sc">+</span> ideology_z <span class="sc">+</span> sex_z <span class="sc">+</span> age_z,</span>
<span id="cb80-5"><a href="fundamentals-of-linear-regression.html#cb80-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb80-6"><a href="fundamentals-of-linear-regression.html#cb80-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.09&quot;</span>)</span></code></pre></div>
<p>Here are the newly standardized coefficient summaries, minus the <code>Intercept</code>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="fundamentals-of-linear-regression.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.9</span>)[<span class="sc">-</span><span class="dv">1</span>, ] <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">3</span>)</span></code></pre></div>
<pre><code>##            Estimate Est.Error   Q2.5  Q97.5
## negemot_z     0.495     0.030  0.436  0.554
## posemot_z    -0.027     0.028 -0.083  0.029
## ideology_z   -0.242     0.031 -0.303 -0.182
## sex_z        -0.004     0.029 -0.061  0.053
## age_z        -0.016     0.029 -0.072  0.041</code></pre>
<p>Our coefficients match up nicely with those on page 52 in the text. Just as with Hayes’s OLS estimates, we should not attempt to interpret the standardized <code>sex_z</code> coefficient from our Bayesian model.</p>
<p>Here’s how we’d fit a <em>partially</em>-standardized model–a model in which all variables except for <code>sex</code> are standardized.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="fundamentals-of-linear-regression.html#cb83-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.10</span> <span class="ot">&lt;-</span> <span class="fu">update</span>(</span>
<span id="cb83-2"><a href="fundamentals-of-linear-regression.html#cb83-2" aria-hidden="true" tabindex="-1"></a>  model2<span class="fl">.9</span>,</span>
<span id="cb83-3"><a href="fundamentals-of-linear-regression.html#cb83-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">newdata =</span> glbwarm,</span>
<span id="cb83-4"><a href="fundamentals-of-linear-regression.html#cb83-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> govact_z <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> negemot_z <span class="sc">+</span> posemot_z <span class="sc">+</span> ideology_z <span class="sc">+</span> sex <span class="sc">+</span> age_z,</span>
<span id="cb83-5"><a href="fundamentals-of-linear-regression.html#cb83-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb83-6"><a href="fundamentals-of-linear-regression.html#cb83-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.10&quot;</span>)</span></code></pre></div>
<p>Notice our use of the <code>update()</code> function. If you want to hastily fit a <strong>brms</strong> model of the same basic form of a prior model, you can just update some of the parameters of that original fit. In this case, all we did was swap out one of the predictors. To accommodate that change, we used the <code>newdata</code> argument so the model had access to the new variable and then we fed in the new formula.</p>
<p>Anyway, here are the coefficient summaries, including the <code>Intercept</code>, for the partially-standardized model.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="fundamentals-of-linear-regression.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.10</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##            Estimate Est.Error   Q2.5  Q97.5
## Intercept     0.004     0.039 -0.071  0.081
## negemot_z     0.496     0.030  0.438  0.554
## posemot_z    -0.026     0.028 -0.082  0.027
## ideology_z   -0.243     0.030 -0.302 -0.186
## sex          -0.007     0.056 -0.117  0.103
## age_z        -0.016     0.028 -0.070  0.037</code></pre>
<p>As Hayes wrote, now <code>sex</code> = -0.007 has a sensible interpretation. “We can say that men and women differ by [-0.007] standard deviations in their support for government action when all other variables in the model are held constant (p. 53).”</p>
<p>On page 54, Hayes gave us the equation to transform unstandardized coefficients to standardized ones:</p>
<p><span class="math display">\[
\tilde b_i = b_i \left ( \frac{\textit{SD}_{X_{i}}}{\textit{SD}_{Y}} \right )
\]</span></p>
<p>Let’s give it a whirl with <code>negemot</code>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="fundamentals-of-linear-regression.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here&#39;s the coefficient for `negemot` from the standardized model, `model2.9`</span></span>
<span id="cb86-2"><a href="fundamentals-of-linear-regression.html#cb86-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.9</span>)[<span class="st">&quot;negemot_z&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</span></code></pre></div>
<pre><code>## [1] 0.4954079</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="fundamentals-of-linear-regression.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># here&#39;s the coefficient for `negemot` from the unstandardized model, `model2.8`</span></span>
<span id="cb88-2"><a href="fundamentals-of-linear-regression.html#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="st">&quot;negemot&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</span></code></pre></div>
<pre><code>## [1] 0.4409823</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="fundamentals-of-linear-regression.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="co"># and here we use Hayes&#39; formula to standardize the unstandardized coefficient</span></span>
<span id="cb90-2"><a href="fundamentals-of-linear-regression.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.8</span>)[<span class="st">&quot;negemot&quot;</span>, <span class="st">&quot;Estimate&quot;</span>] <span class="sc">*</span> (<span class="fu">sd</span>(glbwarm<span class="sc">$</span>negemot) <span class="sc">/</span> <span class="fu">sd</span>(glbwarm<span class="sc">$</span>govact))</span></code></pre></div>
<pre><code>## [1] 0.4954375</code></pre>
<p>Looks like we got it within rounding error–pretty good! However, that was just the posterior mean, the Bayesian point estimate. If we want to more fully express the uncertainty around the mean–and we do–, we’ll need to work with the posterior draws.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="fundamentals-of-linear-regression.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the posterior draws from the unstandardized model</span></span>
<span id="cb92-2"><a href="fundamentals-of-linear-regression.html#cb92-2" aria-hidden="true" tabindex="-1"></a><span class="fu">as_draws_df</span>(model2<span class="fl">.8</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb92-3"><a href="fundamentals-of-linear-regression.html#cb92-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># using Hayes&#39; formula to standardize `b_negemot`</span></span>
<span id="cb92-4"><a href="fundamentals-of-linear-regression.html#cb92-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="st">`</span><span class="at">hand-made b_negemot_z</span><span class="st">`</span> <span class="ot">=</span> b_negemot <span class="sc">*</span> (<span class="fu">sd</span>(glbwarm<span class="sc">$</span>negemot) <span class="sc">/</span> <span class="fu">sd</span>(glbwarm<span class="sc">$</span>govact))) <span class="sc">%&gt;%</span> </span>
<span id="cb92-5"><a href="fundamentals-of-linear-regression.html#cb92-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># tacking on the `b_negemot_z` column from the standardized `model2.9` models posterior draws</span></span>
<span id="cb92-6"><a href="fundamentals-of-linear-regression.html#cb92-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">as_draws_df</span>(model2<span class="fl">.9</span>) <span class="sc">%&gt;%</span></span>
<span id="cb92-7"><a href="fundamentals-of-linear-regression.html#cb92-7" aria-hidden="true" tabindex="-1"></a>              <span class="fu">select</span>(b_negemot_z)) <span class="sc">%&gt;%</span> </span>
<span id="cb92-8"><a href="fundamentals-of-linear-regression.html#cb92-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># converting the data to the long format and grouping by `name`</span></span>
<span id="cb92-9"><a href="fundamentals-of-linear-regression.html#cb92-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">c</span>(<span class="st">`</span><span class="at">hand-made b_negemot_z</span><span class="st">`</span>, b_negemot_z)) <span class="sc">%&gt;%</span> </span>
<span id="cb92-10"><a href="fundamentals-of-linear-regression.html#cb92-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(name) <span class="sc">%&gt;%</span> </span>
<span id="cb92-11"><a href="fundamentals-of-linear-regression.html#cb92-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># here we summarize the results</span></span>
<span id="cb92-12"><a href="fundamentals-of-linear-regression.html#cb92-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(value),</span>
<span id="cb92-13"><a href="fundamentals-of-linear-regression.html#cb92-13" aria-hidden="true" tabindex="-1"></a>            <span class="at">sd =</span> <span class="fu">sd</span>(value),</span>
<span id="cb92-14"><a href="fundamentals-of-linear-regression.html#cb92-14" aria-hidden="true" tabindex="-1"></a>            <span class="at">ll =</span> <span class="fu">quantile</span>(value, <span class="at">probs =</span> .<span class="dv">025</span>),</span>
<span id="cb92-15"><a href="fundamentals-of-linear-regression.html#cb92-15" aria-hidden="true" tabindex="-1"></a>            <span class="at">ul =</span> <span class="fu">quantile</span>(value, <span class="at">probs =</span> .<span class="dv">975</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb92-16"><a href="fundamentals-of-linear-regression.html#cb92-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.double, round, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   name                   mean    sd    ll    ul
##   &lt;chr&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 b_negemot_z           0.495  0.03 0.436 0.554
## 2 hand-made b_negemot_z 0.495  0.03 0.437 0.553</code></pre>
<p>Our summary confirms that we can apply Hayes’s formula to a <code>as_draws_df()</code> column in order to get fuller summary statistics for a hand-converted standardized coefficient. This would be in full compliance with, say, APA recommendations to include 95% intervals with all effect sizes–the standardized regression coefficient being the effect size, here.</p>
</div>
</div>
<div id="measures-of-model-fit" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Measures of model fit<a href="fundamentals-of-linear-regression.html#measures-of-model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the Bayesian world, we don’t tend to appeal to the <span class="math inline">\(\textit{SS}_{residual}\)</span>, the <span class="math inline">\(\textit{MS}_{residual}\)</span>, or the standard error of estimate. We do sometimes, however, appeal to the <span class="math inline">\(R^2\)</span>. I’m not going to go into the technical details here, but you should be aware that the Bayesian <span class="math inline">\(R^2\)</span> is not calculated the same as the OLS <span class="math inline">\(R^2\)</span>. If you want to dive in, check out the <span class="citation">(<a href="#ref-gelmanRsquaredBayesianRegression2019" role="doc-biblioref">2019</a>)</span> paper by Gelman, Goodrich, Gabry, and Vehtari, <a href="https://doi.org/10.1080/00031305.2018.1549100"><em>R-squared for Bayesian regression models</em></a>. Here’s how to get it with <strong>brms</strong>.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="fundamentals-of-linear-regression.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bayes_R2</span>(model2<span class="fl">.8</span>, <span class="at">summary =</span> T) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##    Estimate Est.Error  Q2.5 Q97.5
## R2    0.388     0.021 0.346 0.428</code></pre>
<p>It even comes with 95% intervals, which will make the editors at APA journals happy. If you want to go beyond summary statistics and take a look at the full posterior, just set <code>summary = F</code> and data wrangle and plot as usual.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="fundamentals-of-linear-regression.html#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bayes_R2</span>(model2<span class="fl">.8</span>, <span class="at">summary =</span> F) <span class="sc">%&gt;%</span> </span>
<span id="cb96-2"><a href="fundamentals-of-linear-regression.html#cb96-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb96-3"><a href="fundamentals-of-linear-regression.html#cb96-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb96-4"><a href="fundamentals-of-linear-regression.html#cb96-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> R2)) <span class="sc">+</span></span>
<span id="cb96-5"><a href="fundamentals-of-linear-regression.html#cb96-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;black&quot;</span>, <span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>) <span class="sc">+</span></span>
<span id="cb96-6"><a href="fundamentals-of-linear-regression.html#cb96-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb96-7"><a href="fundamentals-of-linear-regression.html#cb96-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Behold: The Bayesian &quot;</span>, <span class="fu">italic</span>(<span class="st">&quot;R&quot;</span>)<span class="sc">^</span>{<span class="dv">2</span>}, <span class="st">&quot; distribution for model 2.8&quot;</span>)),</span>
<span id="cb96-8"><a href="fundamentals-of-linear-regression.html#cb96-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">x     =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb96-9"><a href="fundamentals-of-linear-regression.html#cb96-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb96-10"><a href="fundamentals-of-linear-regression.html#cb96-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-46-1.png" width="576" /></p>
<p>Another way we examine model fit is with graphical posterior predictive checks. Posterior predictive checking is a very general approach, which you might learn more about from <span class="citation">Gabry et al. (<a href="#ref-gabry2019visualization" role="doc-biblioref">2019</a>)</span> or with a few keyword searches in on <a href="https://statmodeling.stat.columbia.edu/">Gelman’s blog</a>. One basic way is to use the model in order to simulate data and then compare those data with the original data–the basic idea being that good fitting models should produce data similar to the original data.</p>
<p>Recall how we’ve used <code>fitted()</code> to make regression lines and expected values? We’ll, now we’ll use <code>predict()</code> to simulate data based on our models.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="fundamentals-of-linear-regression.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb97-2"><a href="fundamentals-of-linear-regression.html#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(model2<span class="fl">.8</span>,</span>
<span id="cb97-3"><a href="fundamentals-of-linear-regression.html#cb97-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">summary =</span> F,</span>
<span id="cb97-4"><a href="fundamentals-of-linear-regression.html#cb97-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">nsamples =</span> <span class="dv">3</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb97-5"><a href="fundamentals-of-linear-regression.html#cb97-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb97-6"><a href="fundamentals-of-linear-regression.html#cb97-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(glbwarm)) <span class="sc">%&gt;%</span></span>
<span id="cb97-7"><a href="fundamentals-of-linear-regression.html#cb97-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">simulation =</span> <span class="fu">str_c</span>(<span class="st">&quot;simulation: &quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="fu">n</span>())) <span class="sc">%&gt;%</span> </span>
<span id="cb97-8"><a href="fundamentals-of-linear-regression.html#cb97-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>simulation,</span>
<span id="cb97-9"><a href="fundamentals-of-linear-regression.html#cb97-9" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;row&quot;</span>,</span>
<span id="cb97-10"><a href="fundamentals-of-linear-regression.html#cb97-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&quot;govact&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb97-11"><a href="fundamentals-of-linear-regression.html#cb97-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(</span>
<span id="cb97-12"><a href="fundamentals-of-linear-regression.html#cb97-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_rows</span>(</span>
<span id="cb97-13"><a href="fundamentals-of-linear-regression.html#cb97-13" aria-hidden="true" tabindex="-1"></a>      glbwarm <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>govact),</span>
<span id="cb97-14"><a href="fundamentals-of-linear-regression.html#cb97-14" aria-hidden="true" tabindex="-1"></a>      glbwarm <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>govact),</span>
<span id="cb97-15"><a href="fundamentals-of-linear-regression.html#cb97-15" aria-hidden="true" tabindex="-1"></a>      glbwarm <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>govact))</span>
<span id="cb97-16"><a href="fundamentals-of-linear-regression.html#cb97-16" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb97-17"><a href="fundamentals-of-linear-regression.html#cb97-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb97-18"><a href="fundamentals-of-linear-regression.html#cb97-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> negemot, <span class="at">y =</span> govact)) <span class="sc">+</span></span>
<span id="cb97-19"><a href="fundamentals-of-linear-regression.html#cb97-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">height =</span> .<span class="dv">05</span>, <span class="at">width =</span> .<span class="dv">05</span>, </span>
<span id="cb97-20"><a href="fundamentals-of-linear-regression.html#cb97-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">alpha =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">size =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb97-21"><a href="fundamentals-of-linear-regression.html#cb97-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">9</span>)) <span class="sc">+</span></span>
<span id="cb97-22"><a href="fundamentals-of-linear-regression.html#cb97-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb97-23"><a href="fundamentals-of-linear-regression.html#cb97-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> simulation, <span class="at">ncol =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-47-1.png" width="960" /></p>
<p>The question is, do these simulated data sets look like the original data? Let’s see.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="fundamentals-of-linear-regression.html#cb98-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb98-2"><a href="fundamentals-of-linear-regression.html#cb98-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> negemot, <span class="at">y =</span> govact)) <span class="sc">+</span></span>
<span id="cb98-3"><a href="fundamentals-of-linear-regression.html#cb98-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">height =</span> .<span class="dv">05</span>, <span class="at">width =</span> .<span class="dv">05</span>, </span>
<span id="cb98-4"><a href="fundamentals-of-linear-regression.html#cb98-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">alpha =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="at">size =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb98-5"><a href="fundamentals-of-linear-regression.html#cb98-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">9</span>)) <span class="sc">+</span></span>
<span id="cb98-6"><a href="fundamentals-of-linear-regression.html#cb98-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-48-1.png" width="321.6" /></p>
<p>Overall, the simulations aren’t bad. But in all three <code>govact</code> tends to veer above 7.5, which is where the original data appear to be bounded. But otherwise the overall shape is pretty close, at least with respect to <code>negemot</code>.</p>
<p>There’s nothing special about three simulations. Three is just more than one and gives you a sense of the variance across simulations. Also, we only examined the model fit with respect to <code>negemot</code>. Since there are other variables in the model, we might also assess the model based on them.</p>
<p>Another method is with the <code>brms::pp_check()</code> function, which allows users to access a variety of convenience functions from the <a href="https://CRAN.R-project.org/package=bayesplot"><strong>bayesplot</strong> package</a> <span class="citation">(<a href="#ref-gabry2019visualization" role="doc-biblioref">Gabry et al., 2019</a>; <a href="#ref-R-bayesplot" role="doc-biblioref">Gabry &amp; Mahr, 2022</a>)</span>. Here we’ll use the default settings and just tack on <code>theme_bw()</code> for aesthetics.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="fundamentals-of-linear-regression.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb99-2"><a href="fundamentals-of-linear-regression.html#cb99-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pp_check</span>(model2<span class="fl">.8</span>) <span class="sc">+</span></span>
<span id="cb99-3"><a href="fundamentals-of-linear-regression.html#cb99-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-49-1.png" width="576" /></p>
<p>What we did was simulate 10 data sets worth of <code>govact</code> values, plot their densities (i.e., the thin blue lines) and compare them with the density of the original <code>govact</code> values. What we want is for the thin blue lines to largely align with the thick blue line. Though not perfect, the simulations from our <code>model2.8</code> did a pretty okay job of reproducing the original <code>govact</code> distribution. For more ideas on this method, see the <a href="https://CRAN.R-project.org/package=brms/brms.pdf"><strong>brms</strong> reference manual</a> <span class="citation">(<a href="#ref-brms2022RM" role="doc-biblioref">Bürkner, 2022e</a>)</span> and Gabry’s <span class="citation">(<a href="#ref-gabryGraphicalPosteriorPredictive2019" role="doc-biblioref">2019</a>)</span> vignette, <a href="https://CRAN.R-project.org/package=bayesplot/vignettes/graphical-ppcs.html"><em>Graphical posterior predictive checks using the bayesplot package</em></a>.</p>
</div>
<div id="statistical-inference" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Statistical inference<a href="fundamentals-of-linear-regression.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here’s a <strong>tidyverse</strong> way to do Hayes’ simulation from page 57. We’re just using OLS regression with the <code>lm()</code> function. You could do this with Bayesian HMC estimation, but man would it take a while. For our first step, we’ll define two custom functions.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="fundamentals-of-linear-regression.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># this first one will use the `slice_sample()` function to randomly sample from `glbwarm`</span></span>
<span id="cb100-2"><a href="fundamentals-of-linear-regression.html#cb100-2" aria-hidden="true" tabindex="-1"></a>make_sample <span class="ot">&lt;-</span> <span class="cf">function</span>(i) {</span>
<span id="cb100-3"><a href="fundamentals-of-linear-regression.html#cb100-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb100-4"><a href="fundamentals-of-linear-regression.html#cb100-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice_sample</span>(glbwarm, <span class="at">n =</span> <span class="dv">50</span>, <span class="at">replace =</span> F)</span>
<span id="cb100-5"><a href="fundamentals-of-linear-regression.html#cb100-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb100-6"><a href="fundamentals-of-linear-regression.html#cb100-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-7"><a href="fundamentals-of-linear-regression.html#cb100-7" aria-hidden="true" tabindex="-1"></a><span class="co"># this second function will fit our model, the same one from `model2.8`, to each of our subsamples</span></span>
<span id="cb100-8"><a href="fundamentals-of-linear-regression.html#cb100-8" aria-hidden="true" tabindex="-1"></a>glbwarm_model <span class="ot">&lt;-</span> <span class="cf">function</span>(df) {</span>
<span id="cb100-9"><a href="fundamentals-of-linear-regression.html#cb100-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> negemot <span class="sc">+</span> posemot <span class="sc">+</span> ideology <span class="sc">+</span> sex <span class="sc">+</span> age, <span class="at">data =</span> df)</span>
<span id="cb100-10"><a href="fundamentals-of-linear-regression.html#cb100-10" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we’ll run the simulation, wrangle the output, and plot in one fell swoop.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="fundamentals-of-linear-regression.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we need an iteration index, which will double as the values we set our seed with in our `make_sample()` function</span></span>
<span id="cb101-2"><a href="fundamentals-of-linear-regression.html#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">iter =</span> <span class="dv">1</span><span class="sc">:</span><span class="fl">1e4</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb101-3"><a href="fundamentals-of-linear-regression.html#cb101-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(iter) <span class="sc">%&gt;%</span> </span>
<span id="cb101-4"><a href="fundamentals-of-linear-regression.html#cb101-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># inserting our subsamples</span></span>
<span id="cb101-5"><a href="fundamentals-of-linear-regression.html#cb101-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">sample =</span> <span class="fu">map</span>(iter, make_sample)) <span class="sc">%&gt;%</span> </span>
<span id="cb101-6"><a href="fundamentals-of-linear-regression.html#cb101-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># fitting our models</span></span>
<span id="cb101-7"><a href="fundamentals-of-linear-regression.html#cb101-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">model =</span> <span class="fu">map</span>(sample, glbwarm_model)) <span class="sc">%&gt;%</span> </span>
<span id="cb101-8"><a href="fundamentals-of-linear-regression.html#cb101-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># taking those model results and tidying them with the broom package</span></span>
<span id="cb101-9"><a href="fundamentals-of-linear-regression.html#cb101-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">broom =</span> <span class="fu">map</span>(model, broom<span class="sc">::</span>tidy)) <span class="sc">%&gt;%</span> </span>
<span id="cb101-10"><a href="fundamentals-of-linear-regression.html#cb101-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># unnesting allows us to access our model results</span></span>
<span id="cb101-11"><a href="fundamentals-of-linear-regression.html#cb101-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>(broom) <span class="sc">%&gt;%</span> </span>
<span id="cb101-12"><a href="fundamentals-of-linear-regression.html#cb101-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># we&#39;re only going to focus on the estimates for `negemot`</span></span>
<span id="cb101-13"><a href="fundamentals-of-linear-regression.html#cb101-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(term <span class="sc">==</span> <span class="st">&quot;negemot&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb101-14"><a href="fundamentals-of-linear-regression.html#cb101-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb101-15"><a href="fundamentals-of-linear-regression.html#cb101-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># here it is, Figure 2.7</span></span>
<span id="cb101-16"><a href="fundamentals-of-linear-regression.html#cb101-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> estimate)) <span class="sc">+</span></span>
<span id="cb101-17"><a href="fundamentals-of-linear-regression.html#cb101-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> .<span class="dv">025</span>, <span class="at">boundary =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb101-18"><a href="fundamentals-of-linear-regression.html#cb101-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Unstandardized regression coefficient for negemot&quot;</span>,</span>
<span id="cb101-19"><a href="fundamentals-of-linear-regression.html#cb101-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Frequency in 1e4 samples of size 50&quot;</span>) <span class="sc">+</span></span>
<span id="cb101-20"><a href="fundamentals-of-linear-regression.html#cb101-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/sim-1.png" width="576" /></p>
<p>To learn more about this approach to simulations, see <a href="https://r4ds.had.co.nz/many-models.html#nested-data">Section 25.2.1</a> in <em>R4DS</em>.</p>
<div id="testing-a-null-hypothesis." class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Testing a null hypothesis.<a href="fundamentals-of-linear-regression.html#testing-a-null-hypothesis." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As Bayesians, we don’t need to wed ourselves to the null hypothesis. We’re not interested in the probability of the data given the null hypothesis. Bayes’ rule,</p>
<p><span class="math display">\[p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)},\]</span></p>
<p>gives us the probability of the model parameters, given the data. Though I acknowledge <a href="http://daniellakens.blogspot.com/2017/11/the-statisticians-fallacy.html">different researchers might set out to ask different things of their data</a>, I propose we’re generally more interested in determining the most probable parameter values than we are the probabilities tested within the NHST paradigm.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="fundamentals-of-linear-regression.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as_draws_df</span>(model2<span class="fl">.8</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb102-2"><a href="fundamentals-of-linear-regression.html#cb102-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb102-3"><a href="fundamentals-of-linear-regression.html#cb102-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> b_negemot)) <span class="sc">+</span></span>
<span id="cb102-4"><a href="fundamentals-of-linear-regression.html#cb102-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;black&quot;</span>, <span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>) <span class="sc">+</span></span>
<span id="cb102-5"><a href="fundamentals-of-linear-regression.html#cb102-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">posterior_interval</span>(model2<span class="fl">.8</span>)[<span class="st">&quot;b_negemot&quot;</span>, ],</span>
<span id="cb102-6"><a href="fundamentals-of-linear-regression.html#cb102-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;white&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb102-7"><a href="fundamentals-of-linear-regression.html#cb102-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">posterior_interval</span>(model2<span class="fl">.8</span>)[<span class="st">&quot;b_negemot&quot;</span>, ] <span class="sc">%&gt;%</span> <span class="fu">as.double</span>(),</span>
<span id="cb102-8"><a href="fundamentals-of-linear-regression.html#cb102-8" aria-hidden="true" tabindex="-1"></a>                     <span class="at">labels =</span> <span class="fu">posterior_interval</span>(model2<span class="fl">.8</span>)[<span class="st">&quot;b_negemot&quot;</span>, ] <span class="sc">%&gt;%</span> <span class="fu">as.double</span>() <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span> <span class="fu">as.character</span>()) <span class="sc">+</span></span>
<span id="cb102-9"><a href="fundamentals-of-linear-regression.html#cb102-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb102-10"><a href="fundamentals-of-linear-regression.html#cb102-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;The most probable values for our b_negemot parameter are the ones around the peak</span><span class="sc">\n</span><span class="st">of the density. For convenience, the dashed lines denote the 95% credible intervals.</span><span class="sc">\n</span><span class="st">Sure, you could ask yourself, &#39;Is zero within those intervals?&#39; But with such rich output,</span><span class="sc">\n</span><span class="st">that seems like an impoverished question to ask.&quot;</span>) <span class="sc">+</span></span>
<span id="cb102-11"><a href="fundamentals-of-linear-regression.html#cb102-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-51-1.png" width="576" /></p>
<p>For more discussion the NHST paradigm and how it compares with variations of Bayesian statistics, check out Kruschke’s <span class="citation">(<a href="#ref-kruschkeDoingBayesianData2015" role="doc-biblioref">2015</a>)</span> text, <a href="https://sites.google.com/site/doingbayesiandataanalysis/"><em>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</em></a>, particularly chapters 10 through 13. You can find my <span class="citation">(<a href="#ref-kurzDoingBayesianDataAnalysis2023" role="doc-biblioref">2023a</a>)</span> <strong>brms</strong> + <strong>tidyverse</strong> translation of that text at <a href="https://bookdown.org/content/3686/">https://bookdown.org/content/3686/</a>.</p>
</div>
<div id="interval-estimation." class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Interval estimation.<a href="fundamentals-of-linear-regression.html#interval-estimation." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Within the Bayesian paradigm, we don’t use 95% intervals based on the typical frequentist formula. With the <strong>brms</strong> package, we typically use percentile-based intervals. Take the 95% credible intervals for the <code>negemot</code> coefficient from model <code>model2.8</code>:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="fundamentals-of-linear-regression.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_interval</span>(model2<span class="fl">.8</span>)[<span class="st">&quot;b_negemot&quot;</span>, ]</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.3886046 0.4924091</code></pre>
<p>We can actually get those intervals with the simple use of the base <strong>R</strong> <code>quantile()</code> function.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="fundamentals-of-linear-regression.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as_draws_df</span>(model2<span class="fl">.8</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb105-2"><a href="fundamentals-of-linear-regression.html#cb105-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">the_2.5_percentile  =</span> <span class="fu">quantile</span>(b_negemot, <span class="at">probs =</span> .<span class="dv">025</span>),</span>
<span id="cb105-3"><a href="fundamentals-of-linear-regression.html#cb105-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">the_97.5_percentile =</span> <span class="fu">quantile</span>(b_negemot, <span class="at">probs =</span> .<span class="dv">975</span>))</span></code></pre></div>
<pre><code>## # A tibble: 1 × 2
##   the_2.5_percentile the_97.5_percentile
##                &lt;dbl&gt;               &lt;dbl&gt;
## 1              0.389               0.492</code></pre>
<p>The consequence of this is that our Bayesian credible intervals aren’t necessarily symmetric, which is fine because the posterior distribution for a given parameter isn’t always symmetric. But not all Bayesian intervals are percentile based. John Kruschke, for example, often recommends highest posterior density intervals in <a href="https://sites.google.com/site/doingbayesiandataanalysis/">his work</a>. The <strong>brms</strong> package doesn’t have a convenience function for these, but you can compute them with help from the <a href="https://CRAN.R-project.org/package=HDInterval"><strong>HDInterval</strong> package</a> <span class="citation">(<a href="#ref-R-HDInterval" role="doc-biblioref">Meredith &amp; Kruschke, 2018</a>)</span>.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="fundamentals-of-linear-regression.html#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(HDInterval)</span>
<span id="cb107-2"><a href="fundamentals-of-linear-regression.html#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="fundamentals-of-linear-regression.html#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hdi</span>(<span class="fu">as_draws_df</span>(model2<span class="fl">.8</span>)[ , <span class="st">&quot;b_negemot&quot;</span>], <span class="at">credMass =</span> .<span class="dv">95</span>)</span></code></pre></div>
<pre><code>##       b_negemot
## lower 0.3884334
## upper 0.4921508
## attr(,&quot;credMass&quot;)
## [1] 0.95</code></pre>
<p>Finally, because Bayesians aren’t bound to the NHST paradigm, we aren’t bound to 95% intervals, either. For example, in both his excellent <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2020" role="doc-biblioref">2020</a>)</span> <a href="http://xcelab.net/rm/statistical-rethinking/">text</a> and as a default in its accompanying <a href="https://github.com/rmcelreath/rethinking"><strong>rethinking</strong> package</a>, <a href="https://twitter.com/rlmcelreath">Richard McElreath</a> often uses 89% intervals. Alternatively, <a href="https://twitter.com/StatModeling">Andrew Gelman</a> has publicly advocated for <a href="http://andrewgelman.com/2016/11/05/why-i-prefer-50-to-95-intervals/">50% intervals</a>. The most important thing is to express the uncertainty in the posterior in a clearly-specified way. If you’d like, say, 80% intervals in your model summary, you can insert a <code>prob</code> argument into either <code>print()</code> or <code>summary()</code>.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="fundamentals-of-linear-regression.html#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(model2<span class="fl">.8</span>, <span class="at">prob =</span> .<span class="dv">8</span>)</span></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ 1 + negemot + posemot + ideology + sex + age 
##    Data: glbwarm (Number of observations: 815) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.06      0.21     3.79     4.32 1.00     5255     3316
## negemot       0.44      0.03     0.41     0.48 1.00     5468     3047
## posemot      -0.03      0.03    -0.06     0.01 1.00     5267     3212
## ideology     -0.22      0.03    -0.25    -0.18 1.00     5270     3107
## sex          -0.01      0.08    -0.11     0.09 1.00     6013     3219
## age          -0.00      0.00    -0.00     0.00 1.00     5776     3240
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.07      0.03     1.03     1.10 1.00     6157     3074
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Note how the names of two of our columns changed to ‘l-80% CI’ and ‘u-80% CI’.</p>
<p>You can specify custom percentile levels with <code>posterior_summary()</code>:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="fundamentals-of-linear-regression.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span>(model2<span class="fl">.8</span>, <span class="at">probs =</span> <span class="fu">c</span>(.<span class="dv">9</span>, .<span class="dv">8</span>, .<span class="dv">7</span>, .<span class="dv">6</span>, .<span class="dv">4</span>, .<span class="dv">3</span>, .<span class="dv">2</span>, .<span class="dv">1</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb111-2"><a href="fundamentals-of-linear-regression.html#cb111-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##             Estimate Est.Error      Q90      Q80      Q70      Q60      Q40      Q30      Q20      Q10
## b_Intercept     4.06      0.21     4.32     4.23     4.17     4.11     4.01     3.95     3.89     3.79
## b_negemot       0.44      0.03     0.48     0.46     0.46     0.45     0.43     0.43     0.42     0.41
## b_posemot      -0.03      0.03     0.01     0.00    -0.01    -0.02    -0.03    -0.04    -0.05    -0.06
## b_ideology     -0.22      0.03    -0.18    -0.19    -0.20    -0.21    -0.22    -0.23    -0.24    -0.25
## b_sex          -0.01      0.08     0.09     0.06     0.03     0.01    -0.03    -0.05    -0.08    -0.11
## b_age           0.00      0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00
## sigma           1.07      0.03     1.10     1.09     1.08     1.07     1.06     1.05     1.05     1.03
## lprior         -3.26      0.01    -3.26    -3.26    -3.26    -3.26    -3.27    -3.27    -3.27    -3.27
## lp__        -1213.22      1.87 -1211.12 -1211.63 -1212.05 -1212.48 -1213.39 -1213.94 -1214.59 -1215.65</code></pre>
<p>And of course, you can use multiple interval summaries when you <code>summarize()</code> the output from <code>as_draws_df()</code>. E.g.,</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="fundamentals-of-linear-regression.html#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as_draws_df</span>(model2<span class="fl">.8</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb113-2"><a href="fundamentals-of-linear-regression.html#cb113-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(b_Intercept<span class="sc">:</span>b_age) <span class="sc">%&gt;%</span> </span>
<span id="cb113-3"><a href="fundamentals-of-linear-regression.html#cb113-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(name) <span class="sc">%&gt;%</span> </span>
<span id="cb113-4"><a href="fundamentals-of-linear-regression.html#cb113-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="st">`</span><span class="at">l-95% CI</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">quantile</span>(value, <span class="at">probs =</span> .<span class="dv">025</span>),</span>
<span id="cb113-5"><a href="fundamentals-of-linear-regression.html#cb113-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">`</span><span class="at">u-95% CI</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">quantile</span>(value, <span class="at">probs =</span> .<span class="dv">975</span>),</span>
<span id="cb113-6"><a href="fundamentals-of-linear-regression.html#cb113-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">`</span><span class="at">l-50% CI</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">quantile</span>(value, <span class="at">probs =</span> .<span class="dv">25</span>),</span>
<span id="cb113-7"><a href="fundamentals-of-linear-regression.html#cb113-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">`</span><span class="at">u-50% CI</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">quantile</span>(value, <span class="at">probs =</span> .<span class="dv">75</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb113-8"><a href="fundamentals-of-linear-regression.html#cb113-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.double, round, <span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 5
##   name        `l-95% CI` `u-95% CI` `l-50% CI` `u-50% CI`
##   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1 b_age            -0.01       0          0          0   
## 2 b_ideology       -0.27      -0.17      -0.24      -0.2 
## 3 b_Intercept       3.66       4.46       3.93       4.2 
## 4 b_negemot         0.39       0.49       0.42       0.46
## 5 b_posemot        -0.08       0.03      -0.05      -0.01
## 6 b_sex            -0.16       0.14      -0.06       0.04</code></pre>
<p>Throughout this project, I’m going to be lazy and default to conventional 95% intervals, with occasional appearances of 50% intervals.</p>
</div>
<div id="testing-a-hypothesis-about-a-set-of-antecedent-variables." class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> Testing a hypothesis about a set of antecedent variables.<a href="fundamentals-of-linear-regression.html#testing-a-hypothesis-about-a-set-of-antecedent-variables." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here we’ll make use of the <code>update()</code> function to hastily fit our reduced model, <code>model2.11</code>.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="fundamentals-of-linear-regression.html#cb115-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.11</span> <span class="ot">&lt;-</span> <span class="fu">update</span>(</span>
<span id="cb115-2"><a href="fundamentals-of-linear-regression.html#cb115-2" aria-hidden="true" tabindex="-1"></a>  model2<span class="fl">.8</span>,</span>
<span id="cb115-3"><a href="fundamentals-of-linear-regression.html#cb115-3" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> ideology <span class="sc">+</span> sex <span class="sc">+</span> age,</span>
<span id="cb115-4"><a href="fundamentals-of-linear-regression.html#cb115-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb115-5"><a href="fundamentals-of-linear-regression.html#cb115-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.11&quot;</span>)</span></code></pre></div>
<p>We can get a look at the <span class="math inline">\(R^2\)</span> summaries for our competing models like this.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="fundamentals-of-linear-regression.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bayes_R2</span>(model2<span class="fl">.8</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##    Estimate Est.Error Q2.5 Q97.5
## R2     0.39      0.02 0.35  0.43</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="fundamentals-of-linear-regression.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bayes_R2</span>(model2<span class="fl">.11</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##    Estimate Est.Error Q2.5 Q97.5
## R2     0.18      0.02 0.13  0.22</code></pre>
<p>So far it looks like our fuller model, <code>model2.8</code>, accounts for more variation in the data. If we wanted to look at their distributions, we’d set <code>summary = F</code> in the <code>bayes_R2()</code> function and convert the results to a data frame. Here we use <code>bind_cols()</code> to put the <span class="math inline">\(R^2\)</span> results for both in the same tibble.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="fundamentals-of-linear-regression.html#cb120-1" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">bayes_R2</span>(model2<span class="fl">.8</span>, <span class="at">summary =</span> F),</span>
<span id="cb120-2"><a href="fundamentals-of-linear-regression.html#cb120-2" aria-hidden="true" tabindex="-1"></a>            <span class="fu">bayes_R2</span>(model2<span class="fl">.11</span>, <span class="at">summary =</span> F)) <span class="sc">%&gt;%</span> </span>
<span id="cb120-3"><a href="fundamentals-of-linear-regression.html#cb120-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb120-4"><a href="fundamentals-of-linear-regression.html#cb120-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="fu">str_c</span>(<span class="st">&quot;model2.&quot;</span>, <span class="fu">c</span>(<span class="dv">8</span>, <span class="dv">11</span>)))</span>
<span id="cb120-5"><a href="fundamentals-of-linear-regression.html#cb120-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-6"><a href="fundamentals-of-linear-regression.html#cb120-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(r2)</span></code></pre></div>
<pre><code>##    model2.8 model2.11
## 1 0.3999961 0.1847054
## 2 0.3855340 0.1656601
## 3 0.4023894 0.1751479
## 4 0.3881968 0.1287015
## 5 0.4074221 0.1764577
## 6 0.4023420 0.1649124</code></pre>
<p>With our <code>r2</code> tibble in hand, we’re ready to plot.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="fundamentals-of-linear-regression.html#cb122-1" aria-hidden="true" tabindex="-1"></a>r2 <span class="sc">%&gt;%</span> </span>
<span id="cb122-2"><a href="fundamentals-of-linear-regression.html#cb122-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb122-3"><a href="fundamentals-of-linear-regression.html#cb122-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb122-4"><a href="fundamentals-of-linear-regression.html#cb122-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">fill =</span> name)) <span class="sc">+</span></span>
<span id="cb122-5"><a href="fundamentals-of-linear-regression.html#cb122-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>) <span class="sc">+</span></span>
<span id="cb122-6"><a href="fundamentals-of-linear-regression.html#cb122-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb122-7"><a href="fundamentals-of-linear-regression.html#cb122-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">paste</span>(<span class="fu">italic</span>(R)<span class="sc">^</span><span class="dv">2</span>))) <span class="sc">+</span></span>
<span id="cb122-8"><a href="fundamentals-of-linear-regression.html#cb122-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb122-9"><a href="fundamentals-of-linear-regression.html#cb122-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-60-1.png" width="576" /></p>
<p>Yep, the <span class="math inline">\(R^2\)</span> distribution for <code>model2.8</code>, the one including the emotion variables, is clearly larger than that for the more parsimonious <code>model2.11</code>. And it’d just take a little more data wrangling to get a formal <span class="math inline">\(R^2\)</span> difference score.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="fundamentals-of-linear-regression.html#cb123-1" aria-hidden="true" tabindex="-1"></a>r2 <span class="sc">%&gt;%</span> </span>
<span id="cb123-2"><a href="fundamentals-of-linear-regression.html#cb123-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">difference =</span> model2<span class="fl">.11</span> <span class="sc">-</span> model2<span class="fl">.8</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb123-3"><a href="fundamentals-of-linear-regression.html#cb123-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb123-4"><a href="fundamentals-of-linear-regression.html#cb123-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> difference)) <span class="sc">+</span></span>
<span id="cb123-5"><a href="fundamentals-of-linear-regression.html#cb123-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;black&quot;</span>, <span class="at">color =</span> <span class="st">&quot;transparent&quot;</span>) <span class="sc">+</span></span>
<span id="cb123-6"><a href="fundamentals-of-linear-regression.html#cb123-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="cn">NULL</span>, <span class="at">breaks =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb123-7"><a href="fundamentals-of-linear-regression.html#cb123-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="sc">-</span><span class="dv">1</span><span class="sc">:</span><span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb123-8"><a href="fundamentals-of-linear-regression.html#cb123-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(Delta, <span class="fu">italic</span>(R)<span class="sc">^</span>{<span class="dv">2</span>})),</span>
<span id="cb123-9"><a href="fundamentals-of-linear-regression.html#cb123-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;This is the amount the &quot;</span>, <span class="fu">italic</span>(R)<span class="sc">^</span>{<span class="dv">2</span>}, <span class="st">&quot; dropped after pruning the emotion variables from the model.&quot;</span>)),</span>
<span id="cb123-10"><a href="fundamentals-of-linear-regression.html#cb123-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb123-11"><a href="fundamentals-of-linear-regression.html#cb123-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-61-1.png" width="576" /></p>
<p>The <span class="math inline">\(R^2\)</span> approach is popular within the social sciences. But it has its limitations, the first of which is that it doesn’t correct for model complexity. The second is it’s not applicable to a range of models, such as those that do not use the Gaussian likelihood (e.g., logistic regression) or to multilevel models.</p>
<p>Happily, information criteria offer a more general framework. The AIC is the most popular information criteria among frequentists. Within the Bayesian world, we have the DIC, the WAIC, and the LOO. The DIC is quickly falling out of favor and is not immediately available with the <strong>brms</strong> package. However, we can use the WAIC and the LOO, both of which are computed in <strong>brms</strong> via the <a href="https://github.com/stan-dev/loo"><strong>loo</strong> package</a> <span class="citation">(<a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">Vehtari et al., 2017</a>; <a href="#ref-R-loo" role="doc-biblioref">Vehtari et al., 2022</a>; <a href="#ref-yaoUsingStackingAverage2018" role="doc-biblioref">Yao et al., 2018</a>)</span>.</p>
<p>With <strong>brms</strong>, you can compute the WAIC or LOO values and add them to your model fit object with the <code>add_criterion()</code> function.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="fundamentals-of-linear-regression.html#cb124-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.8</span>  <span class="ot">&lt;-</span> <span class="fu">add_criterion</span>(model2<span class="fl">.8</span>,  <span class="at">criterion =</span> <span class="fu">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span>
<span id="cb124-2"><a href="fundamentals-of-linear-regression.html#cb124-2" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.11</span> <span class="ot">&lt;-</span> <span class="fu">add_criterion</span>(model2<span class="fl">.11</span>, <span class="at">criterion =</span> <span class="fu">c</span>(<span class="st">&quot;loo&quot;</span>, <span class="st">&quot;waic&quot;</span>))</span></code></pre></div>
<p>Here’s the main loo-summary output for <code>model4</code>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="fundamentals-of-linear-regression.html#cb125-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.8</span><span class="sc">$</span>criteria<span class="sc">$</span>loo</span></code></pre></div>
<pre><code>## 
## Computed from 4000 by 815 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1213.9 22.7
## p_loo         7.7  0.7
## looic      2427.9 45.5
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &lt; 0.5).
## See help(&#39;pareto-k-diagnostic&#39;) for details.</code></pre>
<p>You get a wealth of output, more of which can be seen with <code>str(model2.8$loo)</code>. For now, notice the “All Pareto k estimates are good (k &lt; 0.5).” For technical details on Pareto <span class="math inline">\(k\)</span> values, see <span class="citation">Vehtari &amp; Gabry (<a href="#ref-vehtariUsingLooPackage2020" role="doc-biblioref">2020</a>)</span>. In short, each case in the data gets its own <span class="math inline">\(k\)</span> value and we like it when those <span class="math inline">\(k\)</span>’s are low. The makers of the <strong>loo</strong> package get worried when those <span class="math inline">\(k\)</span>’s exceed 0.7 and as a result you will get a warning message when they do. Happily, we have no such warning messages in this example.</p>
<p>If you want to work with the <span class="math inline">\(k\)</span> values directly, you can extract them and place them into a data frame like so.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="fundamentals-of-linear-regression.html#cb127-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.8</span><span class="sc">$</span>criteria<span class="sc">$</span>loo<span class="sc">$</span>diagnostics <span class="sc">%&gt;%</span> </span>
<span id="cb127-2"><a href="fundamentals-of-linear-regression.html#cb127-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb127-3"><a href="fundamentals-of-linear-regression.html#cb127-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>()</span></code></pre></div>
<pre><code>##      pareto_k    n_eff
## 1 -0.02469817 5622.864
## 2 -0.03357609 4935.060
## 3  0.14501532 5874.881
## 4  0.16879274 2421.769
## 5 -0.19777749 6086.299
## 6 -0.02300993 5010.521</code></pre>
<p>The <code>pareto_k</code> values can be used to examine cases that are overly-influential on the model parameters, something akin to a Cook’s <span class="math inline">\(D_i\)</span>. See, for example <a href="https://stackoverflow.com/questions/39578834/linear-model-diagnostics-for-bayesian-models-using-rstan/39595436">this discussion on stackoverflow.com</a> in which several members of the <a href="http://mc-stan.org">Stan team</a> weighed in. The issue is also discussed in <span class="citation">Vehtari et al. (<a href="#ref-vehtariPracticalBayesianModel2017" role="doc-biblioref">2017</a>)</span> and in <a href="https://www.youtube.com/watch?v=FUROJM3u5HQ&amp;feature=youtu.be&amp;a=">this lecture by Aki Vehtari</a>.</p>
<p>But anyway, we’re getting ahead of ourselves. Back to the LOO.</p>
<p>Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the values of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower information criteria. With <code>compare_ic()</code>, we can compute a formal difference score between multiple loo objects.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="fundamentals-of-linear-regression.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="fu">loo_compare</span>(model2<span class="fl">.8</span>, model2<span class="fl">.11</span>) <span class="sc">%&gt;%</span> <span class="fu">print</span>(<span class="at">simplify =</span> F)</span></code></pre></div>
<pre><code>##           elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic   se_looic
## model2.8      0.0       0.0 -1213.9     22.7         7.7     0.7   2427.9    45.5 
## model2.11  -118.7      15.9 -1332.7     20.6         5.2     0.4   2665.3    41.1</code></pre>
<p>We also get a standard error. Here it looks like <code>model2.8</code> was substantially better, in terms of LOO-values, than <code>model2.11</code>.</p>
<p>For more on the LOO, see the <a href="https://cran.r-project.org/web/packages/loo/loo.pdf"><strong>loo</strong> reference manual</a> <span class="citation">(<a href="#ref-loo2022RM" role="doc-biblioref">Gabry, 2022</a>)</span>, Vehtari and Gabry’s handy <span class="citation">(<a href="#ref-vehtariUsingLooPackage2020" role="doc-biblioref">2020</a>)</span> <a href="https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html">vignette</a>, or the scholarly papers referenced therein. Also, McElreath discussed the LOO in the second <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2020" role="doc-biblioref">2020</a>)</span> version of his text, as well as in this <a href="https://www.youtube.com/watch?v=gjrsYDJbRh0&amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&amp;index=8">lecture</a>.</p>
</div>
</div>
<div id="multicategorical-antecedent-variables" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Multicategorical antecedent variables<a href="fundamentals-of-linear-regression.html#multicategorical-antecedent-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>“To include a multicategorical antecedent variable representing <span class="math inline">\(g\)</span> groups in a regression model, it must be represented with <span class="math inline">\(g − 1\)</span> variables using one of a variety of different group coding systems”. (p. 66) This isn’t strictly true, but we digress. Hayes went on…</p>
<blockquote>
<p>One popular system for coding groups is <em>indicator coding</em>, also known as <em>dummy coding</em>. With indicator coding, <span class="math inline">\(g − 1\)</span> <em>indicator variables</em> containing either a zero or one represent which of the <span class="math inline">\(g\)</span> groups a case belongs in, and these indicator variables are used as antecedents in a regression model. To construct indicator codes, create <span class="math inline">\(g − 1\)</span> variables <span class="math inline">\(D_i\)</span> for each case set to 1 if the case is in group <span class="math inline">\(i\)</span>, otherwise set <span class="math inline">\(D_i\)</span> to zero. (p. 66, <em>emphasis</em> in the original)</p>
</blockquote>
<p>Before we get to that, we should examine our multicategorical antecedent variable, <code>partyid</code>. It’s coded 1 = Democrat 2 = Independent 3 = Republican. You can get a count of the cases within a give <code>partyid</code> like this.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="fundamentals-of-linear-regression.html#cb131-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb131-2"><a href="fundamentals-of-linear-regression.html#cb131-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(partyid)</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##   partyid     n
##     &lt;dbl&gt; &lt;int&gt;
## 1       1   359
## 2       2   192
## 3       3   264</code></pre>
<p>We can get grouped means for <code>govact</code> like this.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="fundamentals-of-linear-regression.html#cb133-1" aria-hidden="true" tabindex="-1"></a>glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb133-2"><a href="fundamentals-of-linear-regression.html#cb133-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(partyid) <span class="sc">%&gt;%</span> </span>
<span id="cb133-3"><a href="fundamentals-of-linear-regression.html#cb133-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">mean_support_for_governmental_action =</span> <span class="fu">mean</span>(govact))</span></code></pre></div>
<pre><code>## # A tibble: 3 × 2
##   partyid mean_support_for_governmental_action
##     &lt;dbl&gt;                                &lt;dbl&gt;
## 1       1                                 5.06
## 2       2                                 4.61
## 3       3                                 3.92</code></pre>
<p>We can make dummies with the <code>if_else()</code> function. Here we’ll just go ahead and do that right within the <code>brm()</code> function.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="fundamentals-of-linear-regression.html#cb135-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.12</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb135-2"><a href="fundamentals-of-linear-regression.html#cb135-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm <span class="sc">%&gt;%</span> </span>
<span id="cb135-3"><a href="fundamentals-of-linear-regression.html#cb135-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">Democrat =</span> <span class="fu">if_else</span>(partyid <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb135-4"><a href="fundamentals-of-linear-regression.html#cb135-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">Republican =</span> <span class="fu">if_else</span>(partyid <span class="sc">==</span> <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">0</span>)), </span>
<span id="cb135-5"><a href="fundamentals-of-linear-regression.html#cb135-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb135-6"><a href="fundamentals-of-linear-regression.html#cb135-6" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> Democrat <span class="sc">+</span> Republican,</span>
<span id="cb135-7"><a href="fundamentals-of-linear-regression.html#cb135-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb135-8"><a href="fundamentals-of-linear-regression.html#cb135-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.12&quot;</span>)</span></code></pre></div>
<p>Check the results.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="fundamentals-of-linear-regression.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span>(model2<span class="fl">.12</span>)</span></code></pre></div>
<pre><code>##              Estimate Est.Error       Q2.5      Q97.5
## Intercept   4.6039657 0.0926150  4.4240278  4.7829894
## Democrat    0.4586668 0.1141390  0.2280444  0.6818560
## Republican -0.6775160 0.1221056 -0.9201040 -0.4375259</code></pre>
<p>The intercept is the stand-in for Independents and the other two coefficients are difference scores.</p>
<p>The <span class="math inline">\(R^2\)</span> is okay.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="fundamentals-of-linear-regression.html#cb138-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bayes_R2</span>(model2<span class="fl">.12</span>) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##    Estimate Est.Error  Q2.5 Q97.5
## R2    0.132      0.02 0.093 0.173</code></pre>
<p>There’s no need to compute an <span class="math inline">\(F\)</span>-test on our <span class="math inline">\(R^2\)</span>. The posterior mean and its 95% intervals are well away from zero. But you could use your <code>bayes_R2(model2.12, summary = F)</code> plotting skills from above to more fully inspect the posterior, if you’d like.</p>
<p>We could also use information criteria. One method would be to compare the WAIC or LOO value of <code>model2.12</code> with an intercept-only model. First, we’ll need to fit that model.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="fundamentals-of-linear-regression.html#cb140-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.13</span> <span class="ot">&lt;-</span> <span class="fu">brm</span>(</span>
<span id="cb140-2"><a href="fundamentals-of-linear-regression.html#cb140-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> glbwarm, </span>
<span id="cb140-3"><a href="fundamentals-of-linear-regression.html#cb140-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> gaussian,</span>
<span id="cb140-4"><a href="fundamentals-of-linear-regression.html#cb140-4" aria-hidden="true" tabindex="-1"></a>  govact <span class="sc">~</span> <span class="dv">1</span>,</span>
<span id="cb140-5"><a href="fundamentals-of-linear-regression.html#cb140-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">cores =</span> <span class="dv">4</span>,</span>
<span id="cb140-6"><a href="fundamentals-of-linear-regression.html#cb140-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">file =</span> <span class="st">&quot;fits/model02.13&quot;</span>)</span></code></pre></div>
<p>Here we’ll compute the WAIC for each, save the results to their fit objects, and them compare them with <code>loo_compare()</code>.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="fundamentals-of-linear-regression.html#cb141-1" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.12</span> <span class="ot">&lt;-</span> <span class="fu">add_criterion</span>(model2<span class="fl">.12</span>, <span class="at">criterion =</span> <span class="st">&quot;waic&quot;</span>)</span>
<span id="cb141-2"><a href="fundamentals-of-linear-regression.html#cb141-2" aria-hidden="true" tabindex="-1"></a>model2<span class="fl">.13</span> <span class="ot">&lt;-</span> <span class="fu">add_criterion</span>(model2<span class="fl">.13</span>, <span class="at">criterion =</span> <span class="st">&quot;waic&quot;</span>)</span>
<span id="cb141-3"><a href="fundamentals-of-linear-regression.html#cb141-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-4"><a href="fundamentals-of-linear-regression.html#cb141-4" aria-hidden="true" tabindex="-1"></a><span class="fu">loo_compare</span>(model2<span class="fl">.12</span>, model2<span class="fl">.13</span>, <span class="at">criterion =</span> <span class="st">&quot;waic&quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb141-5"><a href="fundamentals-of-linear-regression.html#cb141-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="at">simplify =</span> F)</span></code></pre></div>
<pre><code>##           elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic    se_waic
## model2.12     0.0       0.0 -1353.6      20.1          4.0     0.3    2707.1    40.1
## model2.13   -55.3      10.4 -1408.8      21.4          2.1     0.2    2817.6    42.8</code></pre>
<p>The WAIC comparison suggests <code>model2.12</code>, the one <em>with</em> the <code>partyid</code> dummies, is an improvement over the simple intercept-only model. Another way to compare the information criteria is with AIC-type weighting. The <strong>brms</strong> package offers a variety of weighting methods via the <code>model_weights()</code> function.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="fundamentals-of-linear-regression.html#cb143-1" aria-hidden="true" tabindex="-1"></a>mw <span class="ot">&lt;-</span> <span class="fu">model_weights</span>(model2<span class="fl">.12</span>, model2<span class="fl">.13</span>, <span class="at">weights =</span> <span class="st">&quot;waic&quot;</span>)</span>
<span id="cb143-2"><a href="fundamentals-of-linear-regression.html#cb143-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-3"><a href="fundamentals-of-linear-regression.html#cb143-3" aria-hidden="true" tabindex="-1"></a>mw</span></code></pre></div>
<pre><code>##    model2.12    model2.13 
## 1.000000e+00 1.011758e-24</code></pre>
<p>If you’re not a fan of scientific notation, you can put the results in a tibble and look at them on a plot.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="fundamentals-of-linear-regression.html#cb145-1" aria-hidden="true" tabindex="-1"></a>mw <span class="sc">%&gt;%</span> </span>
<span id="cb145-2"><a href="fundamentals-of-linear-regression.html#cb145-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb145-3"><a href="fundamentals-of-linear-regression.html#cb145-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>() <span class="sc">%&gt;%</span></span>
<span id="cb145-4"><a href="fundamentals-of-linear-regression.html#cb145-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="fu">c</span>(<span class="st">&quot;model&quot;</span>, <span class="st">&quot;WAIC weight&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb145-5"><a href="fundamentals-of-linear-regression.html#cb145-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-6"><a href="fundamentals-of-linear-regression.html#cb145-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="st">`</span><span class="at">WAIC weight</span><span class="st">`</span>, <span class="at">y =</span> model)) <span class="sc">+</span></span>
<span id="cb145-7"><a href="fundamentals-of-linear-regression.html#cb145-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb145-8"><a href="fundamentals-of-linear-regression.html#cb145-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;The weights should sum to 1. In this case virtually all the weight is placed</span><span class="sc">\n</span><span class="st">in model2.12. Recall, that these are RELATIVE weights. Add another model</span><span class="sc">\n</span><span class="st">fit into the mix and the weights might well change.&quot;</span>, </span>
<span id="cb145-9"><a href="fundamentals-of-linear-regression.html#cb145-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="cn">NULL</span>) <span class="sc">+</span></span>
<span id="cb145-10"><a href="fundamentals-of-linear-regression.html#cb145-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb145-11"><a href="fundamentals-of-linear-regression.html#cb145-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.ticks.y =</span> <span class="fu">element_blank</span>())</span></code></pre></div>
<p><img src="02_files/figure-html/unnamed-chunk-70-1.png" width="576" /></p>
<p>You could, of course, do all this with the LOO.</p>
</div>
<div id="assumptions-for-interpretation-and-statistical-inference" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Assumptions for interpretation and statistical inference<a href="fundamentals-of-linear-regression.html#assumptions-for-interpretation-and-statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<p>Regression is a handy tool to have in your statistical toolbox. Its utility as a “general data analytic system” <span class="citation">(<a href="#ref-cohenMultipleRegressionGeneral1968" role="doc-biblioref">Cohen, 1968</a>)</span> will be apparent throughout this book. But it is a human invention that isn’t perfect, it can lead you astray if used indiscriminately, and it is founded on some assumptions that aren’t always realistic or likely to be met in the circumstances in which the method is applied. (p. 68)</p>
</blockquote>
<div id="linearity." class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Linearity.<a href="fundamentals-of-linear-regression.html#linearity." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>“When using OLS regression to model some consequent variable of interest <span class="math inline">\(Y\)</span>, you must be willing to assume that the relationship between the variables in the model are linear in nature, or at least approximately linear” (p. 69)</p>
</div>
<div id="normality." class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> Normality.<a href="fundamentals-of-linear-regression.html#normality." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>brms</strong> package is quite general and allows users to fit models from a variety of likelihoods other than the Gaussian. For example, users can accommodate outliers/extreme values with <a href="https://solomonkurz.netlify.app/blog/2019-02-02-robust-linear-regression-with-student-s-t-distribution/">Student’s t regression</a>. You can do count regression with the Poisson or the negative binomial… For more, see McElreath’s lecture <a href="https://youtu.be/DyrUkqK9Tj4?t=254">introducing the generalized linear model</a> or Bürkner’s <span class="citation">(<a href="#ref-Bürkner2022Parameterization" role="doc-biblioref">2022d</a>)</span> vignette, <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_families.html"><em>Parameterization of response distributions in brms</em></a></p>
</div>
<div id="homoscedasticity." class="section level3 hasAnchor" number="2.8.3">
<h3><span class="header-section-number">2.8.3</span> Homoscedasticity.<a href="fundamentals-of-linear-regression.html#homoscedasticity." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>brms</strong> package can also accommodate homoscedasticity with distributional modeling. In short, one simply models <span class="math inline">\(\sigma\)</span> in addition to the mean, <span class="math inline">\(\mu\)</span>. See Bürkner’s handy <span class="citation">(<a href="#ref-Bürkner2022Distributional" role="doc-biblioref">2022a</a>)</span> vignette, <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html"><em>Estimating distributional models with brms</em></a> on the topic.</p>
</div>
<div id="independence." class="section level3 hasAnchor" number="2.8.4">
<h3><span class="header-section-number">2.8.4</span> Independence.<a href="fundamentals-of-linear-regression.html#independence." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The issue of independence is where the multilevel model comes in. See any relevant text, such as <a href="http://xcelab.net/rm/statistical-rethinking/"><em>Statistical rethinking</em></a> or <a href="https://stat.columbia.edu/~gelman/arm/"><em>Data analysis using regression and multilevel/hierarchical models</em></a> <span class="citation">(<a href="#ref-gelmanDataAnalysisUsing2006" role="doc-biblioref">Gelman &amp; Hill, 2006</a>)</span>. And yes, <strong>brms</strong> is fully capable of handling multilevel models <span class="citation">(see <a href="#ref-burknerBrmsPackageBayesian2017" role="doc-biblioref">Bürkner, 2017</a>, <a href="#ref-burknerAdvancedBayesianMultilevel2018" role="doc-biblioref">2018</a>)</span>.</p>
</div>
</div>
<div id="session-info-1" class="section level2 unnumbered hasAnchor">
<h2>Session info<a href="fundamentals-of-linear-regression.html#session-info-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="fundamentals-of-linear-regression.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code></pre></div>
<pre><code>## R version 4.2.2 (2022-10-31)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Big Sur ... 10.16
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] HDInterval_0.2.4 brms_2.18.0      Rcpp_1.0.9       forcats_0.5.1    stringr_1.4.1    dplyr_1.0.10    
##  [7] purrr_1.0.1      readr_2.1.2      tidyr_1.2.1      tibble_3.1.8     ggplot2_3.4.0    tidyverse_1.3.2 
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.1         backports_1.4.1      plyr_1.8.7           igraph_1.3.4         splines_4.2.2       
##   [6] crosstalk_1.2.0      TH.data_1.1-1        rstantools_2.2.0     inline_0.3.19        digest_0.6.31       
##  [11] htmltools_0.5.3      fansi_1.0.3          magrittr_2.0.3       checkmate_2.1.0      googlesheets4_1.0.1 
##  [16] tzdb_0.3.0           modelr_0.1.8         RcppParallel_5.1.5   matrixStats_0.63.0   vroom_1.5.7         
##  [21] xts_0.12.1           sandwich_3.0-2       prettyunits_1.1.1    colorspace_2.0-3     rvest_1.0.2         
##  [26] haven_2.5.1          xfun_0.35            callr_3.7.3          crayon_1.5.2         jsonlite_1.8.4      
##  [31] lme4_1.1-31          survival_3.4-0       zoo_1.8-10           glue_1.6.2           gtable_0.3.1        
##  [36] gargle_1.2.0         emmeans_1.8.0        distributional_0.3.1 pkgbuild_1.3.1       rstan_2.21.8        
##  [41] abind_1.4-5          scales_1.2.1         mvtnorm_1.1-3        DBI_1.1.3            miniUI_0.1.1.1      
##  [46] xtable_1.8-4         bit_4.0.4            stats4_4.2.2         StanHeaders_2.21.0-7 DT_0.24             
##  [51] htmlwidgets_1.5.4    httr_1.4.4           threejs_0.3.3        posterior_1.3.1      ellipsis_0.3.2      
##  [56] pkgconfig_2.0.3      loo_2.5.1            farver_2.1.1         sass_0.4.2           dbplyr_2.2.1        
##  [61] utf8_1.2.2           tidyselect_1.2.0     labeling_0.4.2       rlang_1.0.6          reshape2_1.4.4      
##  [66] later_1.3.0          munsell_0.5.0        cellranger_1.1.0     tools_4.2.2          cachem_1.0.6        
##  [71] cli_3.6.0            generics_0.1.3       broom_1.0.2          evaluate_0.18        fastmap_1.1.0       
##  [76] processx_3.8.0       knitr_1.40           bit64_4.0.5          fs_1.5.2             nlme_3.1-160        
##  [81] mime_0.12            projpred_2.2.1       xml2_1.3.3           compiler_4.2.2       bayesplot_1.10.0    
##  [86] shinythemes_1.2.0    rstudioapi_0.13      gamm4_0.2-6          reprex_2.0.2         bslib_0.4.0         
##  [91] stringi_1.7.8        highr_0.9            ps_1.7.2             Brobdingnag_1.2-8    lattice_0.20-45     
##  [96] Matrix_1.5-1         psych_2.2.9          nloptr_2.0.3         markdown_1.1         shinyjs_2.1.0       
## [101] tensorA_0.36.2       vctrs_0.5.1          pillar_1.8.1         lifecycle_1.0.3      jquerylib_0.1.4     
## [106] bridgesampling_1.1-2 estimability_1.4.1   httpuv_1.6.5         R6_2.5.1             bookdown_0.28       
## [111] promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18     boot_1.3-28          colourpicker_1.1.1  
## [116] MASS_7.3-58.1        gtools_3.9.4         assertthat_0.2.1     withr_2.5.0          mnormt_2.1.0        
## [121] shinystan_2.6.0      multcomp_1.4-20      mgcv_1.8-41          parallel_4.2.2       hms_1.1.1           
## [126] grid_4.2.2           minqa_1.2.5          coda_0.19-4          rmarkdown_2.16       googledrive_2.0.0   
## [131] shiny_1.7.2          lubridate_1.8.0      base64enc_0.1-3      dygraphs_1.1.1.6</code></pre>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-rstanarm2018" class="csl-entry">
Brilleman, S., Crowther, M., Moreno-Betancur, M., Buros Novik, J., &amp; Wolfe, R. (2018). <em>Joint longitudinal and time-to-event models via <span>Stan</span>.</em> <a href="https://github.com/stan-dev/stancon_talks/">https://github.com/stan-dev/stancon_talks/</a>
</div>
<div id="ref-Bürkner2022Distributional" class="csl-entry">
Bürkner, P.-C. (2022a). <em>Estimating distributional models with brms</em>. <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html">https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html</a>
</div>
<div id="ref-Bürkner2022Multivariate" class="csl-entry">
Bürkner, P.-C. (2022b). <em>Estimating multivariate models with brms</em>. <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html">https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html</a>
</div>
<div id="ref-Bürkner2022Parameterization" class="csl-entry">
Bürkner, P.-C. (2022d). <em>Parameterization of response distributions in brms</em>. <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_families.html">https://CRAN.R-project.org/package=brms/vignettes/brms_families.html</a>
</div>
<div id="ref-burknerBrmsPackageBayesian2017" class="csl-entry">
Bürkner, P.-C. (2017). <span class="nocase">brms</span>: <span>An R</span> package for <span>Bayesian</span> multilevel models using <span>Stan</span>. <em>Journal of Statistical Software</em>, <em>80</em>(1), 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>
</div>
<div id="ref-burknerAdvancedBayesianMultilevel2018" class="csl-entry">
Bürkner, P.-C. (2018). Advanced <span>Bayesian</span> multilevel modeling with the <span>R</span> package brms. <em>The R Journal</em>, <em>10</em>(1), 395–411. <a href="https://doi.org/10.32614/RJ-2018-017">https://doi.org/10.32614/RJ-2018-017</a>
</div>
<div id="ref-brms2022RM" class="csl-entry">
Bürkner, P.-C. (2022e). <em><span class="nocase">brms</span> reference manual, <span>Version</span> 2.18.0</em>. <a href="https://CRAN.R-project.org/package=brms/brms.pdf">https://CRAN.R-project.org/package=brms/brms.pdf</a>
</div>
<div id="ref-cohenMultipleRegressionGeneral1968" class="csl-entry">
Cohen, J. (1968). Multiple regression as a general data-analytic system. <em>Psychological Bulletin</em>, <em>70</em>, 426–443. <a href="https://doi.org/10.1037/h0026714">https://doi.org/10.1037/h0026714</a>
</div>
<div id="ref-loo2022RM" class="csl-entry">
Gabry, J. (2022). <em><span class="nocase">loo</span> reference manual, <span>Version</span> 2.5.1</em>. <a href="https://CRAN.R-project.org/package=loo/loo.pdf">https://CRAN.R-project.org/package=loo/loo.pdf</a>
</div>
<div id="ref-gabryGraphicalPosteriorPredictive2019" class="csl-entry">
Gabry, J. (2019, November 29). <em>Graphical posterior predictive checks using the bayesplot package</em>. <a href="https://CRAN.R-project.org/package=bayesplot/vignettes/graphical-ppcs.html">https://CRAN.R-project.org/package=bayesplot/vignettes/graphical-ppcs.html</a>
</div>
<div id="ref-R-rstanarm" class="csl-entry">
Gabry, J., &amp; Goodrich, B. (2022). <em><span class="nocase">rstanarm</span>: <span>Bayesian</span> applied regression modeling via stan</em> [Manual]. <a href="https://CRAN.R-project.org/package=rstanarm">https://CRAN.R-project.org/package=rstanarm</a>
</div>
<div id="ref-R-bayesplot" class="csl-entry">
Gabry, J., &amp; Mahr, T. (2022). <em><span class="nocase">bayesplot</span>: <span>Plotting</span> for <span>Bayesian</span> models</em>. <a href="https://CRAN.R-project.org/package=bayesplot">https://CRAN.R-project.org/package=bayesplot</a>
</div>
<div id="ref-gabry2019visualization" class="csl-entry">
Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in <span>Bayesian</span> workflow. <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, <em>182</em>(2), 389–402. <a href="https://doi.org/10.1111/rssa.12378">https://doi.org/10.1111/rssa.12378</a>
</div>
<div id="ref-gelmanRsquaredBayesianRegression2019" class="csl-entry">
Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for <span>Bayesian</span> regression models. <em>The American Statistician</em>, <em>73</em>(3), 307–309. <a href="https://doi.org/10.1080/00031305.2018.1549100">https://doi.org/10.1080/00031305.2018.1549100</a>
</div>
<div id="ref-gelmanDataAnalysisUsing2006" class="csl-entry">
Gelman, A., &amp; Hill, J. (2006). <em>Data analysis using regression and multilevel/hierarchical models</em>. <span>Cambridge University Press</span>. <a href="https://doi.org/10.1017/CBO9780511790942">https://doi.org/10.1017/CBO9780511790942</a>
</div>
<div id="ref-grolemundDataScience2017" class="csl-entry">
Grolemund, G., &amp; Wickham, H. (2017). <em>R for data science</em>. <span>O’Reilly</span>. <a href="https://r4ds.had.co.nz">https://r4ds.had.co.nz</a>
</div>
<div id="ref-hayesIntroductionMediationModeration2018" class="csl-entry">
Hayes, Andrew F. (2018). <em>Introduction to mediation, moderation, and conditional process analysis: <span>A</span> regression-based approach</em> (Second edition). <span>The Guilford Press</span>. <a href="https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654">https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654</a>
</div>
<div id="ref-R-priorsense" class="csl-entry">
Kallioinen, N., Bürkner, P.-C., Paananen, T., &amp; Vehtari, A. (2022). <em><span class="nocase">priorsense</span>: <span>Prior</span> diagnostics and sensitivity analysis</em> [Manual].
</div>
<div id="ref-kallioinen2021DetectingAndDiagnosing" class="csl-entry">
Kallioinen, N., Paananen, T., Bürkner, P.-C., &amp; Vehtari, A. (2021). <em>Detecting and diagnosing prior and likelihood sensitivity with power-scaling</em>. <span>arXiv</span>. <a href="https://doi.org/10.48550/ARXIV.2107.14054">https://doi.org/10.48550/ARXIV.2107.14054</a>
</div>
<div id="ref-kruschkeDoingBayesianData2015" class="csl-entry">
Kruschke, J. K. (2015). <em>Doing <span>Bayesian</span> data analysis: <span>A</span> tutorial with <span>R</span>, <span>JAGS</span>, and <span>Stan</span></em>. <span>Academic Press</span>. <a href="https://sites.google.com/site/doingbayesiandataanalysis/">https://sites.google.com/site/doingbayesiandataanalysis/</a>
</div>
<div id="ref-kurzDoingBayesianDataAnalysis2023" class="csl-entry">
Kurz, A. S. (2023a). <em>Doing <span>Bayesian</span> data analysis in brms and the tidyverse</em> (Version 1.1.0). <a href="https://bookdown.org/content/3686/">https://bookdown.org/content/3686/</a>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2020" class="csl-entry">
McElreath, R. (2020). <em>Statistical rethinking: <span>A Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (Second Edition). <span>CRC Press</span>. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a>
</div>
<div id="ref-R-HDInterval" class="csl-entry">
Meredith, M., &amp; Kruschke, J. (2018). <em><span>HDInterval</span>: <span>Highest</span> (posterior) density intervals</em> [Manual]. <a href="https://CRAN.R-project.org/package=HDInterval">https://CRAN.R-project.org/package=HDInterval</a>
</div>
<div id="ref-blavaan2021" class="csl-entry">
Merkle, E. C., Fitzsimmons, E., Uanhoro, J., &amp; Goodrich, B. (2021). Efficient <span>Bayesian</span> structural equation modeling in <span>Stan</span>. <em>Journal of Statistical Software</em>, <em>100</em>(6), 1–22. <a href="https://doi.org/10.18637/jss.v100.i06">https://doi.org/10.18637/jss.v100.i06</a>
</div>
<div id="ref-Merkle2018blavaan" class="csl-entry">
Merkle, E. C., &amp; Rosseel, Y. (2018). <span class="nocase">blavaan</span>: <span>Bayesian</span> structural equation models via parameter expansion. <em>Journal of Statistical Software</em>, <em>85</em>(4), 1–30. <a href="https://doi.org/10.18637/jss.v085.i04">https://doi.org/10.18637/jss.v085.i04</a>
</div>
<div id="ref-R-blavaan" class="csl-entry">
Merkle, E. C., Rosseel, Y., &amp; Goodrich, B. (2022). <em><span class="nocase">blavaan</span>: <span>Bayesian</span> latent variable analysis</em>. <a href="https://CRAN.R-project.org/package=blavaan">https://CRAN.R-project.org/package=blavaan</a>
</div>
<div id="ref-R-psych" class="csl-entry">
Revelle, W. (2022). <em><span class="nocase">psych</span>: <span>Procedures</span> for psychological, psychometric, and personality research</em>. <a href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</a>
</div>
<div id="ref-R-lavaan" class="csl-entry">
Rosseel, Y., &amp; Jorgensen, T. D. (2019). <em><span class="nocase">lavaan</span>: <span>Latent</span> variable analysis</em> [Manual]. <a href="https://lavaan.org">https://lavaan.org</a>
</div>
<div id="ref-standevelopmentteamRStanInterfaceStan2023" class="csl-entry">
Stan Development Team. (2023). <em><span>RStan</span>: <span>The R Interface</span> to <span>Stan</span></em>. <a href="https://CRAN.R-project.org/package=rstan/vignettes/rstan.html">https://CRAN.R-project.org/package=rstan/vignettes/rstan.html</a>
</div>
<div id="ref-vehtariUsingLooPackage2020" class="csl-entry">
Vehtari, A., &amp; Gabry, J. (2020, July 14). <em>Using the loo package (version <span><span class="math inline">\(&gt;\)</span></span>= 2.0.0)</em>. <a href="https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html">https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html</a>
</div>
<div id="ref-R-loo" class="csl-entry">
Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp; Gelman, A. (2022). <em><span class="nocase">loo</span>: <span>Efficient</span> leave-one-out cross-validation and <span>WAIC</span> for bayesian models</em>. <a href="https://CRAN.R-project.org/package=loo/">https://CRAN.R-project.org/package=loo/</a>
</div>
<div id="ref-vehtariPracticalBayesianModel2017" class="csl-entry">
Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical <span>Bayesian</span> model evaluation using leave-one-out cross-validation and <span>WAIC</span>. <em>Statistics and Computing</em>, <em>27</em>(5), 1413–1432. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>
</div>
<div id="ref-yaoUsingStackingAverage2018" class="csl-entry">
Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average <span>Bayesian</span> predictive distributions (with discussion). <em>Bayesian Analysis</em>, <em>13</em>(3), 917–1007. <a href="https://doi.org/10.1214/17-BA1091">https://doi.org/10.1214/17-BA1091</a>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Hayes did not cover count regression with the Poisson likelihood in this textbook. If you’d like to learn more, one place to start is with <a href="https://youtu.be/hRJtKCIDTwc">McElreath’s lecture on binomial and Poisson models</a>.<a href="fundamentals-of-linear-regression.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>We will not be covering structural equation modeling (SEM) in this book If you’re interested, you might check out <a href="https://www.youtube.com/playlist?list=PLw93TUuxrFAZkJVc5dhgTZpOT7qmTjlT7">this video series</a> by Erin M. Buchanan on SEM within <strong>R</strong>, particularly with the handy <a href="https://lavaan.ugent.be/"><strong>lavaan</strong></a> package <span class="citation">(<a href="#ref-Merkle2018blavaan" role="doc-biblioref">Merkle &amp; Rosseel, 2018</a>; <a href="#ref-R-lavaan" role="doc-biblioref">Rosseel &amp; Jorgensen, 2019</a>)</span>.<a href="fundamentals-of-linear-regression.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-simple-mediation-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
