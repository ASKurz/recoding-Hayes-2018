# Some Myths and Additional Extensions of Moderation Analysis



## Truths and myths about mean-centering

Here we load a couple necessary packages, load the data, and take a `glimpse()`.


```r
library(tidyverse)

glbwarm <- read_csv("data/glbwarm/glbwarm.csv")

glimpse(glbwarm)
```

```
## Observations: 815
## Variables: 7
## $ govact   <dbl> 3.6, 5.0, 6.6, 1.0, 4.0, 7.0, 6.8, 5.6, 6.0, 2.6, 1.4, 5.6, 7.0, 3.8, 3.4, 4.2, 1.0...
## $ posemot  <dbl> 3.67, 2.00, 2.33, 5.00, 2.33, 1.00, 2.33, 4.00, 5.00, 5.00, 1.00, 4.00, 1.00, 5.67,...
## $ negemot  <dbl> 4.67, 2.33, 3.67, 5.00, 1.67, 6.00, 4.00, 5.33, 6.00, 2.00, 1.00, 4.00, 5.00, 4.67,...
## $ ideology <int> 6, 2, 1, 1, 4, 3, 4, 5, 4, 7, 6, 4, 2, 4, 5, 2, 6, 4, 2, 4, 4, 2, 6, 4, 4, 3, 4, 5,...
## $ age      <int> 61, 55, 85, 59, 22, 34, 47, 65, 50, 60, 71, 60, 71, 59, 32, 36, 69, 70, 41, 48, 38,...
## $ sex      <int> 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,...
## $ partyid  <int> 2, 1, 1, 1, 1, 2, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 1, 1, 3,...
```

Before we fit our models, we'll go ahead and make our mean-centered predictors, `negemot_c` and `age_c`.


```r
glbwarm <-
  glbwarm %>% 
  mutate(negemot_c = negemot - mean(negemot),
         age_c = age - mean(age))
```

Now we're ready to fit Models 1 and 2. But before we do, it's worth repeating part of the text:

>Mean-centering has been recommended in a few highly regarded books on regression analysis (e.g., [Aiken & West, 1991](https://books.google.com/books/about/Multiple_Regression.html?id=LcWLUyXcmnkC); [Cohen et al., 2003](https://books.google.com/books/about/Applied_Multiple_Regression_Correlation.html?id=fAnSOgbdFXIC)), and several explanations have been offered for why mean-centering should be undertaken prior to computation of the product and model estimation. The explanation that seems to have resulted in the most misunderstanding is that $X$ and $W$ are likely to be highly correlated with $XW$ and this will produce estimation problems caused by collinearity and result in poor or "strange" estimates of regression coefficients, large standard errors, and reduced power of the statistical test of the interaction. But his is, in large part, simply a myth. (p. 304)

Let's load brms.


```r
library(brms)
```

As we'll see in just a bit, there are some important reasons for Bayesians using HMC to mean center that wouldn't pop up within the OLS paradigm. First let's fit `model1` and `model2`.


```r
model1 <- 
  brm(data = glbwarm, family = gaussian,
      govact ~ 1 + negemot + age + negemot:age,
      chains = 4, cores = 4)

model2 <- 
  update(model1, newdata = glbwarm,
         govact ~ 1 + negemot_c + age_c + negemot_c:age_c,
         chains = 4, cores = 4)
```

As with Hayes's OLS models, our HMC models yield the same Bayesian $R^2$ distributions, within simulation error.


```r
bayes_R2(model1) %>% round(digits = 3)
```

```
##    Estimate Est.Error  Q2.5 Q97.5
## R2    0.354     0.021 0.311 0.394
```

```r
bayes_R2(model2) %>% round(digits = 3)
```

```
##    Estimate Est.Error Q2.5 Q97.5
## R2    0.354     0.021 0.31 0.395
```

Our model summaries also correspond nicely with those in Table 9.1.


```r
print(model1, digits = 3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ 1 + negemot + age + negemot:age 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept      4.343     0.333    3.705    5.008       1682 1.000
## negemot        0.145     0.086   -0.026    0.312       1598 1.000
## age           -0.031     0.006   -0.043   -0.019       1663 1.000
## negemot:age    0.007     0.002    0.004    0.010       1595 1.000
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma    1.097     0.028    1.045    1.155       2553 1.001
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

```r
print(model2, digits = 3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ negemot_c + age_c + negemot_c:age_c 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept          4.597     0.038    4.525    4.674       3777 1.000
## negemot_c          0.501     0.025    0.452    0.551       3322 1.000
## age_c             -0.005     0.002   -0.010   -0.001       4000 0.999
## negemot_c:age_c    0.007     0.002    0.004    0.010       4000 0.999
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma    1.097     0.028    1.044    1.154       3563 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

However, notice the 'Eff.Sample' columns. The values for `model2` were substantially larger than those for `model1`. 'Eff.Sample' is Bürkner's term for the number of effective samples. Recall that because we've been using brms defaults, we have 4 HMC chains, each of which contains 2000 draws (iterations), the first 1000 of which are warmup values. After we discard the warmup values, that leaves 1000 draws from each chain--4000 total. As it turns out, Markov chains, and thus HMC chains, are typically *autocorrelated*, which means that each draw is partially dependent on the previous draw. Ideally, the autocorrelations are near zero. That's often not the case. 

The [bayesplot package](https://github.com/stan-dev/bayesplot) offers a variety of [diagnostic plots](https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html#effective-sample-size). Here we'll use the `mcmc_acf()` function to make autocorrelation plots for all model parameters. Note that when we add `add_chain = T` to `brms::posterior_samples()`, we add an index to the data that allows us to keep track of which iteration comes from which chain. That index will come in handy for our `mcmc_acf()` plots.

But before we get there, we'll be using an [xkcd](https://xkcd.com)-inspired theme with help from the [xkcd package](https://cran.r-project.org/web/packages/xkcd/index.html) for our plots in this chapter.


```r
# install.packages("xkcd", dependencies = T)

library(xkcd)
```

If you haven't used the xkcd package, before, you might also need to take a few extra steps outlined [here](https://cran.r-project.org/web/packages/xkcd/vignettes/xkcd-intro.pdf), part of which requires help from the [extrafont package](https://cran.r-project.org/web/packages/extrafont/README.html),


```r
library(extrafont)

download.file("http://simonsoftware.se/other/xkcd.ttf",
              dest = "xkcd.ttf", mode = "wb")
 
system("mkdir ~/.fonts")
system("cp xkcd.ttf  ~/.fonts")
# This line of code returned an error message
# font_import(pattern = "[X/x]kcd", prompt = FALSE)

# This line from (https://stackoverflow.com/questions/49221040/error-in-font-import-while-installing-xkcd-font) fixed the problem
font_import(path = "~/.fonts", pattern = "[X/x]kcd", prompt=FALSE)
fonts()
fonttable()
 if(.Platform$OS.type != "unix") {
   ## Register fonts for Windows bitmap output
   loadfonts(device="win")
 } else {
   loadfonts()
 }
```


After installing, I still experienced error messages, which were alleviated after I followed [these steps outlined by Remi.b](https://stackoverflow.com/questions/48553545/polygon-edge-not-found-with-the-xkcd-package). You may or may not need them.

But anyways, here are our `mcmc_acf()` plots.


```r
library(bayesplot)

post1 <- posterior_samples(model1, add_chain = T)
mcmc_acf(post1, 
         pars = c("b_Intercept", "b_negemot", "b_age", "b_negemot:age", "sigma"),
         lags = 4) +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-8-1.png" width="768" />

```r
post2 <- posterior_samples(model2, add_chain = T)
mcmc_acf(post2, 
         pars = c("b_Intercept", "b_negemot_c", "b_age_c", "b_negemot_c:age_c", "sigma"),
         lags = 4) +
  theme_xkcd() 
```

<img src="09_files/figure-html/unnamed-chunk-8-2.png" width="768" />

As it turns out, `theme_xkcd()` can't handle special characters like "_", so it returns rectangles instead. So it goes...

But again, high autocorrelations in the HMC chains have consequences for the effective sample size. In the [Visual MCMC diagnostics using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html#effective-sample-size) vignette, Gabry wrote:

>The effective sample size is an estimate of the number of independent draws from the posterior distribution of the estimand of interest. Because the draws within a Markov chain are *not* independent if there is autocorrelation, the effective sample size, $n_{eff}$, will be smaller than the total sample size, $N$. The larger the ratio of $n_{eff}$ to $N$ the better.

The 'Eff.Sample' values were all close to 4000 with `model2` and the autocorrelations were very low, too. The reverse was true for `model1`. The upshot is that even though we have 4000 samples for each parameter, those samples don't necessarily give us the same quality of information fully independent samples would. 'Eff.Sample' helps you determine how concerned you should be. And, as it turns out, things like centering can help increase a models 'Eff.Sample' values.

Wading in further, we can use the `neff_ratio()` function to collect the $n_{eff}$ to $N$ ratio for each model parameter and then use `mcmc_neff()` to make a visual diagnostic. Here we do so for `model1` and `model2`.


```r
ratios_model1 <- 
  neff_ratio(model1, 
             pars = c("b_Intercept", "b_negemot", "b_age", "b_negemot:age", "sigma"))
ratios_model2 <- 
  neff_ratio(model2,
             pars = c("b_Intercept", "b_negemot_c", "b_age_c", "b_negemot_c:age_c", "sigma"))

mcmc_neff(ratios_model1) + 
  yaxis_text(hjust = 0) +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-9-1.png" width="768" />

```r
mcmc_neff(ratios_model2) + 
  yaxis_text(hjust = 0) +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-9-2.png" width="768" />

Although none of the $n_{eff}$ to $N$ ratios were in the shockingly-low range for either model, there were substantially closer to 1 for `model2`.

In addition to autocorrelations and $n_{eff}$ to $N$ ratios, there is also the issue that the parameters in the model can themselves be correlated. If you like a visual approach, you can use `brms::pairs()` to retrieve histograms for each parameter along with scatter plots showing the shape of their correlations. Here we'll use the `off_diag_args` argument to customize some of the plot settings.


```r
pairs(model1,
      off_diag_args = list(size = 1/10,
                           alpha = 1/5))
```

<img src="09_files/figure-html/unnamed-chunk-10-1.png" width="624" />

```r
pairs(model2,
      off_diag_args = list(size = 1/10,
                           alpha = 1/5))
```

<img src="09_files/figure-html/unnamed-chunk-10-2.png" width="624" />

When fitting models with HMC, centering can make a difference for the parameter correlations. If you prefer a more numeric approach, `vcov()` will yield the variance/covariance matrix--or correlation matrix when using `correlation = T`--for the parameters in a model.


```r
vcov(model1, correlation = T) %>% round(digits = 2)
```

```
##             Intercept negemot   age negemot:age
## Intercept        1.00   -0.93 -0.96        0.88
## negemot         -0.93    1.00  0.89       -0.96
## age             -0.96    0.89  1.00       -0.92
## negemot:age      0.88   -0.96 -0.92        1.00
```

```r
vcov(model2, correlation = T) %>% round(digits = 2)
```

```
##                 Intercept negemot_c age_c negemot_c:age_c
## Intercept            1.00      0.02  0.03            0.05
## negemot_c            0.02      1.00  0.05           -0.09
## age_c                0.03      0.05  1.00           -0.01
## negemot_c:age_c      0.05     -0.09 -0.01            1.00
```

*And so wait, what does that even mean for a parameter to correlate with another parameter?* you might ask. Fair enough. Let's compute a correlation step by step. First, `posterior_samples()`:


```r
post <- posterior_samples(model1)

head(post)
```

```
##   b_Intercept  b_negemot       b_age b_negemot:age    sigma      lp__
## 1    4.601742 0.11579358 -0.03407338   0.007279384 1.103365 -1235.889
## 2    4.374973 0.12836145 -0.03303011   0.008040433 1.092021 -1235.508
## 3    4.468470 0.09774705 -0.03503205   0.008483724 1.120604 -1235.919
## 4    4.476113 0.10907074 -0.03525339   0.008603188 1.090927 -1236.579
## 5    4.269829 0.17040481 -0.03090699   0.007015664 1.114033 -1235.271
## 6    4.690371 0.07817528 -0.03894929   0.008895996 1.136302 -1237.102
```

Now we've put our posterior iterations into a data object, `post`, we can make a scatter plot of two parameters. Here we'll choose `b_negemot` and the interaction coefficient, `b_negemot:age`.


```r
post %>% 
  ggplot(aes(x = b_negemot, y = `b_negemot:age`)) +
  geom_point(size = 1/10, alpha = 1/5) +
  labs(subtitle = "Each dot is the parameter pair\nfrom a single iteration. Looking\nacross the 4,000 total posterior\niterations, it becomes clear the\ntwo parameters are highly\nnegatively correlated.") +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-13-1.png" width="384" />

And indeed, the Pearson's correlation is:


```r
cor(post$b_negemot, post$`b_negemot:age`)
```

```
## [1] -0.9571633
```

And what was that part from the `vcov()` output, again?


```r
vcov(model1, correlation = T)["negemot", "negemot:age"]
```

```
## [1] -0.9571633
```

Boom! That's where the correlations come from.

This entire topic of HMC diagnostics can seem baffling, especially when compared to the simplicity of OLS. If this is your first introduction, you might want to watch lectures [10](https://www.youtube.com/watch?v=BWEtS3HuU5A&list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc) and [11](https://www.youtube.com/watch?v=13mEekRdOcQ&list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc) from McElreath's [Statistical Rethinking Fall 2017 lecture series](https://www.youtube.com/playlist?list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc). Accordingly, you might check out chapter 8 of his [*Statistical Rethinking* text](https://xcelab.net/rm/statistical-rethinking/) and [my project explaining how to reproduce the analyses in that chapter in brms](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/blob/master/Ch._08_Markov_Chain_Monte_Carlo.md).

### The effect of mean-centering on multicollinearity and the standard error of $b_{3}$.

This can be difficult to keep track of, but what we just looked at were the correlations among **model parameters**. These are *not* the same as correlations among **variables**. As such, those correlations are not the same as those in Table 9.2. But we can get those, too. First we'll have to do a little more data processing to get all the necessary mean-centered variables and standardized variables.


```r
glbwarm <-
  glbwarm %>% 
  mutate(negemot_x_age     = negemot*age,
         negemot_c_x_age_c = negemot_c*age_c,
         negemot_z         = (negemot - mean(negemot))/sd(negemot),
         age_z             = (age     - mean(age)    )/sd(age)) %>% 
  mutate(negemot_z_x_age_z = negemot_z*age_z)
```

And recall that to get our sweet Bayesian correlations, we use the multivariate `cbind()` syntax to fit an intercepts-only model. Here we do that for all three of the Table 9.2 sections.


```r
correlations1 <- 
  brm(data = glbwarm, family = gaussian,
      cbind(negemot, age, negemot_x_age) ~ 1,
      chains = 4, cores = 4)

correlations2 <- 
  brm(data = glbwarm, family = gaussian,
      cbind(negemot_c, age_c, negemot_c_x_age_c) ~ 1,
      chains = 4, cores = 4)

correlations3 <- 
  brm(data = glbwarm, family = gaussian,
      cbind(negemot_z, age_z, negemot_z_x_age_z) ~ 1,
      chains = 4, cores = 4)
```

Their summaries:


```r
print(correlations1, digits = 3)
```

```
##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: negemot ~ 1 
##          age ~ 1 
##          negemot_x_age ~ 1 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## negemot_Intercept        3.558     0.054    3.454    3.664       3395 1.000
## age_Intercept           49.513     0.573   48.377   50.620       3429 1.000
## negemotxage_Intercept  174.771     3.434  167.802  181.368       2955 1.000
## 
## Family Specific Parameters: 
##                             Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma_negemot                  1.529     0.038    1.458    1.606       2564 1.001
## sigma_age                     16.359     0.397   15.605   17.161       3319 1.000
## sigma_negemotxage             97.422     2.363   92.860  102.135       2674 1.000
## rescor(negemot,age)           -0.059     0.035   -0.128    0.010       3520 1.000
## rescor(negemot,negemotxage)    0.765     0.015    0.735    0.793       2526 1.001
## rescor(age,negemotxage)        0.548     0.024    0.499    0.594       4000 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

```r
print(correlations2, digits = 3)
```

```
##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: negemot_c ~ 1 
##          age_c ~ 1 
##          negemot_c_x_age_c ~ 1 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## negemotc_Intercept         0.000     0.053   -0.102    0.106       4000 0.999
## agec_Intercept             0.005     0.566   -1.095    1.110       4000 0.999
## negemotcxagec_Intercept   -1.422     0.859   -3.115    0.251       4000 1.000
## 
## Family Specific Parameters: 
##                                Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma_negemotc                    1.533     0.039    1.460    1.611       4000 0.999
## sigma_agec                       16.372     0.410   15.606   17.180       4000 0.999
## sigma_negemotcxagec              24.247     0.620   23.072   25.476       4000 0.999
## rescor(negemotc,agec)            -0.056     0.035   -0.124    0.014       4000 1.000
## rescor(negemotc,negemotcxagec)    0.092     0.034    0.023    0.159       4000 1.000
## rescor(agec,negemotcxagec)       -0.015     0.036   -0.084    0.057       4000 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

```r
print(correlations3, digits = 3)
```

```
##  Family: MV(gaussian, gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: negemot_z ~ 1 
##          age_z ~ 1 
##          negemot_z_x_age_z ~ 1 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## negemotz_Intercept        -0.000     0.035   -0.069    0.069       4000 0.999
## agez_Intercept            -0.000     0.035   -0.068    0.066       4000 0.999
## negemotzxagez_Intercept   -0.057     0.035   -0.124    0.012       4000 0.999
## 
## Family Specific Parameters: 
##                                Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma_negemotz                    1.003     0.025    0.957    1.053       4000 1.000
## sigma_agez                        1.002     0.025    0.956    1.051       4000 1.000
## sigma_negemotzxagez               0.972     0.024    0.927    1.022       4000 1.000
## rescor(negemotz,agez)            -0.056     0.035   -0.124    0.013       4000 1.000
## rescor(negemotz,negemotzxagez)    0.091     0.035    0.024    0.161       4000 1.000
## rescor(agez,negemotzxagez)       -0.014     0.035   -0.083    0.053       4000 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

A more condensed way to get that information might be with the `VarCorr()` function. Just make sure to tack `$residual__$cor` onto the end.


```r
VarCorr(correlations1)$residual__$cor %>% 
  round(digits = 3)
```

```
## , , negemot
## 
##             Estimate Est.Error   Q2.5 Q97.5
## negemot        1.000     0.000  1.000 1.000
## age           -0.059     0.035 -0.128 0.010
## negemotxage    0.765     0.015  0.735 0.793
## 
## , , age
## 
##             Estimate Est.Error   Q2.5 Q97.5
## negemot       -0.059     0.035 -0.128 0.010
## age            1.000     0.000  1.000 1.000
## negemotxage    0.548     0.024  0.499 0.594
## 
## , , negemotxage
## 
##             Estimate Est.Error  Q2.5 Q97.5
## negemot        0.765     0.015 0.735 0.793
## age            0.548     0.024 0.499 0.594
## negemotxage    1.000     0.000 1.000 1.000
```

For the sake of space, I'll let you check that out for `correlations2` and `correlations3`. If you're tricky with your `VarCorr()` indexing, you can also get the model-implied variances.


```r
VarCorr(correlations1)$residual__$cov[1, , "negemot"] %>% round(digits = 3)
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
##     2.341     0.116     2.126     2.578
```

```r
VarCorr(correlations1)$residual__$cov[2, , "age"] %>% round(digits = 3)
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
##   267.775    13.017   243.525   294.496
```

```r
VarCorr(correlations1)$residual__$cov[3, , "negemotxage"] %>% round(digits = 3)
```

```
##  Estimate Est.Error      Q2.5     Q97.5 
##  9496.592   460.872  8622.952 10431.460
```

And if you're like totally lost with all this indexing, you might code `VarCorr(correlations1) %>% str()` and spend a little time looking at what `VarCorr()` produces.

On page 309, Hayes explained why the OLS variance for $b_{3}$ is unaffected by mean centering. The story was similar for our HMC model, too:


```r
fixef(model1)["negemot:age", "Est.Error"]
```

```
## [1] 0.001609585
```

```r
fixef(model2)["negemot_c:age_c", "Est.Error"]
```

```
## [1] 0.001554206
```

For more details, you might also see the [28.11. Standardizing Predictors and Outputs subsection of the Stan Modeling Language User’s Guide and Reference Manual, 2.17.0](http://mc-stan.org/users/documentation/)--[Stan](http://mc-stan.org), of course, being the computational engine underneath our brms hood.

### The effect of mean-centering on $b_{1}$, $b_{2}$, and their ~~standard errors~~ posterior $SD$s.

If you only care about posterior means, you can reproduce the results at the bottom of page 310 like:


```r
fixef(model1)["negemot", 1] + 
  fixef(model1)["negemot:age", 1]*mean(glbwarm$age)
```

```
## [1] 0.5009198
```

But we're proper Bayesians and like a summary of the spread in the posterior. So we'll evoke `posterior_samples()` and the other usual steps.


```r
post <- posterior_samples(model1)

post %>% 
  transmute(our_contidional_effect_given_W_bar = b_negemot + `b_negemot:age`*mean(glbwarm$age)) %>%
  summarize(mean = mean(our_contidional_effect_given_W_bar),
            sd = sd(our_contidional_effect_given_W_bar)) %>% 
  round(digits = 3)
```

```
##    mean    sd
## 1 0.501 0.025
```

And note how the standard error Hayes computed at the top of page 311 corresponds nicely with the posterior $SD$ we just computed. Hayes employed a fancy formula; we just used `sd()`.

### The ~~centering option in PROCESS~~.

I'm not aware of a similar function in brms. You'll have to use your data wrangling skills.

## The estimation and interpretation of standardized regression coefficients in a moderation analysis

### Variant 1.

We've already computed standardized predictors. Now we just need to standardize the criterion, `govact`.


```r
glbwarm <-
  glbwarm %>% 
  mutate(govact_z = (govact - mean(govact))/sd(govact))
```

Fit:


```r
model3 <- 
  update(model1, newdata = glbwarm,
         govact_z ~ 1 + negemot_z + age_z + negemot_z:age_z,
         chains = 4, cores = 4)
```


```r
bayes_R2(model3) %>% round(digits = 3)
```

```
##    Estimate Est.Error  Q2.5 Q97.5
## R2    0.354     0.022 0.309 0.396
```


```r
print(model3, digits = 3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact_z ~ negemot_z + age_z + negemot_z:age_z 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept          0.008     0.028   -0.045    0.061       4000 1.000
## negemot_z          0.563     0.029    0.504    0.619       4000 1.000
## age_z             -0.063     0.028   -0.117   -0.007       4000 1.000
## negemot_z:age_z    0.131     0.029    0.074    0.188       4000 1.000
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma    0.806     0.020    0.769    0.847       4000 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

### Variant 2.

This time we need to standardize our interaction term, `negemot_x_age_z`, by hand.


```r
glbwarm <-
  glbwarm %>% 
  mutate(negemot_x_age_z = (negemot_x_age - mean(negemot_x_age))/sd(negemot_x_age))
```

Now we're ready to fit.


```r
model4 <- 
  update(model1, newdata = glbwarm,
         govact_z ~ 1 + negemot_z + age_z + negemot_x_age_z,
         chains = 4, cores = 4)
```


```r
bayes_R2(model4) %>% round(digits = 3)
```

```
##    Estimate Est.Error Q2.5 Q97.5
## R2    0.354     0.021 0.31 0.394
```


```r
print(model4, digits = 3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact_z ~ negemot_z + age_z + negemot_x_age_z 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept          0.000     0.029   -0.056    0.056       3171 1.000
## negemot_z          0.168     0.094   -0.020    0.349       1211 1.004
## age_z             -0.366     0.072   -0.510   -0.226       1196 1.005
## negemot_x_age_z    0.508     0.112    0.285    0.728       1172 1.005
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma    0.807     0.021    0.767    0.849       2907 1.001
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

The results correspond nicely to those in Table 9.1. 

## A caution on manual centering and standardization

It's worthwhile considering the issue of listwise deletion when data are partially missing. The brms default is to delete rows with missingness, "NA" in R, for the predictors. However, [brms allows users to perform one-step Bayesian imputation for missing values using the `mi()` syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html). First we'll fit see what happens when you fit a model in brms when some of the `negemot_z` values are missing, but without using the `mi()` syntax. And of course before we do that, we'll make a `negemot_z_missing` variable, which is identical to `negemot_z`, but about 10% of the values are missing.


```r
set.seed(815)
glbwarm <-
  glbwarm %>% 
  mutate(missing = rbinom(n = 815, size = 1, prob = .1)) %>% 
  mutate(negemot_z_missing = ifelse(missing == 1, NA, negemot_z))
```

If you've never used `rbinom()` before, code `?rbinom` or look it up in your favorite web search engine. Here's our listwise deletion model, which corresponds to what you'd get from a typical OLS-based program.


```r
model5 <- 
  update(model3, newdata = glbwarm,
         govact_z ~ 1 + negemot_z_missing + age_z + negemot_z_missing:age_z,
         chains = 4, cores = 4)
```

Let's compare the listwise deletion results with the model based on all the data.


```r
print(model3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact_z ~ negemot_z + age_z + negemot_z:age_z 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept           0.01      0.03    -0.05     0.06       4000 1.00
## negemot_z           0.56      0.03     0.50     0.62       4000 1.00
## age_z              -0.06      0.03    -0.12    -0.01       4000 1.00
## negemot_z:age_z     0.13      0.03     0.07     0.19       4000 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     0.81      0.02     0.77     0.85       4000 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

```r
print(model5)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact_z ~ negemot_z_missing + age_z + negemot_z_missing:age_z 
##    Data: glbwarm (Number of observations: 719) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                         Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## Intercept                   0.00      0.03    -0.06     0.06       4000 1.00
## negemot_z_missing           0.56      0.03     0.50     0.62       4000 1.00
## age_z                      -0.05      0.03    -0.11     0.00       4000 1.00
## negemot_z_missing:age_z     0.12      0.03     0.06     0.18       4000 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     0.81      0.02     0.77     0.86       4000 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

In this case, the model results were similar to those based on all the data because we used `rbinom()` to delete the predictor values completely at random. With real data and real-live missing data mechanisms, the situation isn't often so rosy. But anyway, the real story, here, is the `Data: glbwarm (Number of observations: n)` line at the top of the `print()` outputs. The number, $n$, was 815 in the model using all the data and 719 for the one based on listwise deletion. That's a lot of missing information.

The `mi()` syntax will allow us to use all the rows in a model, even if one or more of the predictors contain missing values. The syntax makes the model a multivariate model in that now we'll be modeling both `govact_z` *and* `negemot_z_missing`. There are multiple ways to write a [multivariate model in brms](https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html). One nice way is to write the model for each criterion separately in a `bf()` statement. You combine the `bf()` statements together with the `+` operator. And for models like the ones in Hayes's text, you'll also want to tack on `set_rescor(FALSE)`. You can do this within the `brm()` function, as usual. But I find that this clutters the code up more than I like. So another approach is to save the combination of `bf()` statements as an object.


```r
my_model <- 
  bf(govact_z  ~ 1 + mi(negemot_z_missing) + age_z + mi(negemot_z_missing):age_z) + 
  bf(negemot_z_missing | mi() ~ 1) + 
  set_rescor(FALSE)
```

With our multivariate formula saved as `my_model`, we're ready to plug it into `brm()` and fit.


```r
model6 <- 
  brm(data = glbwarm,
      family = gaussian,
      my_model,
      chains = 4, cores = 4)
```

Let's see what we've done.


```r
print(model6)
```

```
##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: govact_z ~ 1 + mi(negemot_z_missing) + age_z + mi(negemot_z_missing):age_z 
##          negemot_z_missing | mi() ~ 1 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## govactz_Intercept                     0.01      0.03    -0.05     0.06       4000 1.00
## negemotzmissing_Intercept             0.00      0.04    -0.07     0.08       4000 1.00
## govactz_age_z                        -0.07      0.03    -0.12    -0.01       4000 1.00
## govactz_minegemot_z_missing           0.56      0.03     0.50     0.62       4000 1.00
## govactz_minegemot_z_missing:age_z     0.13      0.03     0.07     0.19       4000 1.00
## 
## Family Specific Parameters: 
##                       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma_govactz             0.81      0.02     0.77     0.85       4000 1.00
## sigma_negemotzmissing     1.00      0.03     0.95     1.05       4000 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

When using the multivariate `mi()` syntax, your `print()` output becomes more complicated. Now we have a regression model for both `govact_z` and `negemot_z_missing`. At a minimum, each has its own intercept and residual variance (i.e., sigma). In the 'Population-Level Effects' section, the first part of the names for each regression coefficient clarifies which $Y$-variable it corresponds to (e.g., `govactz_Intercept` is the intercept for our primary $Y$-variable, `govact_z`). In the 'Family Specific Parameters' section, the sigmas are similarly labeled. 

Perhaps most importantly, we see "Data: glbwarm (Number of observations: 815)" at the top of the output. The multivariate `mi()` syntax used all the available data. No listwise deletion necessary.

The `print()` output for our model obscured some of the results. To clarify what the `mi()` syntax did, let's peek at the first columns returned by `posterior_samples()`.


```r
post <- posterior_samples(model6)

post[, 1:20] %>% 
  glimpse()
```

```
## Observations: 4,000
## Variables: 20
## $ b_govactz_Intercept                     <dbl> -0.009990340, 0.021540051, 0.002501819, -0.034147985...
## $ b_negemotzmissing_Intercept             <dbl> 0.0169003031, -0.0314997166, 0.0268748216, 0.0229868...
## $ b_govactz_age_z                         <dbl> -0.049833149, -0.059868350, -0.075648875, -0.0778382...
## $ bsp_govactz_minegemot_z_missing         <dbl> 0.5816318, 0.5392590, 0.5393454, 0.4762133, 0.540401...
## $ `bsp_govactz_minegemot_z_missing:age_z` <dbl> 0.14544354, 0.10537405, 0.13267566, 0.13912678, 0.14...
## $ sigma_govactz                           <dbl> 0.8140491, 0.7789521, 0.8355927, 0.8168074, 0.789764...
## $ sigma_negemotzmissing                   <dbl> 1.0399030, 0.9872644, 1.0050982, 0.9943740, 0.967165...
## $ `Ymi_negemotzmissing[7]`                <dbl> 2.03642753, 0.01721568, 0.67960492, 0.64964174, 0.67...
## $ `Ymi_negemotzmissing[22]`               <dbl> 1.03507669, -0.32474974, 0.47110954, -0.38361393, 0....
## $ `Ymi_negemotzmissing[31]`               <dbl> -1.0684763, 1.1596497, -1.1248319, 0.8671490, -0.405...
## $ `Ymi_negemotzmissing[55]`               <dbl> -0.23866444, 1.11734470, -0.58935567, 0.27172464, -0...
## $ `Ymi_negemotzmissing[60]`               <dbl> -0.23798926, 1.76955914, -0.24605036, 1.70170122, 2....
## $ `Ymi_negemotzmissing[66]`               <dbl> -0.570751760, 0.910622983, -1.110464685, -0.59118488...
## $ `Ymi_negemotzmissing[72]`               <dbl> 0.07032315, 0.29686228, 1.90339923, 1.58553141, 0.68...
## $ `Ymi_negemotzmissing[86]`               <dbl> -0.58489937, -0.19960985, -0.17835803, -0.09381337, ...
## $ `Ymi_negemotzmissing[87]`               <dbl> -0.41169811, -0.61084609, -1.09517536, -2.48993414, ...
## $ `Ymi_negemotzmissing[98]`               <dbl> 0.47187314, -0.20029597, -0.74314411, -0.60489423, -...
## $ `Ymi_negemotzmissing[103]`              <dbl> 1.326050797, -0.414836975, 0.585970543, -0.004622591...
## $ `Ymi_negemotzmissing[107]`              <dbl> -0.455625258, -1.346992477, -0.670417852, -0.9688168...
## $ `Ymi_negemotzmissing[131]`              <dbl> -1.02817613, -0.38086307, -0.45937949, -0.83533452, ...
```

Columns `b_govactz_Intercept` through `sigma_negemotzmissing` were business as usual. But notice all the `Ymi_negemotzmissing[i]` columns. In each of these we see 4,000 posterior draws for the missing `negemot_z_missing` values. The `[i]` part of the column names indexes which row number the iterations correspond to. Since we made a lot of missing values in the data, I won't go through them all. But we can focus on a few to get a sense of the results.


```r
post %>% 
  select(`Ymi_negemotzmissing[7]`:`Ymi_negemotzmissing[131]`) %>% 
  gather(row, value) %>% 
  group_by(row) %>% 
  # Yep, that's right, we're summarizing as usual
  summarize(mean = mean(value),
            sd = sd(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>% 
  mutate_if(is.double, round, digits = 2) %>%
  mutate(row = str_extract(row, "\\d+") %>% as.integer())  # this line just makes the row names easier to read
```

```
## # A tibble: 13 x 5
##      row   mean    sd    ll    ul
##    <int>  <dbl> <dbl> <dbl> <dbl>
##  1   103  0.65   0.84 -0.99  2.28
##  2   107 -0.580  0.75 -2.07  0.89
##  3   131 -0.45   0.76 -1.96  1.01
##  4    22  0.38   0.78 -1.16  1.89
##  5    31 -0.13   0.74 -1.56  1.33
##  6    55 -0.27   0.84 -1.89  1.39
##  7    60  0.48   0.78 -1.04  1.96
##  8    66 -0.34   0.81 -1.93  1.22
##  9     7  0.91   0.84 -0.7   2.59
## 10    72  0.34   0.75 -1.14  1.84
## 11    86 -0.33   0.83 -1.99  1.29
## 12    87 -0.67   0.73 -2.09  0.75
## 13    98 -0.18   0.85 -1.89  1.44
```

In conventional mean-imputation, you just plug the sample mean into the missing value slot (which is a sin against data; don't do this). With multiple imputation, you create a small number of alternative data sets, typically 5, into which you impute plausible values into the missing value slots. With one-step Bayesian imputation using the `mi()` syntax, you get an entire posterior distribution for each missing value. And if you have variables in the data set that might help predict what those missing values are, you’d just plug that into the model. For more on the topic, see Bürkner’s [vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_missings.html), McElreath’s [lecture on the topic](https://www.youtube.com/watch?v=Yi0EqAu043A), or my [effort to translate the chapter 14 code in McElreath’s text into brms](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse/blob/master/Ch._14_Missing_Data_and_Other_Opportunities.md).

The take home message is there is no need to ignore missing data or use outdated procedures like listwise deletion. Be a champion and model your missing data with brms.

## More than one moderator

None of this is a problem for brms. But instead of using the `model=i` syntax in Hayes's PROCESS, you just have to specify your model formula in `brm()`.

### Additive multiple moderation.

It's trivial to add `sex`, its interaction with `negemot`, and the two covariates (i.e., `posemot` and `ideology`) to the model. We can even do it within `update()`.


```r
model7 <- 
  update(model1, newdata = glbwarm,
         govact ~ 1 + negemot + sex + age + posemot + ideology + negemot:sex + negemot:age,
         chains = 4, cores = 4)
```

Our output matches nicely with the formula at the bottom of page 232 and the PROCESS output in Figure 9.2. 


```r
print(model7, digits = 3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ negemot + sex + age + posemot + ideology + negemot:sex + negemot:age 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept      5.274     0.341    4.614    5.946       2315 0.999
## negemot        0.091     0.084   -0.074    0.252       2051 0.999
## sex           -0.747     0.194   -1.129   -0.365       2176 1.001
## age           -0.018     0.006   -0.030   -0.006       2068 0.999
## posemot       -0.023     0.027   -0.075    0.031       3936 0.999
## ideology      -0.207     0.027   -0.261   -0.156       3764 1.001
## negemot:sex    0.206     0.050    0.103    0.305       2148 1.001
## negemot:age    0.005     0.002    0.002    0.008       1947 1.000
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma    1.048     0.027    0.997    1.101       3812 1.000
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

On page 325, Hayes discussed the unique variance each of the two moderation terms accounted for after controlling for the other covariates. In order to get our Bayesian version of these, we'll have to fit two additional models, one after removing each of the interaction terms.


```r
model8 <- 
  update(model7, newdata = glbwarm,
         govact ~ 1 + negemot + sex + age + posemot + ideology + negemot:sex,
         chains = 4, cores = 4)

model9 <- 
  update(model7, newdata = glbwarm,
         govact ~ 1 + negemot + sex + age + posemot + ideology + negemot:age,
         chains = 4, cores = 4)
```

Here we'll extract the `bayes_R2()` iterations for each of the three models, place them all in a single tibble, and then do a little arithmetic to get the difference scores. After all that data wrangling, we'll `summarize()` as usual.


```r
r2_without_age_interaction <- bayes_R2(model8, summary = F) %>% as_tibble()
r2_without_sex_interaction <- bayes_R2(model9, summary = F) %>% as_tibble()
r2_with_both_interactions  <- bayes_R2(model7, summary = F) %>% as_tibble()

r2s <-
  tibble(r2_without_age_interaction = r2_without_age_interaction$R2,
         r2_without_sex_interaction = r2_without_sex_interaction$R2,
         r2_with_both_interactions  = r2_with_both_interactions$R2) %>% 
  mutate(`delta R2 due to age interaction` = r2_with_both_interactions - r2_without_age_interaction,
         `delta R2 due to sex interaction` = r2_with_both_interactions - r2_without_sex_interaction)

r2s %>% 
  select(`delta R2 due to age interaction`:`delta R2 due to sex interaction`) %>% 
  gather() %>% 
  group_by(key) %>% 
  summarize(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>% 
  mutate_if(is.double, round, digits = 3)
```

```
## # A tibble: 2 x 4
##   key                              mean     ll    ul
##   <chr>                           <dbl>  <dbl> <dbl>
## 1 delta R2 due to age interaction 0.007 -0.049 0.062
## 2 delta R2 due to sex interaction 0.012 -0.041 0.068
```

Recall that $R^2$ is in a 0-to-1 metric. It's a proportion. If you want to convert that to a percentage, as in percent of variance explained, you'd just multiply by 100. To make it explicit, let's do that.


```r
r2s %>% 
  select(`delta R2 due to age interaction`:`delta R2 due to sex interaction`) %>% 
  gather() %>% 
  group_by(key) %>%
  summarize(mean = mean(value)*100,
            ll = quantile(value, probs = .025)*100,
            ul = quantile(value, probs = .975)*100) %>% 
  mutate_if(is.double, round, digits = 3)
```

```
## # A tibble: 2 x 4
##   key                              mean    ll    ul
##   <chr>                           <dbl> <dbl> <dbl>
## 1 delta R2 due to age interaction 0.665 -4.88  6.19
## 2 delta R2 due to sex interaction 1.23  -4.12  6.84
```

Hopefully it's clear how our proportions turned percentages correspond to the figures on page 325. However, note how our 95% credible intervals do not cohere with the $p$-values from Hayes's $F$-tests. 

If we want to prep for our version of Figure 9.3, we'll need to carefully specify the predictor values we'll pass through the `fitted()` function. Here we do so and save them in `nd`.


```r
nd <-
  tibble(negemot = rep(seq(from = .5, to = 6.5, length.out = 30),
                       times = 6),
         sex = rep(rep(0:1, each = 30),
                   times = 3),
         age = rep(c(30, 50, 70), each = 60),
         posemot = mean(glbwarm$posemot),
         ideology = mean(glbwarm$ideology))

head(nd)
```

```
## # A tibble: 6 x 5
##   negemot   sex   age posemot ideology
##     <dbl> <int> <dbl>   <dbl>    <dbl>
## 1   0.5       0    30    3.13     4.08
## 2   0.707     0    30    3.13     4.08
## 3   0.914     0    30    3.13     4.08
## 4   1.12      0    30    3.13     4.08
## 5   1.33      0    30    3.13     4.08
## 6   1.53      0    30    3.13     4.08
```

With our `nd` values in hand, we're ready to make our version of Figure 9.3.


```r
fitted(model7, newdata = nd) %>% 
  as_tibble() %>% 
  bind_cols(nd) %>% 
  # These lines will make the strip text match with those with Hayes's Figure
  mutate(sex = ifelse(sex == 0, str_c("Females, W = ", sex),
                      str_c("Males, W = ", sex)),
         age = str_c("Age, Z, = ", age)) %>% 

  # finally, the plot!
  ggplot(aes(x = negemot, group = sex)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = sex),
              alpha = 1/3, color = "transparent") +
  geom_line(aes(y = Estimate, color = sex),
            size = 1) +
  scale_x_continuous(breaks = 1:6) +
  coord_cartesian(xlim = 1:6,
                  ylim = 3:6) +
  labs(x = expression(paste("Negative Emotions about Climate Change, ", italic(X))),
       y = expression(paste("Support for Government Action to Mitigate Climate Change, ", italic(Y)))) +
  theme_xkcd() +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  facet_grid(age ~ .)
```

<img src="09_files/figure-html/unnamed-chunk-39-1.png" width="576" />

### Moderated moderation.

To fit the moderated moderation model in brms, just add to two new interaction terms to the `formula`.


```r
model10 <- 
  update(model7, newdata = glbwarm,
         govact ~ 1 + negemot + sex + age + posemot + ideology + 
           negemot:sex + negemot:age + sex:age + 
           negemot:sex:age,
         chains = 4, cores = 4)
```


```r
print(model10, digits = 3)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: govact ~ negemot + sex + age + posemot + ideology + negemot:sex + negemot:age + sex:age + negemot:sex:age 
##    Data: glbwarm (Number of observations: 815) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## Intercept          4.558     0.477    3.638    5.446       1084 1.001
## negemot            0.272     0.116    0.056    0.500       1088 1.001
## sex                0.517     0.631   -0.713    1.745        957 1.001
## age               -0.003     0.009   -0.021    0.015       1062 1.001
## posemot           -0.020     0.028   -0.078    0.035       3262 0.999
## ideology          -0.205     0.026   -0.257   -0.154       3391 1.001
## negemot:sex       -0.127     0.164   -0.452    0.189        999 1.000
## negemot:age        0.001     0.002   -0.004    0.005       1109 1.000
## sex:age           -0.025     0.012   -0.048   -0.002        922 1.001
## negemot:sex:age    0.007     0.003    0.001    0.013        969 1.000
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample  Rhat
## sigma    1.047     0.026    0.997    1.098       3661 0.999
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

Our `print()` output matches fairly well with the OLS results on pages 332 and 333. Our new Bayesian $R^2$ is:


```r
bayes_R2(model10) %>% round(digits = 3)
```

```
##    Estimate Est.Error  Q2.5 Q97.5
## R2    0.417      0.02 0.375 0.457
```

Because we haven't changed the predictor variables in the model--just added interactions among them--there's no need to redo our `nd` values. Rather, all we need to do is pass them through `fitted()` based on our new `model10` and plot. Without further ado, here our Figure 9.6.


```r
fitted(model10, newdata = nd) %>% 
  as_tibble() %>% 
  bind_cols(nd) %>% 
  # These lines will make the strip text match with those with Hayes's Figure
  mutate(sex = ifelse(sex == 0, str_c("Females, W = ", sex),
                      str_c("Males, W = ", sex)),
         age = str_c("Age, Z, = ", age)) %>% 
  
  # behold, Figure 9.6!
  ggplot(aes(x = negemot, group = sex)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = sex),
              alpha = 1/3, color = "transparent") +
  geom_line(aes(y = Estimate, color = sex),
            size = 1) +
  scale_x_continuous(breaks = 1:6) +
  coord_cartesian(xlim = 1:6,
                  ylim = 3:6) +
  labs(x = expression(paste("Negative Emotions about Climate Change, ", italic(X))),
       y = expression(paste("Support for Government Action to Mitigate Climate Change, ", italic(Y)))) +
  theme_xkcd() +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  facet_grid(age ~ .)
```

<img src="09_files/figure-html/unnamed-chunk-42-1.png" width="576" />

For the pick-a-point values Hayes covered on page 338, recall that when using `posterior_sample()`, our $b_{4}$ is `b_negemot:sex` and our $b_{7}$ is `b_negemot:sex:age`.


```r
post <- posterior_samples(model10)

post %>% 
  transmute(`age = 30` = `b_negemot:sex` + `b_negemot:sex:age`*30, 
            `age = 50` = `b_negemot:sex` + `b_negemot:sex:age`*50, 
            `age = 70` = `b_negemot:sex` + `b_negemot:sex:age`*70) %>% 
  gather(theta_XW_on_Y_given, value) %>%
  group_by(theta_XW_on_Y_given) %>%
  summarize(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>% 
  mutate_if(is.double, round, digits = 3)
```

```
## # A tibble: 3 x 4
##   theta_XW_on_Y_given  mean     ll    ul
##   <chr>               <dbl>  <dbl> <dbl>
## 1 age = 30            0.071 -0.085 0.227
## 2 age = 50            0.204  0.109 0.3  
## 3 age = 70            0.336  0.183 0.496
```

The way we made a JN technique plot with `fitted()` way back in chapter 7 isn't going to work, here. At least not as far as I can see. Rather, we're going to have to skillfully manipulate our `post` object. For those new to R, this might be a little confusing at first. So I'm going to make a crude attempt first and then get more sophisticated.

Crude attempt:


```r
post %>% 
  transmute(`age = 30` = `b_negemot:sex` + `b_negemot:sex:age`*30, 
            `age = 50` = `b_negemot:sex` + `b_negemot:sex:age`*50, 
            `age = 70` = `b_negemot:sex` + `b_negemot:sex:age`*70) %>% 
  gather(theta_XW_on_Y_given, value) %>%
  mutate(`theta XW on Y given` = str_extract(theta_XW_on_Y_given, "\\d+") %>% as.double()) %>% 
  group_by(`theta XW on Y given`) %>%
  summarize(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>%
  
  # the plot
  ggplot(aes(x = `theta XW on Y given`)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 38.114) +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = mean), 
            size = 1) +
  coord_cartesian(xlim = 20:85,
                  ylim = c(-.25, .75)) +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-44-1.png" width="576" />

Notice how we just took the code from our pick-a-point analysis, left out the `mutate_if()` rounding part, and dumped it into a plot. So one obvious approach would be to pick like 30 or 50 `age` values to plug into `transmute()` and just do the same thing. If you're super afraid of coding, that'd be one intuitive but extremely verbose attempt. And I've done stuff like that earlier in my R career. There's no shame in being extremely verbose and redundant if that's what makes sense. Another way is to think in terms of functions. When we made `age = 30` within `transmute()`, we took a specific `age` value (i.e., 30) and plugged it into the formula `b_negemot:sex + b_negemot:sex:age*i` where $i$ = 30. And when we made `age = 50` we did exactly the same thing but switched out the 30 for a 50. So what we need is a function that will take a range of values for $i$, plug them into our `b_negemot:sex + b_negemot:sex:age*i` formula, and then neatly return the output. A nice base R function for that is `sapply()`.


```r
sapply(15:90, function(i){
  post$`b_negemot:sex` + post$`b_negemot:sex:age`*i
}) %>% 
  as_tibble() %>% 
  str()
```

```
## Classes 'tbl_df', 'tbl' and 'data.frame':	4000 obs. of  76 variables:
##  $ V1 : num  -0.0712 -0.0131 0.0422 0.0194 -0.1911 ...
##  $ V2 : num  -0.0651 -0.0047 0.0481 0.0248 -0.1783 ...
##  $ V3 : num  -0.05887 0.00366 0.05405 0.03022 -0.16561 ...
##  $ V4 : num  -0.0527 0.012 0.06 0.0356 -0.1529 ...
##  $ V5 : num  -0.0465 0.0204 0.0659 0.041 -0.1402 ...
##  $ V6 : num  -0.0403 0.0287 0.0718 0.0464 -0.1274 ...
##  $ V7 : num  -0.0341 0.0371 0.0777 0.0518 -0.1147 ...
##  $ V8 : num  -0.0279 0.0455 0.0837 0.0572 -0.102 ...
##  $ V9 : num  -0.0218 0.0538 0.0896 0.0626 -0.0893 ...
##  $ V10: num  -0.0156 0.0622 0.0955 0.068 -0.0765 ...
##  $ V11: num  -0.0094 0.0705 0.1014 0.0734 -0.0638 ...
##  $ V12: num  -0.00321 0.07889 0.10737 0.07876 -0.05107 ...
##  $ V13: num  0.00297 0.08725 0.11329 0.08416 -0.03835 ...
##  $ V14: num  0.00915 0.09561 0.11921 0.08955 -0.02562 ...
##  $ V15: num  0.0153 0.104 0.1251 0.0949 -0.0129 ...
##  $ V16: num  0.021523 0.112324 0.131062 0.100336 -0.000164 ...
##  $ V17: num  0.0277 0.1207 0.137 0.1057 0.0126 ...
##  $ V18: num  0.0339 0.129 0.1429 0.1111 0.0253 ...
##  $ V19: num  0.0401 0.1374 0.1488 0.1165 0.038 ...
##  $ V20: num  0.0463 0.1458 0.1548 0.1219 0.0507 ...
##  $ V21: num  0.0524 0.1541 0.1607 0.1273 0.0635 ...
##  $ V22: num  0.0586 0.1625 0.1666 0.1327 0.0762 ...
##  $ V23: num  0.0648 0.1708 0.1725 0.1381 0.0889 ...
##  $ V24: num  0.071 0.179 0.178 0.143 0.102 ...
##  $ V25: num  0.0772 0.1876 0.1844 0.1489 0.1144 ...
##  $ V26: num  0.0834 0.1959 0.1903 0.1543 0.1271 ...
##  $ V27: num  0.0895 0.2043 0.1962 0.1597 0.1398 ...
##  $ V28: num  0.0957 0.2126 0.2022 0.1651 0.1526 ...
##  $ V29: num  0.102 0.221 0.208 0.17 0.165 ...
##  $ V30: num  0.108 0.229 0.214 0.176 0.178 ...
##  $ V31: num  0.114 0.238 0.22 0.181 0.191 ...
##  $ V32: num  0.12 0.246 0.226 0.187 0.203 ...
##  $ V33: num  0.127 0.254 0.232 0.192 0.216 ...
##  $ V34: num  0.133 0.263 0.238 0.197 0.229 ...
##  $ V35: num  0.139 0.271 0.244 0.203 0.242 ...
##  $ V36: num  0.145 0.279 0.25 0.208 0.254 ...
##  $ V37: num  0.151 0.288 0.255 0.214 0.267 ...
##  $ V38: num  0.158 0.296 0.261 0.219 0.28 ...
##  $ V39: num  0.164 0.305 0.267 0.224 0.293 ...
##  $ V40: num  0.17 0.313 0.273 0.23 0.305 ...
##  $ V41: num  0.176 0.321 0.279 0.235 0.318 ...
##  $ V42: num  0.182 0.33 0.285 0.241 0.331 ...
##  $ V43: num  0.188 0.338 0.291 0.246 0.343 ...
##  $ V44: num  0.195 0.346 0.297 0.251 0.356 ...
##  $ V45: num  0.201 0.355 0.303 0.257 0.369 ...
##  $ V46: num  0.207 0.363 0.309 0.262 0.382 ...
##  $ V47: num  0.213 0.371 0.315 0.268 0.394 ...
##  $ V48: num  0.219 0.38 0.321 0.273 0.407 ...
##  $ V49: num  0.226 0.388 0.327 0.278 0.42 ...
##  $ V50: num  0.232 0.397 0.332 0.284 0.433 ...
##  $ V51: num  0.238 0.405 0.338 0.289 0.445 ...
##  $ V52: num  0.244 0.413 0.344 0.295 0.458 ...
##  $ V53: num  0.25 0.422 0.35 0.3 0.471 ...
##  $ V54: num  0.257 0.43 0.356 0.305 0.483 ...
##  $ V55: num  0.263 0.438 0.362 0.311 0.496 ...
##  $ V56: num  0.269 0.447 0.368 0.316 0.509 ...
##  $ V57: num  0.275 0.455 0.374 0.321 0.522 ...
##  $ V58: num  0.281 0.463 0.38 0.327 0.534 ...
##  $ V59: num  0.287 0.472 0.386 0.332 0.547 ...
##  $ V60: num  0.294 0.48 0.392 0.338 0.56 ...
##  $ V61: num  0.3 0.488 0.398 0.343 0.573 ...
##  $ V62: num  0.306 0.497 0.404 0.348 0.585 ...
##  $ V63: num  0.312 0.505 0.409 0.354 0.598 ...
##  $ V64: num  0.318 0.514 0.415 0.359 0.611 ...
##  $ V65: num  0.325 0.522 0.421 0.365 0.623 ...
##  $ V66: num  0.331 0.53 0.427 0.37 0.636 ...
##  $ V67: num  0.337 0.539 0.433 0.375 0.649 ...
##  $ V68: num  0.343 0.547 0.439 0.381 0.662 ...
##  $ V69: num  0.349 0.555 0.445 0.386 0.674 ...
##  $ V70: num  0.355 0.564 0.451 0.392 0.687 ...
##  $ V71: num  0.362 0.572 0.457 0.397 0.7 ...
##  $ V72: num  0.368 0.58 0.463 0.402 0.713 ...
##  $ V73: num  0.374 0.589 0.469 0.408 0.725 ...
##  $ V74: num  0.38 0.597 0.475 0.413 0.738 ...
##  $ V75: num  0.386 0.605 0.481 0.419 0.751 ...
##  $ V76: num  0.393 0.614 0.487 0.424 0.763 ...
```

Okay, to that looks a little monstrous. But what we did in the first argument in `sapply()` was tell the function which values we'd like to use in some function. We chose each integer ranging from 15 to 90--which, if you do the math, is 76 values. We then told `sapply()` to plug those values into a custom function, which we defined as `function(i){post$b_negemot:sex + post$b_negemot:sex:age*i}`. In our custom function, `i` was a placeholder for each of those 76 integers. But remember that `post` has 4000 rows, each one corresponding to one of the 4000 posterior iterations. Thus, for each of our 76 `i`-values, we got 4000 results. The `sapply()` function returns a matrix. Since we like to work within the tidyverse and use ggplot2, we just went ahead and put those results in a tibble.

Anyway, with our `sapply()` output in hand, all we need to do is a little more indexing and summarizing and we're ready to plot. The result is our very own version of Figure 9.7.


```r
sapply(15:90, function(i){
  post$`b_negemot:sex` + post$`b_negemot:sex:age`*i
}) %>% 
  as_tibble() %>% 
  gather() %>% 
  mutate(age = rep(15:90, each = 4000)) %>% 
  group_by(age) %>% 
  summarize(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>% 
  
  ggplot(aes(x = age)) +
  geom_hline(yintercept = 0, color = "grey75") +
  geom_vline(xintercept = 38.114, color = "grey75") +
  geom_ribbon(aes(ymin = ll, ymax = ul),
              alpha = 1/2) +
  geom_line(aes(y = mean), 
            size = 1) +
  coord_cartesian(xlim = 20:85,
                  ylim = c(-.25, .75)) +
  labs(x = expression(paste("Age, ", italic(Z))),
       y = "Conditional Two-way Interaction Between\nNegative Emotions and Sex") +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-46-1.png" width="576" />

Or for kicks and giggles, another way to get a clearer sense of how our data informed the shape of the plot, here we replace our `geom_ribbon() + geom_line()` code with `geom_pointrange()`.


```r
sapply(15:90, function(i){
  post$`b_negemot:sex` + post$`b_negemot:sex:age`*i
}) %>% 
  as_tibble() %>% 
  gather() %>% 
  mutate(age = rep(15:90, each = 4000)) %>% 
  group_by(age) %>% 
  summarize(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>% 
  
  ggplot(aes(x = age)) +
  geom_hline(yintercept = 0, color = "grey75") +
  geom_vline(xintercept = 38.114, color = "grey75") +
  geom_pointrange(aes(y = mean, ymin = ll, ymax = ul),
                  shape = 16, size = 1/3) +
  coord_cartesian(xlim = 20:85,
                  ylim = c(-.25, .75)) +
  labs(x = expression(paste("Age, ", italic(Z))),
       y = "Conditional Two-way Interaction Between\nNegative Emotions and Sex") +
  theme_xkcd()
```

<img src="09_files/figure-html/unnamed-chunk-47-1.png" width="576" />

Although I probably wouldn’t try to use a plot like this in a manuscript, I hope it makes clear how the way we’ve been implementing the JN technique is just the pick-a-point approach in bulk. No magic. 

For all you tidyverse fanatics out there, don't worry. There are more tidyverse-centric ways to get the plot values than with `sapply()`. We'll get to them soon enough. It's advantageous to have good old base R `sapply()` up your sleeve, too. And new R users, it's helpful to know that `sapply()` is one part of the `apply()` family of base R functions, which you might learn more about [here](https://www.r-{bloggers.com}r-tutorial-on-the-apply-family-of-functions/) or [here](http://www.dummies.com/programming/r/how-to-use-the-apply-family-of-functions-in-r/) or [here](https://nsaunders.wordpress.com/2010/08/20/a-brief-introduction-to-apply-in-r/).

## Comparing conditional effects

### Comparing conditional effects in the additive multiple moderation model.

### Comparing conditional effects in the moderated moderation model.

### Implementation in ~~PROCESS~~ brms.

Since we don't have the `contrast` feature automated like in PROCESS, we'll have to carefully follow the equations at the bottom of page 344 to specify the values properly in R.


```r
post %>% 
  transmute(`30-year-old men`   = b_negemot + `b_negemot:sex`*1 + `b_negemot:age`*30 + `b_negemot:sex:age`*1*30, 
            `50-year-old women` = b_negemot + `b_negemot:sex`*0 + `b_negemot:age`*50 + `b_negemot:sex:age`*0*30) %>%
  mutate(contrast = `30-year-old men` - `50-year-old women`) %>% 
  gather() %>%
  group_by(key) %>%
  summarize(mean = mean(value),
            sd = sd(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %>% 
  mutate_if(is.double, round, digits = 3)
```

```
## # A tibble: 3 x 5
##   key                mean    sd     ll    ul
##   <chr>             <dbl> <dbl>  <dbl> <dbl>
## 1 30-year-old men   0.371 0.062  0.252 0.495
## 2 50-year-old women 0.319 0.037  0.247 0.391
## 3 contrast          0.052 0.07  -0.084 0.191
```

Notice how our posterior $SD$ corresponded nicely to the standard error in Hayes's contrast test. And we didn't even have to worry about using the frightening formula 9.21 on page 345. That information was contained in the posterior distribution all along. All we had to do was combine the parameter iterations with a little algebra and then `summarize()`.

## References {-}

[Hayes, A. F. (2018). *Introduction to mediation, moderation, and conditional process analysis: A regression-based approach.* (2nd ed.). New York, NY, US: The Guilford Press.](http://afhayes.com/introduction-to-mediation-moderation-and-conditional-process-analysis.html)

## Session info {-}


```r
sessionInfo()
```

```
## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS High Sierra 10.13.4
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] bayesplot_1.5.0 xkcd_0.0.6      extrafont_0.17  brms_2.3.4      Rcpp_0.12.17    bindrcpp_0.2.2 
##  [7] forcats_0.3.0   stringr_1.3.1   dplyr_0.7.6     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1    
## [13] tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1
## 
## loaded via a namespace (and not attached):
##   [1] colorspace_1.3-2     ggridges_0.5.0       rsconnect_0.8.8      rprojroot_1.3-2     
##   [5] htmlTable_1.12       markdown_0.8         base64enc_0.1-3      rstudioapi_0.7      
##   [9] rstan_2.17.3         DT_0.4               mvtnorm_1.0-8        lubridate_1.7.4     
##  [13] xml2_1.2.0           bridgesampling_0.4-0 splines_3.5.1        mnormt_1.5-5        
##  [17] knitr_1.20           shinythemes_1.1.1    Formula_1.2-3        jsonlite_1.5        
##  [21] broom_0.4.5          Rttf2pt1_1.3.7       cluster_2.0.7-1      shiny_1.1.0         
##  [25] compiler_3.5.1       httr_1.3.1           backports_1.1.2      assertthat_0.2.0    
##  [29] Matrix_1.2-14        lazyeval_0.2.1       cli_1.0.0            later_0.7.3         
##  [33] acepack_1.4.1        htmltools_0.3.6      tools_3.5.1          igraph_1.2.1        
##  [37] coda_0.19-1          gtable_0.2.0         glue_1.2.0           reshape2_1.4.3      
##  [41] cellranger_1.1.0     nlme_3.1-137         extrafontdb_1.0      crosstalk_1.0.0     
##  [45] psych_1.8.4          xfun_0.3             rvest_0.3.2          mime_0.5            
##  [49] miniUI_0.1.1.1       gtools_3.8.1         zoo_1.8-2            scales_0.5.0        
##  [53] colourpicker_1.0     hms_0.4.2            promises_1.0.1       Brobdingnag_1.2-5   
##  [57] parallel_3.5.1       inline_0.3.15        shinystan_2.5.0      RColorBrewer_1.1-2  
##  [61] yaml_2.1.19          gridExtra_2.3        loo_2.0.0            StanHeaders_2.17.2  
##  [65] rpart_4.1-13         latticeExtra_0.6-28  stringi_1.2.3        dygraphs_1.1.1.5    
##  [69] checkmate_1.8.5      rlang_0.2.1          pkgconfig_2.0.1      matrixStats_0.53.1  
##  [73] evaluate_0.10.1      lattice_0.20-35      bindr_0.1.1          labeling_0.3        
##  [77] rstantools_1.5.0     htmlwidgets_1.2      tidyselect_0.2.4     plyr_1.8.4          
##  [81] magrittr_1.5         bookdown_0.7         R6_2.2.2             Hmisc_4.1-1         
##  [85] pillar_1.2.3         haven_1.1.2          foreign_0.8-70       withr_2.1.2         
##  [89] xts_0.10-2           nnet_7.3-12          survival_2.42-3      abind_1.4-5         
##  [93] modelr_0.1.2         crayon_1.3.4         utf8_1.1.4           rmarkdown_1.10      
##  [97] grid_3.5.1           readxl_1.1.0         data.table_1.11.4    threejs_0.3.1       
## [101] digest_0.6.15        xtable_1.8-2         httpuv_1.4.4.2       stats4_3.5.1        
## [105] munsell_0.5.0        shinyjs_1.0
```
